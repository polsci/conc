[
  {
    "objectID": "index.html#introduction-to-conc",
    "href": "index.html#introduction-to-conc",
    "title": "Conc",
    "section": "Introduction to Conc",
    "text": "Introduction to Conc\nConc is a Python library that brings tools for corpus linguistic analysis to Jupyter notebooks. Conc aims to allow researchers to analyse large corpora in efficient ways using standard hardware, with the ability to produce clear, publication-ready reports and extend analysis where required using standard Python libraries.\n\n\n\nExample Concordance\n\n\nA staple of data science, Jupyter notebooks allow researchers to present their analysis in an interactive form that combines code, reporting and discussion. They are an ideal format for collaborating with other researchers during research or to share analysis in a way others can reproduce and interact with.\nConc uses spaCy for tokenising texts. SpaCy functionality to annotate texts will be supported soon.\nConc uses well-supported Python libraries for processing data and prioritises fast code libraries and data structures. The library produces clear reports with important information to interpret result by default. Conc makes it easy to extend analysis using other libraries or software. Conc’s corpus format is well-documented and there are code examples to help you work with Conc results and data structures outside of Conc if you want to extend your analysis.\nConc’s documentation site has more information on Conc, why it was developed and the principles guiding Conc’s development.",
    "crumbs": [
      "Introduction to Conc"
    ]
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Conc",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nAcknowledgements\n\nDevelopment Status\n\nInstallation\n\nUsing Conc\n\n\nDirect links to Conc documentation\n\nGetting Started\n\nTutorials (Tutorials to get you started with Conc)\n\nDocumentation (Explanations, Conc API Reference, information on Development)",
    "crumbs": [
      "Introduction to Conc"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Conc",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nConc is developed by Dr Geoff Ford.\nWork to create this Python library has been made possible by funding/support from:\n\n“Mapping LAWS: Issue Mapping and Analyzing the Lethal Autonomous Weapons Debate” (Royal Society of New Zealand’s Marsden Fund Grant 19-UOC-068)\n\n“Into the Deep: Analysing the Actors and Controversies Driving the Adoption of the World’s First Deep Sea Mining Governance” (Royal Society of New Zealand’s Marsden Fund Grant 22-UOC-059)\nSabbatical, University of Canterbury, Semester 1 2025.\n\nThanks to Jeremy Moses and Sian Troath from the Mapping LAWS project team for their support and feedback as first users of ConText (a web-based application built on an earlier version of Conc that I’ve also recently released).\nDr Ford is a researcher with Te Pokapū Aronui ā-Matihiko | UC Arts Digital Lab (ADL). Thanks to the ADL team and the ongoing support of the University of Canterbury’s Faculty of Arts who make work like this possible.\nThanks to Dr Chris Thomson and Karin Stahel for their feedback on early versions of Conc.\nAbove all, thanks to my family for their love, patience and kindness.",
    "crumbs": [
      "Introduction to Conc"
    ]
  },
  {
    "objectID": "index.html#development-status",
    "href": "index.html#development-status",
    "title": "Conc",
    "section": "Development Status",
    "text": "Development Status\nConc is in active development. It is currently released for beta testing. See the CHANGELOG for notes on releases and the Roadmap for planned updates.\nAlthough this is a Beta release, I’m currently using Conc for research and postgraduate teaching. I’m keen to support new users. If you have any questions, encounter hurdles using Conc or have feature requests, create an issue.",
    "crumbs": [
      "Introduction to Conc"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Conc",
    "section": "Installation",
    "text": "Installation\nInstalling Conc is simple. Below is the essential information if you want to use Conc. The installation page has more information. You can also install the development version of Conc, which may include new functionality and bug fixes. If you want to download sample corpora you will need to install optional dependencies. If you have an older computer with a pre-2013 CPU, you will probably need to install a version of Polars compiled for older machines, see the install page for details.\n\n1. Install via pip\nConc is tested with Python 3.10+. You can install Conc from pypi using this command:\npip install conc\nAdd the -U flag to upgrade if you are already running Conc.\n\n\n2. Install a spaCy model for tokenization\nConc uses a SpaCy language model for tokenization. After installing Conc, install a model. If you are working with English-language texts, install SpaCy’s small English model (which is Conc’s default) like this:\npython -m spacy download en_core_web_sm\nIf you are working with a different language or want to use a different ‘en’ model, check the SpaCy models documentation for the relevant model name.",
    "crumbs": [
      "Introduction to Conc"
    ]
  },
  {
    "objectID": "index.html#using-conc",
    "href": "index.html#using-conc",
    "title": "Conc",
    "section": "Using Conc",
    "text": "Using Conc\n\nGetting started\nA good place to start is the Get started with Conc tutorial, which demonstrates how to build a corpus and output Conc reports. There are also simple code recipes for common Conc tasks.\n\n\nConc Documentation\nThere is a dedicated Conc documentation site. This includes tutorials, examples demonstrating how to create reports for analysis, explanation of Conc functionality and its Corpus format, and a reference to Conc’s classes and methods. Here are links to the documentation site sections:\n\nTutorials to get you started with Conc\n\nThe Explanations section includes information on how Conc works, how to work with the Conc corpus format and Conc results with other Python libraries\n\nThe Conc API Reference provides detailed documentation of Conc classes and functions\n\nThe Development section gives information on Conc development, including a Roadmap and Developer’s Guide",
    "crumbs": [
      "Introduction to Conc"
    ]
  },
  {
    "objectID": "explanations/why.html",
    "href": "explanations/why.html",
    "title": "Why Conc?",
    "section": "",
    "text": "Conc originated in work for my PhD research back to 2013. I had been introduced to corpus linguistics by academics from the Linguistics department at the University of Canterbury. I quickly hit the limits of tools available at the time, which were not designed for large corpora and did not allow extension to develop new analytic approaches. I was doing my PhD after a number of years working as a software developer specialising in web-based applications. As part of my research, I developed a web-based corpus browser to implement analysis of a large corpus of New Zealand’s parliamentary debates. This had a MySQL backend, and combined server-side processing with an interactive web interface. With something built from the ground up, I was able to implement analysis approaches that were most relevant to my research and experiment and extend the techniques used.\nSince then, I have applied corpus analysis to research and teaching, and that is where Conc originated and has been developed. In around 2020 I developed a prototype version of Conc based on Numpy. As well as support for Jupyter notebook-based analysis, this was integrated with a web UI for other project team members to use to explore the large corpora we were collecting for the project.\nI have recently had the opportunity to take sabbatical leave for research. This has given me the opportunity to release Conc in for other researchers. I hope you find it useful for your research!",
    "crumbs": [
      "Explanations",
      "Why Conc?"
    ]
  },
  {
    "objectID": "explanations/why.html#where-does-conc-come-from",
    "href": "explanations/why.html#where-does-conc-come-from",
    "title": "Why Conc?",
    "section": "",
    "text": "Conc originated in work for my PhD research back to 2013. I had been introduced to corpus linguistics by academics from the Linguistics department at the University of Canterbury. I quickly hit the limits of tools available at the time, which were not designed for large corpora and did not allow extension to develop new analytic approaches. I was doing my PhD after a number of years working as a software developer specialising in web-based applications. As part of my research, I developed a web-based corpus browser to implement analysis of a large corpus of New Zealand’s parliamentary debates. This had a MySQL backend, and combined server-side processing with an interactive web interface. With something built from the ground up, I was able to implement analysis approaches that were most relevant to my research and experiment and extend the techniques used.\nSince then, I have applied corpus analysis to research and teaching, and that is where Conc originated and has been developed. In around 2020 I developed a prototype version of Conc based on Numpy. As well as support for Jupyter notebook-based analysis, this was integrated with a web UI for other project team members to use to explore the large corpora we were collecting for the project.\nI have recently had the opportunity to take sabbatical leave for research. This has given me the opportunity to release Conc in for other researchers. I hope you find it useful for your research!",
    "crumbs": [
      "Explanations",
      "Why Conc?"
    ]
  },
  {
    "objectID": "explanations/why.html#why-conc-jupyter-notebooks-for-corpus-analysis",
    "href": "explanations/why.html#why-conc-jupyter-notebooks-for-corpus-analysis",
    "title": "Why Conc?",
    "section": "Why Conc + Jupyter notebooks for corpus analysis?",
    "text": "Why Conc + Jupyter notebooks for corpus analysis?\nDoing corpus analysis using an open source library like Conc and Jupyter notebooks has a number of advantages:\n\nNotebooks provide a way to record the thinking that happens during the analysis process. Note-taking on analysis undertaken with desktop applications requires jumping between applications, copying output, parameters and other information from the desktop application to your note-taking medium. This is not a restriction of notebooks, because the code, result outputs, and notes can be recorded in one place. Unlike desktop applications, with a notebook you can record what you are doing through your research, what you have found and what you need to do.\n\nThe information stored in a notebook, which reflects the “thinking through analysis” described above, persists. This means you can leave your analysis mid-process in a form that can be easily continued at a later date.\n\nNotebooks provide a convenient way to share in-progress and completed analysis with other team members. Collaborators can run your code, add their own reporting, and add to the discussion of the results.\n\nNotebook-based analysis is reproducible analysis. Academic papers don’t typically allow for the detail of analysis to be represented that you’ve probably undertaken. Other researchers can run your code, get the same results, and step through your research process. They can also test your analysis, and change inputs and parameters to test out your analysis.\n\nWant to do something new? An open source library like Conc allows for interrogation of its inner workings, as well as modification and extension. Conc is built using nbdev, which is a notebook-based software development framework. Code is developed in notebooks, you can download these, inspect the code, and extend the functionality to your own requirements. It is a great platform to develop new approaches to corpus analysis.",
    "crumbs": [
      "Explanations",
      "Why Conc?"
    ]
  },
  {
    "objectID": "explanations/why.html#why-use-conc",
    "href": "explanations/why.html#why-use-conc",
    "title": "Why Conc?",
    "section": "Why use Conc?",
    "text": "Why use Conc?\n\nConc supports creation of corpora from different source types.\n\nConc handles large corpora.\n\nConc is fast!\n\nConc outputs tables and visualisations that are clear and interpretable.\n\nConc corpus and result formats are documented so you can continue your analysis using your own code, you can develop new techniques and ensure results are presented how you want.",
    "crumbs": [
      "Explanations",
      "Why Conc?"
    ]
  },
  {
    "objectID": "explanations/why.html#conc-screenshots",
    "href": "explanations/why.html#conc-screenshots",
    "title": "Why Conc?",
    "section": "Conc screenshots",
    "text": "Conc screenshots\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\n\n\nKeywords Report\n\n\n\n\n\n\n\n\n\nExample Concordance\n\n\n\n\n\n\n\nInteractive Concordance Plot",
    "crumbs": [
      "Explanations",
      "Why Conc?"
    ]
  },
  {
    "objectID": "explanations/why.html#conc-principles",
    "href": "explanations/why.html#conc-principles",
    "title": "Why Conc?",
    "section": "Conc Principles",
    "text": "Conc Principles\nThese principles are intended to guide the development of Conc:\n\nuse standard Python libraries for data processing, analysis and visualisation (i.e. Numpy, Scipy, Polars, Plotly)\nuse fast code libraries (i.e. Conc uses Polars vs Pandas) and fast data structures (e.g. Numpy arrays, columnar data stores)\n\nprovide clear and complete information when reporting results\n\npre-compute time-intensive and repeatedly used views of the data\n\nwork with smaller slices of the data where possible\n\ncache specific anaysis during a session to reduce computation for repeated calls\n\ndocument corpus representations so that they can be worked with directly\n\nallow researchers to work with Conc results and extend analysis using other Python libraries (e.g. output Pandas dataframes)\n\nmake use of meta-data to allow within-corpus comparisons and to provide context for analysis",
    "crumbs": [
      "Explanations",
      "Why Conc?"
    ]
  },
  {
    "objectID": "explanations/anatomy.html",
    "href": "explanations/anatomy.html",
    "title": "Anatomy of a corpus",
    "section": "",
    "text": "A Conc corpus is a directory containing files with specific names and formats to represent the data. This document provides an overview of the various files and what they contain. Here is the directory structure of an example Conc corpus:\n\n\n├── garden-party.corpus\n│   ├── tokens.parquet\n│   ├── spaces.parquet\n│   ├── README.md\n│   ├── puncts.parquet\n│   ├── corpus.json\n│   ├── vocab.parquet\n│   └── metadata.parquet\n\n\nNote: by default the library creates a directory with the .corpus suffix. The directory name is created automatically on build based on a slugified version of the corpus name you assigned.\nFor example, if you passed in the name:\nGarden Party Corpus\nThe directory will be:\ngarden-party.corpus\nThe directory can be renamed and still loaded. The .corpus extension is intended to make corpora on your filesystem easier to find or identify.\nTo distribute a corpus, send a zip of the directory for others to extract or just share the directory as-is.\nBelow is an overview of the files in a Conc corpus directory. The data can be accessed via Conc or accessed directly from the files.\n\n\n\n\n\n\n\n\nFile\nAccess via Conc\nDescription\n\n\n\n\nREADME.md\n-\nHuman readable information about the corpus to aide distribution\n\n\ncorpus.json\nspecific properties e.g. conc.token_count\nMachine readable information about the corpus, including name, description, various summary statistics, and models used to build the corpus\n\n\nvocab.parquet\ncorpus.vocab\nA table mapping token strings to token IDs and frequency information\n\n\ntokens.parquet\ncorpus.tokens\nA table with indices based on token positions used to query the corpus with tokens represented by numeric IDs\n\n\nmetadata.parquet\ncorpus.metadata\nA table with metadata for each document (if there is any)\n\n\nspaces.parquet\ncorpus.spaces\nA table to allow recovery of document spacing without the original texts\n\n\npuncts.parquet\ncorpus.puncts\nA table with punctuation positions\n\n\n\nBelow is more information about each file. You can obviously work with a corpus using Conc, but you can work with the processed corpus parquet and JSON files directly. Conc works with parquet files using the Polars library, but there are other libraries that support the format. Python provides native support for JSON, but there are more efficient libraries. Conc uses the msgspec library to read and write JSON.\nBrown Corpus\n\nAbout\nThis directory contains a corpus created using the Conc Python library.\n\n\nCorpus Information\nA Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.\nDate created: 2025-07-23 22:27:11\nDocument count: 500\nToken count: 1138566\nWord token count: 980144\nUnique tokens: 42930\nUnique word tokens: 42907\nConc Version Number: 0.1.10\nspaCy model: en_core_web_sm, version 3.8.0\n\n\nUsing this corpus\nConc can be installed via pip. The Conc documentation site has tutorials and detailed information to get you started with Conc or to work with the corpus data directly.\n\n\nCite Conc\nIf you use Conc in your work, please cite it as follows: Ford, G. (2025). Conc: a Python library for efficient corpus analysis (Version 0.1.10) [Computer software]. https://doi.org/10.5281/zenodo.16358752",
    "crumbs": [
      "Explanations",
      "Anatomy of a corpus"
    ]
  },
  {
    "objectID": "explanations/anatomy.html#introduction",
    "href": "explanations/anatomy.html#introduction",
    "title": "Anatomy of a corpus",
    "section": "",
    "text": "A Conc corpus is a directory containing files with specific names and formats to represent the data. This document provides an overview of the various files and what they contain. Here is the directory structure of an example Conc corpus:\n\n\n├── garden-party.corpus\n│   ├── tokens.parquet\n│   ├── spaces.parquet\n│   ├── README.md\n│   ├── puncts.parquet\n│   ├── corpus.json\n│   ├── vocab.parquet\n│   └── metadata.parquet\n\n\nNote: by default the library creates a directory with the .corpus suffix. The directory name is created automatically on build based on a slugified version of the corpus name you assigned.\nFor example, if you passed in the name:\nGarden Party Corpus\nThe directory will be:\ngarden-party.corpus\nThe directory can be renamed and still loaded. The .corpus extension is intended to make corpora on your filesystem easier to find or identify.\nTo distribute a corpus, send a zip of the directory for others to extract or just share the directory as-is.\nBelow is an overview of the files in a Conc corpus directory. The data can be accessed via Conc or accessed directly from the files.\n\n\n\n\n\n\n\n\nFile\nAccess via Conc\nDescription\n\n\n\n\nREADME.md\n-\nHuman readable information about the corpus to aide distribution\n\n\ncorpus.json\nspecific properties e.g. conc.token_count\nMachine readable information about the corpus, including name, description, various summary statistics, and models used to build the corpus\n\n\nvocab.parquet\ncorpus.vocab\nA table mapping token strings to token IDs and frequency information\n\n\ntokens.parquet\ncorpus.tokens\nA table with indices based on token positions used to query the corpus with tokens represented by numeric IDs\n\n\nmetadata.parquet\ncorpus.metadata\nA table with metadata for each document (if there is any)\n\n\nspaces.parquet\ncorpus.spaces\nA table to allow recovery of document spacing without the original texts\n\n\npuncts.parquet\ncorpus.puncts\nA table with punctuation positions\n\n\n\nBelow is more information about each file. You can obviously work with a corpus using Conc, but you can work with the processed corpus parquet and JSON files directly. Conc works with parquet files using the Polars library, but there are other libraries that support the format. Python provides native support for JSON, but there are more efficient libraries. Conc uses the msgspec library to read and write JSON.",
    "crumbs": [
      "Explanations",
      "Anatomy of a corpus"
    ]
  },
  {
    "objectID": "explanations/anatomy.html#notes-on-specific-conc-corpus-files-and-data-formats",
    "href": "explanations/anatomy.html#notes-on-specific-conc-corpus-files-and-data-formats",
    "title": "Anatomy of a corpus",
    "section": "Notes on specific Conc corpus files and data formats",
    "text": "Notes on specific Conc corpus files and data formats\nThe following information will help you if you want to work with the corpus data/files directly.\n\nREADME.md\nBelow is an example of the README.md file generated by the Conc.",
    "crumbs": [
      "Explanations",
      "Anatomy of a corpus"
    ]
  },
  {
    "objectID": "explanations/anatomy.html#brown-corpus",
    "href": "explanations/anatomy.html#brown-corpus",
    "title": "Anatomy of a corpus",
    "section": "Brown Corpus",
    "text": "Brown Corpus\n\nAbout\nThis directory contains a corpus created using the Conc Python library.\n\n\nCorpus Information\nA Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.\nDate created: 2025-07-23 22:27:11\nDocument count: 500\nToken count: 1138566\nWord token count: 980144\nUnique tokens: 42930\nUnique word tokens: 42907\nConc Version Number: 0.1.10\nspaCy model: en_core_web_sm, version 3.8.0\n\n\nUsing this corpus\nConc can be installed via pip. The Conc documentation site has tutorials and detailed information to get you started with Conc or to work with the corpus data directly.\n\n\nCite Conc\nIf you use Conc in your work, please cite it as follows: Ford, G. (2025). Conc: a Python library for efficient corpus analysis (Version 0.1.10) [Computer software]. https://doi.org/10.5281/zenodo.16358752",
    "crumbs": [
      "Explanations",
      "Anatomy of a corpus"
    ]
  },
  {
    "objectID": "tutorials/recipes.html",
    "href": "tutorials/recipes.html",
    "title": "Quick Conc Recipes",
    "section": "",
    "text": "The Get started with Conc tutorial (in progress) is a detailed step through Conc functionality. This page provides simple code recipes for common Conc tasks. See the Conc API Reference for information on available methods, functions and parameters.",
    "crumbs": [
      "Tutorials",
      "Quick Conc Recipes"
    ]
  },
  {
    "objectID": "tutorials/recipes.html#install-conc",
    "href": "tutorials/recipes.html#install-conc",
    "title": "Quick Conc Recipes",
    "section": "Install Conc",
    "text": "Install Conc\n\n\n\n\n\n\nBasic install\n\n\n\n\n\nSee the installation page if you have a pre-2013 device, want to install the latest development version, or want to install optional dependencies.\nConc is tested with Python 3.10+. Create a new environment (with venv, conda or similiar) and run the following command to install Conc in your terminal. If you working within a notebook environment, you can usually run commands in a code cell with an ! symbol before the command.\npip install conc\nThe Conc install process installs spaCy, but you will need a spaCy model. Once the package and its dependencies are installed, run the following command to download the English language model for spaCy. If you want to use a different language, consult the spaCy models page for information on available models.\npython -m spacy download en_core_web_sm",
    "crumbs": [
      "Tutorials",
      "Quick Conc Recipes"
    ]
  },
  {
    "objectID": "tutorials/recipes.html#building-or-loading-a-corpus-for-analysis",
    "href": "tutorials/recipes.html#building-or-loading-a-corpus-for-analysis",
    "title": "Quick Conc Recipes",
    "section": "Building or loading a corpus for analysis",
    "text": "Building or loading a corpus for analysis\n\n\n\n\n\n\nBuild a Conc corpus from text files (or a compressed archive of text files) and prepare to report on it\n\n\n\n\n\nThe example assumes you’ve set the name and description of your corpus, as well as the path_to_source_file and save_path variables. The path_to_source_file can be a directory of text files or a compressed archive of text files (i.e. .zip, .tar.gz).\n\nfrom conc.corpus import Corpus\nfrom conc.conc import Conc\n\n\ncorpus = Corpus(name=name, description=description).build_from_files(source_path = path_to_source_file, save_path = save_path)\ncorpus.summary()\nconc = Conc(corpus=corpus) # prerequisite for running reports\n\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nName\nGarden Party Corpus\n\n\nDescription\nA corpus of short stories from The Garden Party: and Other Stories by Katherine Mansfield. Texts downloaded from Project Gutenberg https://gutenberg.org/ and are in the public domain. The text files contain the short story without the title. https://github.com/ucdh/scraping-garden-party\n\n\nDate Created\n2025-07-06 12:11:27\n\n\nConc Version\n0.1.6\n\n\nCorpus Path\n/home/geoff/data/conc-test-corpora/garden-party.corpus\n\n\nDocument Count\n15\n\n\nToken Count\n74,664\n\n\nWord Token Count\n59,514\n\n\nUnique Tokens\n5,410\n\n\nUnique Word Tokens\n5,392\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a Conc corpus from a CSV (or a compressed CSV) and prepare to report on it\n\n\n\n\n\nThe example assumes you’ve set the name and description of your corpus, as well as the path_to_source_file and save_path variables. The path_to_source_file can be a .csv file or a compressed .csv.gz file. This example assumes the CSV has a column called text containing the text to be analysed and another column called source, which will be retained as metadata.\n\nfrom conc.corpus import Corpus\nfrom conc.conc import Conc\n\n\ncorpus = Corpus(name=name, description=description).build_from_csv(source_path=path_to_source_file, save_path=save_path, text_column='text', metadata_columns=['source', 'category'])\ncorpus.summary()\nconc = Conc(corpus=corpus) # prerequisite for running reports\n\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nName\nBrown Corpus\n\n\nDescription\nA Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.\n\n\nDate Created\n2025-07-06 12:11:34\n\n\nConc Version\n0.1.6\n\n\nCorpus Path\n/home/geoff/data/conc-test-corpora/brown.corpus\n\n\nDocument Count\n500\n\n\nToken Count\n1,138,566\n\n\nWord Token Count\n980,144\n\n\nUnique Tokens\n42,930\n\n\nUnique Word Tokens\n42,907\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoad a Conc corpus and prepare to report on it\n\n\n\n\n\nThe example assumes you’ve built a corpus previously with Conc and you’ve set the corpus_path (i.e. the corpus path created when you built your corpus).\n\nfrom conc.corpus import Corpus\nfrom conc.conc import Conc\n\n\ncorpus = Corpus().load(corpus_path=corpus_path)\ncorpus.summary()\nconc = Conc(corpus=corpus) # prerequisite for running reports\n\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nName\nGarden Party Corpus\n\n\nDescription\nA corpus of short stories from The Garden Party: and Other Stories by Katherine Mansfield. Texts downloaded from Project Gutenberg https://gutenberg.org/ and are in the public domain. The text files contain the short story without the title. https://github.com/ucdh/scraping-garden-party\n\n\nDate Created\n2025-07-06 12:11:27\n\n\nConc Version\n0.1.6\n\n\nCorpus Path\n/home/geoff/data/conc-test-corpora/garden-party.corpus\n\n\nDocument Count\n15\n\n\nToken Count\n74,664\n\n\nWord Token Count\n59,514\n\n\nUnique Tokens\n5,410\n\n\nUnique Word Tokens\n5,392",
    "crumbs": [
      "Tutorials",
      "Quick Conc Recipes"
    ]
  },
  {
    "objectID": "tutorials/recipes.html#building-and-loading-a-lightweight-corpus-representation-list-corpus-for-use-as-a-reference-corpus",
    "href": "tutorials/recipes.html#building-and-loading-a-lightweight-corpus-representation-list-corpus-for-use-as-a-reference-corpus",
    "title": "Quick Conc Recipes",
    "section": "Building and loading a lightweight corpus representation (list corpus) for use as a reference corpus",
    "text": "Building and loading a lightweight corpus representation (list corpus) for use as a reference corpus\nA Conc corpus created using the Corpus class includes a representation of the tokenised text of each document in the corpus. Conc also supports a lightweight corpus representation, referred to as a list corpus (see the ListCorpus class), which is intended for use as a reference corpus for keyness analysis.\n\n\n\n\n\n\nBuilding a list corpus\n\n\n\n\n\nTo build a list corpus, you need a Conc corpus. There are recipes for building a Conc corpus above.\nNote: It can be helpful to add standardize_word_token_punctuation_characters=True to Corpus.build_from_files or Corpus_build_from_csv when building a Conc corpus for use as a reference corpus. This will standardise the apostrophe character in frequent word tokens. The Keyness.keywords method has a parameter called handle_common_typographic_differences that is set to True by default which adjusts frequent tokens with apostrophes in the reference corpus to match the target corpus usage.\nOnce you have built a Conc corpus, a list corpus can be created using the ListCorpus.build_from_corpus method. A new directory will be created in save_path with the name of the source corpus. The created directory name ends with .listcorpus to differentiate a list corpus from the standard Conc corpus format. List corpora support methods to describe the corpus as shown in the example below. See the ListCorpus documentation for the full API and documentation on the format.\n\nfrom conc.listcorpus import ListCorpus\n\n\nlistcorpus = ListCorpus().build_from_corpus(source_corpus_path = path_to_brown_corpus, save_path = save_path)\nlistcorpus.summary()\n\n\n\n\n\n\n\nList Corpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nname\nBrown Corpus\n\n\ndescription\nA Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.\n\n\ndate_created\n2025-07-06 12:11:34\n\n\nconc_version\n0.1.6\n\n\ncorpus_path\n/home/geoff/data/conc-test-corpora/brown.listcorpus\n\n\ndocument_count\n500\n\n\ntoken_count\n1,138,566\n\n\nword_token_count\n980,144\n\n\nunique_tokens\n42,930\n\n\nunique_word_tokens\n42,907\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoad a list corpus\n\n\n\n\n\n\nlistcorpus = ListCorpus().load(path_to_brown_listcorpus)\nlistcorpus.summary()\n\n\n\n\n\n\n\nList Corpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nname\nBrown Corpus\n\n\ndescription\nA Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.\n\n\ndate_created\n2025-07-06 12:11:34\n\n\nconc_version\n0.1.6\n\n\ncorpus_path\n/home/geoff/data/conc-test-corpora/brown.listcorpus\n\n\ndocument_count\n500\n\n\ntoken_count\n1,138,566\n\n\nword_token_count\n980,144\n\n\nunique_tokens\n42,930\n\n\nunique_word_tokens\n42,907",
    "crumbs": [
      "Tutorials",
      "Quick Conc Recipes"
    ]
  },
  {
    "objectID": "tutorials/recipes.html#reports-for-corpus-analysis",
    "href": "tutorials/recipes.html#reports-for-corpus-analysis",
    "title": "Quick Conc Recipes",
    "section": "Reports for corpus analysis",
    "text": "Reports for corpus analysis\nThe example snippets below assume you’ve built or loaded a corpus and prepared it for reporting (see above).\n\n\n\n\n\n\nFrequency report\n\n\n\n\n\n\nconc.frequencies().display()\n\n\n\n\n\n\n\nFrequencies\n\n\nFrequencies of word tokens, Garden Party Corpus\n\n\nRank\nToken\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe\n2,911\n489.13\n\n\n2\nand\n1,798\n302.11\n\n\n3\na\n1,407\n236.41\n\n\n4\nto\n1,376\n231.21\n\n\n5\nshe\n1,171\n196.76\n\n\n6\nwas\n1,102\n185.17\n\n\n7\nit\n1,021\n171.56\n\n\n8\nher\n937\n157.44\n\n\n9\nof\n908\n152.57\n\n\n10\ni\n719\n120.81\n\n\n11\nhe\n718\n120.64\n\n\n12\nin\n683\n114.76\n\n\n13\nthat\n643\n108.04\n\n\n14\nyou\n642\n107.87\n\n\n15\n’s\n524\n88.05\n\n\n16\nn’t\n522\n87.71\n\n\n17\nsaid\n514\n86.37\n\n\n18\non\n504\n84.69\n\n\n19\nhad\n469\n78.80\n\n\n20\nhis\n440\n73.93\n\n\n\nReport based on word tokens\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens: 59,514\n\n\nUnique word tokens: 5,392\n\n\nShowing 20 rows\n\n\nPage 1 of 270\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNgram frequencies report\n\n\n\n\n\n\nconc.ngram_frequencies(ngram_length = 2).display()\n\n\n\n\n\n\n\nNgram Frequencies\n\n\nGarden Party Corpus\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nit was\n247\n41.50\n\n\n2\nin the\n214\n35.96\n\n\n3\non the\n183\n30.75\n\n\n4\nof the\n156\n26.21\n\n\n5\nto the\n139\n23.36\n\n\n6\nat the\n133\n22.35\n\n\n7\nshe was\n132\n22.18\n\n\n8\nand the\n124\n20.84\n\n\n9\nit ’s\n120\n20.16\n\n\n10\ndo n’t\n105\n17.64\n\n\n11\nthey were\n104\n17.47\n\n\n12\nhe was\n100\n16.80\n\n\n13\nshe had\n95\n15.96\n\n\n14\ncould n’t\n88\n14.79\n\n\n15\nand she\n88\n14.79\n\n\n16\nto be\n88\n14.79\n\n\n17\nshe said\n87\n14.62\n\n\n18\nthere was\n84\n14.11\n\n\n19\na little\n83\n13.95\n\n\n20\nout of\n83\n13.95\n\n\n\nReport based on word tokens\n\n\nNgram length: 2\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 24,351\n\n\nTotal ngrams: 47,706\n\n\nShowing 20 rows\n\n\nPage 1 of 1218\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeywords report\n\n\n\n\n\nFor a keywords report you will need another corpus to compare against. This example assumes you’ve already built that and have defined the reference_corpus_path variable. When you load the reference corpus, set it to reference_corpus or some other variable name to help you distinguish it from the corpus you are reporting on.\n\nreference_corpus = Corpus().load(corpus_path=reference_corpus_path)\nconc.set_reference_corpus(reference_corpus)\nconc.keywords(min_document_frequency = 5, min_document_frequency_reference = 5).display()\n\n\n\n\n\n\n\nKeywords\n\n\nTarget corpus: Garden Party Corpus, Reference corpus: Brown Corpus\n\n\nRank\nToken\nFrequency\nFrequency Reference\nNormalized Frequency\nNormalized Frequency Reference\nRelative Risk\nLog Ratio\nLog Likelihood\n\n\n\n\n1\nbye\n25\n7\n4.20\n0.07\n58.82\n5.88\n110.23\n\n\n2\nvelvet\n14\n5\n2.35\n0.05\n46.11\n5.53\n58.78\n\n\n3\nshone\n13\n5\n2.18\n0.05\n42.82\n5.42\n53.69\n\n\n4\nqueer\n15\n6\n2.52\n0.06\n41.17\n5.36\n61.39\n\n\n5\ngloves\n17\n7\n2.86\n0.07\n40.00\n5.32\n69.11\n\n\n6\ncried\n59\n26\n9.91\n0.27\n37.37\n5.22\n235.92\n\n\n7\ndarling\n36\n18\n6.05\n0.18\n32.94\n5.04\n139.33\n\n\n8\nfaintly\n14\n7\n2.35\n0.07\n32.94\n5.04\n54.18\n\n\n9\noh\n149\n93\n25.04\n0.95\n26.39\n4.72\n540.97\n\n\n10\nhandkerchief\n14\n9\n2.35\n0.09\n25.62\n4.68\n50.36\n\n\n11\ndear\n78\n54\n13.11\n0.55\n23.79\n4.57\n273.99\n\n\n12\nbreathed\n13\n9\n2.18\n0.09\n23.79\n4.57\n45.67\n\n\n13\nawful\n23\n17\n3.86\n0.17\n22.28\n4.48\n79.04\n\n\n14\nah\n26\n20\n4.37\n0.20\n21.41\n4.42\n88.12\n\n\n15\nbreast\n14\n11\n2.35\n0.11\n20.96\n4.39\n47.09\n\n\n16\ndashed\n10\n8\n1.68\n0.08\n20.59\n4.36\n33.42\n\n\n17\ngasped\n6\n5\n1.01\n0.05\n19.76\n4.30\n19.76\n\n\n18\nparted\n6\n5\n1.01\n0.05\n19.76\n4.30\n19.76\n\n\n19\ntimid\n6\n5\n1.01\n0.05\n19.76\n4.30\n19.76\n\n\n20\nsigh\n13\n11\n2.18\n0.11\n19.46\n4.28\n42.56\n\n\n\nReport based on word tokens\n\n\nFiltered tokens by minimum document frequency in target corpus (5), minimum document frequency in reference corpus (5)\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens in target corpus: 59,514\n\n\nTotal word tokens in reference corpus: 980,144\n\n\nKeywords: 825\n\n\nShowing 20 rows\n\n\nPage 1 of 42\n\n\n\n\n\n\n\n\n\nNote: Conc’s lightweight list corpus format can be used as a reference corpus for keyness analysis. There is a recipe above to build a list corpus.\n\nreference_corpus = ListCorpus().load(corpus_path=path_to_listcorpus)\nconc.set_reference_corpus(reference_corpus)\nconc.keywords(min_document_frequency = 5, min_document_frequency_reference = 5).display()\n\n\n\n\n\n\n\nKeywords\n\n\nTarget corpus: Garden Party Corpus, Reference corpus: Brown Corpus\n\n\nRank\nToken\nFrequency\nFrequency Reference\nNormalized Frequency\nNormalized Frequency Reference\nRelative Risk\nLog Ratio\nLog Likelihood\n\n\n\n\n1\nbye\n25\n7\n4.20\n0.07\n58.82\n5.88\n110.23\n\n\n2\nvelvet\n14\n5\n2.35\n0.05\n46.11\n5.53\n58.78\n\n\n3\nshone\n13\n5\n2.18\n0.05\n42.82\n5.42\n53.69\n\n\n4\nqueer\n15\n6\n2.52\n0.06\n41.17\n5.36\n61.39\n\n\n5\ngloves\n17\n7\n2.86\n0.07\n40.00\n5.32\n69.11\n\n\n6\ncried\n59\n26\n9.91\n0.27\n37.37\n5.22\n235.92\n\n\n7\ndarling\n36\n18\n6.05\n0.18\n32.94\n5.04\n139.33\n\n\n8\nfaintly\n14\n7\n2.35\n0.07\n32.94\n5.04\n54.18\n\n\n9\noh\n149\n93\n25.04\n0.95\n26.39\n4.72\n540.97\n\n\n10\nhandkerchief\n14\n9\n2.35\n0.09\n25.62\n4.68\n50.36\n\n\n11\ndear\n78\n54\n13.11\n0.55\n23.79\n4.57\n273.99\n\n\n12\nbreathed\n13\n9\n2.18\n0.09\n23.79\n4.57\n45.67\n\n\n13\nawful\n23\n17\n3.86\n0.17\n22.28\n4.48\n79.04\n\n\n14\nah\n26\n20\n4.37\n0.20\n21.41\n4.42\n88.12\n\n\n15\nbreast\n14\n11\n2.35\n0.11\n20.96\n4.39\n47.09\n\n\n16\ndashed\n10\n8\n1.68\n0.08\n20.59\n4.36\n33.42\n\n\n17\ngasped\n6\n5\n1.01\n0.05\n19.76\n4.30\n19.76\n\n\n18\nparted\n6\n5\n1.01\n0.05\n19.76\n4.30\n19.76\n\n\n19\ntimid\n6\n5\n1.01\n0.05\n19.76\n4.30\n19.76\n\n\n20\nsigh\n13\n11\n2.18\n0.11\n19.46\n4.28\n42.56\n\n\n\nReport based on word tokens\n\n\nFiltered tokens by minimum document frequency in target corpus (5), minimum document frequency in reference corpus (5)\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens in target corpus: 59,514\n\n\nTotal word tokens in reference corpus: 980,144\n\n\nKeywords: 825\n\n\nShowing 20 rows\n\n\nPage 1 of 42\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollocates report\n\n\n\n\n\n\nconc.collocates('could').display()\n\n\n\n\n\n\n\nCollocates of \"could\"\n\n\nGarden Party Corpus\n\n\nRank\nToken\nCollocate Frequency\nFrequency\nLogdice\nLog Likelihood\n\n\n\n\n1\nn’t\n111\n522\n12.28\n233.85\n\n\n2\nhave\n35\n240\n11.33\n50.03\n\n\n3\nshe\n94\n1,171\n11.13\n52.86\n\n\n4\nhe\n55\n718\n10.93\n27.90\n\n\n5\nthey\n35\n398\n10.89\n23.65\n\n\n6\nit\n71\n1,021\n10.89\n28.44\n\n\n7\nbe\n26\n251\n10.86\n23.35\n\n\n8\nhelp\n12\n27\n10.71\n44.49\n\n\n9\nwhat\n23\n270\n10.63\n14.61\n\n\n10\nnot\n19\n229\n10.48\n11.45\n\n\n11\nbut\n25\n417\n10.36\n6.43\n\n\n12\nhow\n13\n126\n10.32\n11.60\n\n\n13\ncould\n16\n207\n10.31\n107.37\n\n\n14\nwhy\n11\n99\n10.20\n11.00\n\n\n15\nhardly\n8\n17\n10.19\n30.81\n\n\n16\ndo\n16\n242\n10.19\n5.58\n\n\n17\nunderstand\n8\n19\n10.18\n28.60\n\n\n18\nthat\n30\n643\n10.18\n2.39\n\n\n19\nwas\n44\n1,102\n10.11\n0.79\n\n\n20\nno\n13\n181\n10.10\n5.66\n\n\n\nReport based on word tokens\n\n\nContext tokens left: 5, context tokens right: 5\n\n\nFiltered tokens by minimum collocation frequency (5)\n\n\nUnique collocates: 83\n\n\nShowing 20 rows\n\n\nPage 1 of 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcordance\n\n\n\n\n\n\nconc.concordance('could', order = '1R2R3R', context_length = 10).display()\n\n\n\n\n\n\n\nConcordance for \"could\"\n\n\nGarden Party Corpus, Context tokens: 10, Order: 1R2R3R\n\n\nDoc Id\nLeft\nNode\nRight\n\n\n\n\n4\nherself and got as close to the sea as she\ncould\n, and sung something , something she had made up\n\n\n13\n’ll not be a minute . ” And before he\ncould\nanswer she was gone . He had half a mind\n\n\n4\nuntil I ’ve had something . Do you think we\ncould\nask Kate for two cups of hot water ? ”\n\n\n2\n, and there will be no time to explain what\ncould\nbe explained so simply .... But to - night it\n\n\n1\nHere ’s this huge house and garden . Surely you\ncould\nbe happy in — in — appreciating it for a\n\n\n12\naway from the Listening Ear . Good Heavens , what\ncould\nbe more tragic than that lament ! Every note was\n\n\n1\neven a successful , established , big paying concern —\ncould\nbe played with . A man had either to put\n\n\n8\nplay . It was exactly like a play . Who\ncould\nbelieve the sky at the back was n’t painted ?\n\n\n13\n. Was her luggage ready ? In that case they\ncould\ncut off sharp with her cabin luggage and let the\n\n\n2\nbut , after all , they ’re boys . I\ncould\ncut off to sea , or get a job up\n\n\n2\none leg over . But which leg ? She never\ncould\ndecide . And when she did finally put one leg\n\n\n10\nloved having to arrange things ; she always felt she\ncould\ndo it so much better than anybody else . Four\n\n\n2\nsitting there in the washhouse ; it was all they\ncould\ndo not to burst into a little chorus of animals\n\n\n11\nto practise . ” Oh , it was all I\ncould\ndo not to burst out crying . I went over\n\n\n7\n. But after supper they were all so tired they\ncould\ndo nothing but yawn until it was late enough to\n\n\n14\nastonished Fenella . “ You did n’t think your grandma\ncould\ndo that , did you ? ” said she .\n\n\n2\nwanted , somehow , to celebrate the fact that they\ncould\ndo what they liked now . There was no man\n\n\n9\nbent over her . This was such bliss that he\ncould\ndream no further . But it gave him the courage\n\n\n6\naway — anywhere , as though by walking away he\ncould\nescape .... It was cold in the street . There\n\n\n9\nto Anne . “ Anne , do you think you\ncould\never care for me ? ” It was done .\n\n\n\nTotal Concordance Lines: 207\n\n\nTotal Documents: 15\n\n\nShowing 20 lines\n\n\nPage 1 of 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcordance with lines filtered to show only those with a specific context string\n\n\n\n\n\n\nconc.concordance('could', order = '1R2R3R', context_length = 10, filter_context_str = 'she', filter_context_length = 5).display()\n\n\n\n\n\n\n\nConcordance for \"could\"\n\n\nGarden Party Corpus, Context tokens: 10, Order: 1R2R3R\n\n\nDoc Id\nLeft\nNode\nRight\n\n\n\n\n4\nherself and got as close to the sea as she\ncould\n, and sung something , something she had made up\n\n\n13\n’ll not be a minute . ” And before he\ncould\nanswer she was gone . He had half a mind\n\n\n2\none leg over . But which leg ? She never\ncould\ndecide . And when she did finally put one leg\n\n\n10\nloved having to arrange things ; she always felt she\ncould\ndo it so much better than anybody else . Four\n\n\n5\nhave to dance , out of politeness , until she\ncould\nfind Meg . Very stiffly she walked into the middle\n\n\n15\nand a tiny horn filled with fresh strawberries . She\ncould\nhardly bear to watch him . But just as the\n\n\n4\nguest in case — ” “ Oh , but she\ncould\nhardly expect to be paid ! ” cried Constantia .\n\n\n4\nour dear father _ so _ much , ” she\ncould\nhave cried if she ’d wanted to . “ Have\n\n\n6\n? Was n’t there anywhere in the world where she\ncould\nhave her cry out — at last ? Ma Parker\n\n\n10\nquite warm . A warm little silver star . She\ncould\nhave kissed it . The front door bell pealed ,\n\n\n7\ninduced William ... ? How extraordinary it was .... What\ncould\nhave made him ... ? She felt confused , more\n\n\n13\nto go chasing after the ship ’s doctor ? She\ncould\nhave sent a note from the hotel even if the\n\n\n8\nbiting its tail just by her left ear . She\ncould\nhave taken it off and laid it on her lap\n\n\n6\nher . Oh , was n’t there anywhere where she\ncould\nhide and keep herself to herself and stay as long\n\n\n4\nher Buddha . Oh , what was it , what\ncould\nit be ? And yet she had always felt there\n\n\n13\naffair had been urgent . Urgent ? Did it —\ncould\nit mean that she had been ill on the voyage\n\n\n10\nlong black velvet ribbon . Never had she imagined she\ncould\nlook like that . Is mother right ? she thought\n\n\n12\nwould have to leave the school , too . She\ncould\nnever face the Science Mistress or the girls after it\n\n\n2\nher real grudge against life ; that was what she\ncould\nnot understand . That was the question she asked and\n\n\n4\nsuch a strange smile ; she looked different . She\ncould\nn’t be going to cry . “ Jug , Jug\n\n\n\nConcordance lines restricted to those containing \"she\" in span 5 tokens to left, 5 tokens to right\n\n\nTotal Concordance Lines: 72\n\n\nTotal Documents: 13\n\n\nShowing 20 lines\n\n\nPage 1 of 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcordance plot\n\n\n\n\n\n\nconc.concordance_plot('he').display()\n\n\nConcordance Plot for \"he\"Garden Party CorpusTotal Documents: 15Total Concordance Lines: 718Page 1 of 2\n    \n    \n    \n    \n\n\n\n\n\n\n\n\n\n\n\nYou can still output individual texts (and access the text and tokens programmatically)! Here is how …\n\n\n\n\n\nThe texts are accessible via the Corpus object. If you display a text (as below) it will show any available metadata. Add show_metadata=False to just show the text. If you leave off the max_tokens parameter, it will display the entire text. Check the Text class documentation for information on available parameters to control reflow and wrapping of text via the display method.\n\ncorpus.text(2).display(max_tokens = 300)\n\n\n    \n\n\n\n\n\nMetadata\n\n\n\n\n\nAttribute\nValue\n\n\n\n\ndocument_id\n2\n\n\nfile\nat-the-bay.txt\n\n\n\n\n\n\n        I\n\nVery early morning. The sun was not yet risen, and the whole of\nCrescent Bay was hidden under a white sea-mist. The big bush-covered\nhills at the back were smothered. You could not see where they ended\nand the paddocks and bungalows began. The sandy road was gone and the\npaddocks and bungalows the other side of it; there were no white dunes\ncovered with reddish grass beyond them; there was nothing to mark which\nwas beach and where was the sea. A heavy dew had fallen. The grass was\nblue. Big drops hung on the bushes and just did not fall; the silvery,\nfluffy toi-toi was limp on its long stalks, and all the marigolds and\nthe pinks in the bungalow gardens were bowed to the earth with wetness.\nDrenched were the cold fuchsias, round pearls of dew lay on the flat\nnasturtium leaves. It looked as though the sea had beaten up softly in\nthe darkness, as though one immense wave had come rippling,\nrippling—how far? Perhaps if you had waked up in the middle of the\nnight you might have seen a big fish flicking in at the window and gone\nagain....\n\nAh-Aah! sounded the sleepy sea. And from the bush there came the sound\nof little streams flowing, quickly, lightly, slipping between the\nsmooth stones, gushing into ferny basins and out again; and there was\nthe splashing of big drops on large leaves, and something else—…\n[300 of 18238 tokens]\n\n\nTexts are also accessible as strings if you want to work with them like that …\n\ntext = corpus.text(1).as_string(max_tokens = 50)\nprint(text)\n\nThat evening for the first time in his life, as he pressed through the\nswing door and descended the three broad steps to the pavement, old Mr.\nNeave felt he was too old for the spring. Spring—warm, eager,\nrestless—\n\n\nor as tokens (this example loops through all texts in the corpus and displays the first 10 tokens) …\n\nfor i in range(1, corpus.document_count + 1):\n    tokens = corpus.text(i).as_tokens() # this is all the tokens\n    print(tokens[:10]) # slicing to first 10 tokens\n\n['That', 'evening', 'for', 'the', 'first', 'time', 'in', 'his', 'life', ',']\n['I', '\\r\\n\\r\\n', 'Very', 'early', 'morning', '.', 'The', 'sun', 'was', 'not']\n['A', 'stout', 'man', 'with', 'a', 'pink', 'face', 'wears', 'dingy', 'white']\n['I', '\\r\\n\\r\\n', 'The', 'week', 'after', 'was', 'one', 'of', 'the', 'busiest']\n['Exactly', 'when', 'the', 'ball', 'began', 'Leila', 'would', 'have', 'found', 'it']\n['When', 'the', 'literary', 'gentleman', ',', 'whose', 'flat', 'old', 'Ma', 'Parker']\n['On', 'his', 'way', 'to', 'the', 'station', 'William', 'remembered', 'with', 'a']\n['Although', 'it', 'was', 'so', 'brilliantly', 'fine', '—', 'the', 'blue', 'sky']\n['Of', 'course', 'he', 'knew', '—', 'no', 'man', 'better', '—', 'that']\n['And', 'after', 'all', 'the', 'weather', 'was', 'ideal', '.', 'They', 'could']\n['_', 'Eleven', 'o’clock', '.', 'A', 'knock', 'at', 'the', 'door', '.']\n['With', 'despair', '—', 'cold', ',', 'sharp', 'despair', '—', 'buried', 'deep']\n['It', 'seemed', 'to', 'the', 'little', 'crowd', 'on', 'the', 'wharf', 'that']\n['The', 'Picton', 'boat', 'was', 'due', 'to', 'leave', 'at', 'half', '-']\n['In', 'her', 'blue', 'dress', ',', 'with', 'her', 'cheeks', 'lightly', 'flushed']",
    "crumbs": [
      "Tutorials",
      "Quick Conc Recipes"
    ]
  },
  {
    "objectID": "tutorials/recipes.html#working-with-conc-results",
    "href": "tutorials/recipes.html#working-with-conc-results",
    "title": "Quick Conc Recipes",
    "section": "Working with Conc results",
    "text": "Working with Conc results\n\n\n\n\n\n\nAccessing and working with Conc results\n\n\n\n\n\nIn the examples above we run reports and use .display() to output a report. You can access report data directly as a Polars dataframe using .to_frame(). This means you can work with the data in Polars to further filter the results or extend analysis.\n\n# same specific tokens to restrict the results below to\npronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'mine', 'yours', \n            'his', 'hers', 'its', 'ours', 'yours', 'theirs']\n\n# retrieve keyword report for these pronouns, which has normalized frequencies for each corpus\n# the next line returns the result as a Polars dataframe using the `to_frame` method\ndf = conc.keywords(restrict_tokens = pronouns).to_frame()\n\n\n# now go for it - work with the Polars dataframe\n# e.g. show a table of results showing specific columns, display top 10 results ordered by normalized frequency descending\ndf.select(['token', 'normalized_frequency', 'normalized_frequency_reference']).sort('normalized_frequency', descending=True).head(10)\n\n\n\n\n\n\ntoken\nnormalized_frequency\nnormalized_frequency_reference\n\n\n\n\n\"she\"\n196.760426\n21.01732\n\n\n\"it\"\n171.556272\n71.326254\n\n\n\"her\"\n157.441946\n29.454856\n\n\n\"i\"\n120.81191\n44.585285\n\n\n\"he\"\n120.643882\n69.091889\n\n\n\"you\"\n107.873778\n33.311432\n\n\n\"his\"\n73.932184\n66.551446\n\n\n\"they\"\n66.875021\n28.995739\n\n\n\"them\"\n35.285815\n18.232015\n\n\n\"him\"\n32.093289\n26.720564\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Conc results using Pandas\n\n\n\n\n\nYou are not restricted to Conc for your analysis. Conc report data can be exported to other formats. For example, although Conc uses Polars internally for its efficiency, you can convert report results into a Pandas dataframe, which is flexible and interoperable with many Python libraries for data analysis. Here is an example …\n\nimport pandas as pd # Conc does not install Pandas - so if you haven't already, install it with \"pip install pandas\"\n\n\n# same specific tokens to restrict the results below to\npronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'mine', 'yours', \n            'his', 'hers', 'its', 'ours', 'yours', 'theirs']\n\n# retrieve keyword report for these pronouns, which has normalized frequencies for each corpus\n# the next line returns the result as a Polars dataframe, which is then converted to a Pandas dataframe\ndf = conc.keywords(restrict_tokens = pronouns).to_frame().to_pandas()\n\n\n# now go for it - do the pandas stuff you are familiar with ...\n# e.g. show a table of results showing specific columns, display top 10 results ordered by normalized frequency descending\n# note you could do this in Polars (see above), but this is assuming you are more familiar with Pandas\ndf.sort_values(by='normalized_frequency', ascending=False)[['token', 'normalized_frequency', 'normalized_frequency_reference']].head(10)\n\n\n\n\n\n\n\n\ntoken\nnormalized_frequency\nnormalized_frequency_reference\n\n\n\n\n0\nshe\n196.760426\n21.017320\n\n\n7\nit\n171.556272\n71.326254\n\n\n2\nher\n157.441946\n29.454856\n\n\n6\ni\n120.811910\n44.585285\n\n\n12\nhe\n120.643882\n69.091889\n\n\n5\nyou\n107.873778\n33.311432\n\n\n15\nhis\n73.932184\n66.551446\n\n\n9\nthey\n66.875021\n28.995739\n\n\n10\nthem\n35.285815\n18.232015\n\n\n14\nhim\n32.093289\n26.720564\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWant to work with Conc results using other software/libraries? Exporting Conc results to CSV, JSON and other formats\n\n\n\n\n\nIf you want to work with Conc results in other libraries or software, you can write the results to an interoperable data format. Conc results can be accessed as Polars dataframes, which can be exported to CSV or JSON formats. See the Polars input/output documentation for other formats supported, including Parquet, newline-delimited JSON, Excel and more.\n\n# same specific tokens to restrict the results below to\npronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'mine', 'yours', \n            'his', 'hers', 'its', 'ours', 'yours', 'theirs']\n\n# retrieve keyword report for these pronouns, which has normalized frequencies for each corpus\n# the next line returns the result as a Polars dataframe using the `to_frame` method\ndf = conc.keywords(restrict_tokens = pronouns).to_frame()\n\nThe result in df is currently a Polars dataframe, write to CSV …\n\ndf.write_csv(f'pronoun-results.csv')\n\nHere is how to write the results to a JSON file …\n\ndf.write_json(f'pronoun-results.json')",
    "crumbs": [
      "Tutorials",
      "Quick Conc Recipes"
    ]
  },
  {
    "objectID": "tutorials/recipes.html#working-with-conc-corpus-data",
    "href": "tutorials/recipes.html#working-with-conc-corpus-data",
    "title": "Quick Conc Recipes",
    "section": "Working with Conc corpus data",
    "text": "Working with Conc corpus data\nIf Conc does not do something you want, you can work with a Conc corpus data directly.\n\n\n\n\n\n\nWorking with a loaded Corpus\n\n\n\n\n\nWhen you build or load a corpus the token, vocab and other data is accessible via the Corpus object. These could be very large and are not loaded into memory, but they can be queried via Polars Lazy API. Read about the Anatomy of a Conc Corpus for more information. Here is an example …\n\nimport polars as pl\n\n\n# e.g. filter vocab by tokens ending in 'ing'\ncorpus.vocab.filter(pl.col('token').str.ends_with('ing')).head(5).collect(engine='streaming')\n\n\n\n\n\n\nrank\ntokens_sort_order\ntoken_id\ntoken\nfrequency_lower\nfrequency_orth\nis_punct\nis_space\n\n\n\n\n99\n4979\n5920\n\"something\"\n100\n96\nfalse\nfalse\n\n\n171\n2275\n4862\n\"going\"\n59\n59\nfalse\nfalse\n\n\n205\n3206\n5170\n\"looking\"\n48\n46\nfalse\nfalse\n\n\n214\n5509\n830\n\"thing\"\n44\n44\nfalse\nfalse\n\n\n236\n3650\n4679\n\"nothing\"\n51\n41\nfalse\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Conc corpus data files directly\n\n\n\n\n\nThere are a number of different data files that make up a Conc corpus. These can be queried directly if required. Read about the Anatomy of a Conc Corpus for an explanation of the format and data. Here is an example …\n\nimport polars as pl\n\n\n# e.g. lazy query to get tokens from vocab ending in 'ing'\ndisplay(pl.scan_parquet(f'{save_path}garden-party.corpus/vocab.parquet').filter(pl.col('token').str.ends_with('ing')).head(5).collect(engine='streaming'))\n\n\n\n\n\n\nrank\ntokens_sort_order\ntoken_id\ntoken\nfrequency_lower\nfrequency_orth\nis_punct\nis_space\n\n\n\n\n99\n4979\n5920\n\"something\"\n100\n96\nfalse\nfalse\n\n\n171\n2275\n4862\n\"going\"\n59\n59\nfalse\nfalse\n\n\n205\n3206\n5170\n\"looking\"\n48\n46\nfalse\nfalse\n\n\n214\n5509\n830\n\"thing\"\n44\n44\nfalse\nfalse\n\n\n236\n3650\n4679\n\"nothing\"\n51\n41\nfalse\nfalse",
    "crumbs": [
      "Tutorials",
      "Quick Conc Recipes"
    ]
  },
  {
    "objectID": "tutorials/install.html",
    "href": "tutorials/install.html",
    "title": "Installing Conc",
    "section": "",
    "text": "Conc is tested with Python 3.10+. Before installing Conc, create a new environment (with venv, conda or similiar).",
    "crumbs": [
      "Tutorials",
      "Installing Conc"
    ]
  },
  {
    "objectID": "tutorials/install.html#install-via-pip",
    "href": "tutorials/install.html#install-via-pip",
    "title": "Installing Conc",
    "section": "Install via pip",
    "text": "Install via pip\nYou can install Conc from pypi using this command:\npip install conc\nAdd the -U flag to upgrade if you are already running Conc.",
    "crumbs": [
      "Tutorials",
      "Installing Conc"
    ]
  },
  {
    "objectID": "tutorials/install.html#install-a-spacy-model-for-tokenization",
    "href": "tutorials/install.html#install-a-spacy-model-for-tokenization",
    "title": "Installing Conc",
    "section": "Install a spaCy model for tokenization",
    "text": "Install a spaCy model for tokenization\nConc uses a SpaCy language model for tokenization. After installing Conc, install a model. If you are working with English-language texts, install SpaCy’s small English model (which is Conc’s default) like this:\npython -m spacy download en_core_web_sm\nIf you are working with a different language or want to use a different ‘en’ model, check the SpaCy models documentation for the relevant model name.",
    "crumbs": [
      "Tutorials",
      "Installing Conc"
    ]
  },
  {
    "objectID": "tutorials/install.html#install-the-development-version",
    "href": "tutorials/install.html#install-the-development-version",
    "title": "Installing Conc",
    "section": "Install the development version",
    "text": "Install the development version\nThe Github site may be ahead of the Pypi version, so for latest functionality install from Github. The Github code is pre-release and may change. The documentation reflects the most recent functionality.\nTo install the pre-release/development version of Conc, which will be ahead of the version on Pypi, you can install from the repository:\npip install git+https://github.com/polsci/conc.git",
    "crumbs": [
      "Tutorials",
      "Installing Conc"
    ]
  },
  {
    "objectID": "tutorials/install.html#install-optional-dependencies",
    "href": "tutorials/install.html#install-optional-dependencies",
    "title": "Installing Conc",
    "section": "Install optional dependencies",
    "text": "Install optional dependencies\nConc has some optional dependencies you can install to download source texts to create sample corpora. These are primarily intended for creating corpora for development. To minimize Conc’s requirements these are not installed by default. If you want to get sample corpora to test out Conc’s functionality you can install these with the following command.\npip install nltk datasets",
    "crumbs": [
      "Tutorials",
      "Installing Conc"
    ]
  },
  {
    "objectID": "tutorials/install.html#pre-2013-cpu-install-polars-with-support-for-older-machines",
    "href": "tutorials/install.html#pre-2013-cpu-install-polars-with-support-for-older-machines",
    "title": "Installing Conc",
    "section": "Pre-2013 CPU? Install Polars with support for older machines",
    "text": "Pre-2013 CPU? Install Polars with support for older machines\nPolars is optimized for modern CPUs with support for AVX2 instructions. If you get kernel crashes running Conc on an older machine (probably pre-2013), this is likely to be an issue with Polars. Polars has an alternate installation option to support older machines, which installs a Polars build compiled without AVX2 support. Replace the standard Polars package with the legacy-support package to use Conc on older machines.\npip uninstall polars\npip install polars-lts-cpu==1.30.0",
    "crumbs": [
      "Tutorials",
      "Installing Conc"
    ]
  },
  {
    "objectID": "api/keyness.html",
    "href": "api/keyness.html",
    "title": "keyness",
    "section": "",
    "text": "Conc implements Log Ratio, which is a keyness measure introduced by Andrew Hardie in this “informal introduction”. In that piece Hardie also discusses Relative Risk, which is also implemented in Conc. The Log Likelihood Ratio implementation is based on discussion in Paul Rayson’s Log-likelihood and effect size calculator. Rayson discusses a variety of keyness measures and practical issues in implementing these. Conc follows the approach mentioned by Rayson of applying an observed frequency of 0.5 for words that do not appear in the target or reference corpus to avoid division by zero issues. Conc also follows handling of zero values in calculating log likelihood (see Note 2).",
    "crumbs": [
      "API",
      "keyness"
    ]
  },
  {
    "objectID": "api/keyness.html#about-concs-keyness-functionality",
    "href": "api/keyness.html#about-concs-keyness-functionality",
    "title": "keyness",
    "section": "",
    "text": "Conc implements Log Ratio, which is a keyness measure introduced by Andrew Hardie in this “informal introduction”. In that piece Hardie also discusses Relative Risk, which is also implemented in Conc. The Log Likelihood Ratio implementation is based on discussion in Paul Rayson’s Log-likelihood and effect size calculator. Rayson discusses a variety of keyness measures and practical issues in implementing these. Conc follows the approach mentioned by Rayson of applying an observed frequency of 0.5 for words that do not appear in the target or reference corpus to avoid division by zero issues. Conc also follows handling of zero values in calculating log likelihood (see Note 2).",
    "crumbs": [
      "API",
      "keyness"
    ]
  },
  {
    "objectID": "api/keyness.html#using-the-keyness-class",
    "href": "api/keyness.html#using-the-keyness-class",
    "title": "keyness",
    "section": "Using the Keyness class",
    "text": "Using the Keyness class\nThere are examples below showing how to use the Keyness class directly to output keyword tables. The recommended way to use this functionality is through the Conc class. This provides an interface to create frequency lists, concordances, collocation tables, keyword tables and more.",
    "crumbs": [
      "API",
      "keyness"
    ]
  },
  {
    "objectID": "api/keyness.html#keyness-class-api-reference",
    "href": "api/keyness.html#keyness-class-api-reference",
    "title": "keyness",
    "section": "Keyness class API reference",
    "text": "Keyness class API reference\n\nsource\n\nKeyness\n\n Keyness (corpus:conc.corpus.Corpus|conc.listcorpus.ListCorpus,\n          reference_corpus:conc.corpus.Corpus|conc.listcorpus.ListCorpus)\n\nClass for keyness analysis reporting.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncorpus\nconc.corpus.Corpus | conc.listcorpus.ListCorpus\nCorpus instance\n\n\nreference_corpus\nconc.corpus.Corpus | conc.listcorpus.ListCorpus\nCorpus for comparison\n\n\n\n\nsource\n\n\nKeyness.keywords\n\n Keyness.keywords (effect_size_measure:str='log_ratio',\n                   statistical_significance_measure:str='log_likelihood',\n                   order:str|None=None, order_descending:bool=True,\n                   statistical_significance_cut:float|None=None,\n                   apply_bonferroni:bool=False,\n                   min_document_frequency:int=0,\n                   min_document_frequency_reference:int=0,\n                   min_frequency:int=0, min_frequency_reference:int=0,\n                   case_sensitive:bool=False, normalize_by:int=10000,\n                   page_size:int=20, page_current:int=1,\n                   show_document_frequency:bool=False,\n                   exclude_tokens:list[str]=[],\n                   exclude_tokens_text:str='',\n                   restrict_tokens:list[str]=[],\n                   restrict_tokens_text:str='',\n                   exclude_punctuation:bool=True,\n                   handle_common_typographic_differences:bool=True,\n                   exclude_negative_keywords:bool=True)\n\nGet keywords for the corpus.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\neffect_size_measure\nstr\nlog_ratio\neffect size measure to use, currently only ‘log_ratio’ is supported\n\n\nstatistical_significance_measure\nstr\nlog_likelihood\nstatistical significance measure to use, currently only ‘log_likelihood’ is supported\n\n\norder\nstr | None\nNone\ndefault of None orders by statistical significance measure, results can also be ordered by: frequency, frequency_reference, document_frequency, document_frequency_reference, log_likelihood\n\n\norder_descending\nbool\nTrue\norder is descending or ascending\n\n\nstatistical_significance_cut\nfloat | None\nNone\nstatistical significance p-value to filter results, e.g. 0.05 or 0.01 or 0.001 - ignored if None or 0\n\n\napply_bonferroni\nbool\nFalse\napply Bonferroni correction to the statistical significance cut-off\n\n\nmin_document_frequency\nint\n0\nminimum document frequency in target for token to be included in the report\n\n\nmin_document_frequency_reference\nint\n0\nminimum document frequency in reference for token to be included in the report\n\n\nmin_frequency\nint\n0\nminimum frequency in target for token to be included in the report\n\n\nmin_frequency_reference\nint\n0\nminimum document frequency in reference for token to be included in the report\n\n\ncase_sensitive\nbool\nFalse\nfrequencies for tokens with or without case preserved\n\n\nnormalize_by\nint\n10000\nnormalize frequencies by a number (e.g. 10000)\n\n\npage_size\nint\n20\nnumber of rows to return, if 0 returns all\n\n\npage_current\nint\n1\ncurrent page, ignored if page_size is 0\n\n\nshow_document_frequency\nbool\nFalse\nshow document frequency in output\n\n\nexclude_tokens\nlist\n[]\nexclude specific tokens from report results\n\n\nexclude_tokens_text\nstr\n\ntext to explain which tokens have been excluded, will be added to the report notes\n\n\nrestrict_tokens\nlist\n[]\nrestrict report to return results for a list of specific tokens (see note below)\n\n\nrestrict_tokens_text\nstr\n\ntext to explain which tokens are included, will be added to the report notes\n\n\nexclude_punctuation\nbool\nTrue\nexclude punctuation tokens\n\n\nhandle_common_typographic_differences\nbool\nTrue\nwhether to detect and normalize common differences in word tokens due to typographic differences (i.e. currently focused on apostrophes in common English contractions), ignored when exclude_punctuation is False\n\n\nexclude_negative_keywords\nbool\nTrue\nwhether to exclude negative keywords from the report\n\n\nReturns\nResult\n\nreturn a Result object with the frequency table\n\n\n\n\nNote about restrict_tokens\nWhen using restrict_tokens you may want to see full output of keyness results. To ensure you get output for all tokens, make sure that statistical_significance_cut=None and exclude_negative_keywords=False.\nNote: Polars 1.32.x seems to have introduced some instability about handling of is_in calls, which is required by restrict_tokens. Polars has been temporarily pinned to 1.31.0 to resolve this.\n\n\nExamples\nSee the note above about accessing this functionality through the Conc class.\n\n# load the target corpus\ngardenparty = Corpus().load(path_to_gardenparty_corpus)\n\n\n# load the reference corpus\nbrown = Corpus().load(path_to_brown_corpus)\n\n\n# instantiate the Keyness class\nkeyness = Keyness(corpus = gardenparty, reference_corpus = brown)\n\n\n# generate and display the keywords report\nkeyness.keywords(show_document_frequency = True, statistical_significance_cut = 0.0001, apply_bonferroni = True, order_descending = True, page_current = 1).display()\n\n\n\n\n\n\n\nKeywords\n\n\nTarget corpus: Garden Party Corpus, Reference corpus: Brown Corpus\n\n\nRank\nToken\nFrequency\nFrequency Reference\nDocument Frequency\nDocument Frequency Reference\nNormalized Frequency\nNormalized Frequency Reference\nRelative Risk\nLog Ratio\nLog Likelihood\n\n\n\n\n1\nshe\n1,171\n2,060\n15\n216\n196.76\n21.02\n9.36\n3.23\n2,710.68\n\n\n2\nher\n937\n2,887\n15\n253\n157.44\n29.45\n5.35\n2.42\n1,442.33\n\n\n3\njosephine\n117\n0\n1\n0\n19.66\n0.00\n3,853.78\n11.91\n669.34\n\n\n4\nsaid\n514\n1,944\n15\n315\n86.37\n19.83\n4.35\n2.12\n648.89\n\n\n5\nn’t\n522\n2,016\n15\n286\n87.71\n20.57\n4.26\n2.09\n644.51\n\n\n6\nyou\n642\n3,265\n15\n293\n107.87\n33.31\n3.24\n1.70\n566.70\n\n\n7\nit\n1,021\n6,991\n15\n498\n171.56\n71.33\n2.41\n1.27\n552.39\n\n\n8\noh\n149\n93\n15\n62\n25.04\n0.95\n26.39\n4.72\n540.97\n\n\n9\nlittle\n307\n823\n15\n322\n51.58\n8.40\n6.14\n2.62\n531.41\n\n\n10\nconstantia\n91\n0\n1\n0\n15.29\n0.00\n2,997.38\n11.55\n520.60\n\n\n11\ni\n719\n4,370\n15\n335\n120.81\n44.59\n2.71\n1.44\n483.12\n\n\n12\nlaura\n86\n14\n2\n6\n14.45\n0.14\n101.17\n6.66\n412.65\n\n\n13\nisabel\n71\n1\n2\n1\n11.93\n0.01\n1,169.31\n10.19\n395.76\n\n\n14\ngrandma\n73\n15\n2\n5\n12.27\n0.15\n80.15\n6.32\n339.03\n\n\n15\nwas\n1,102\n9,931\n15\n466\n185.17\n101.32\n1.83\n0.87\n307.65\n\n\n16\nfenella\n49\n0\n1\n0\n8.23\n0.00\n1,613.98\n10.66\n280.32\n\n\n17\ndear\n78\n54\n13\n36\n13.11\n0.55\n23.79\n4.57\n273.99\n\n\n18\nberyl\n50\n3\n1\n3\n8.40\n0.03\n274.49\n8.10\n263.34\n\n\n19\nhammond\n47\n2\n1\n2\n7.90\n0.02\n387.02\n8.60\n252.40\n\n\n20\nyes\n87\n109\n14\n74\n14.62\n1.11\n13.15\n3.72\n241.33\n\n\n\nReport based on word tokens\n\n\nFrequent tokens with apostrophes have been normalized in reference corpus to match target usage\n\n\nKeywords filtered based on p-value 0.0001 with Bonferroni correction (based on 5392 tests)\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens in target corpus: 59,514\n\n\nTotal word tokens in reference corpus: 980,144\n\n\nKeywords: 337\n\n\nShowing 20 rows\n\n\nPage 1 of 17",
    "crumbs": [
      "API",
      "keyness"
    ]
  },
  {
    "objectID": "api/frequency.html",
    "href": "api/frequency.html",
    "title": "frequency",
    "section": "",
    "text": "There are examples below showing how to use the Frequency class directly to get frequency lists for a corpus. The example below also shows example frequency lists. The recommended way to use this functionality is through the Conc class. This provides an interface to create frequency lists, concordances, collocation tables, keyword tables and more.",
    "crumbs": [
      "API",
      "frequency"
    ]
  },
  {
    "objectID": "api/frequency.html#using-the-frequency-class",
    "href": "api/frequency.html#using-the-frequency-class",
    "title": "frequency",
    "section": "",
    "text": "There are examples below showing how to use the Frequency class directly to get frequency lists for a corpus. The example below also shows example frequency lists. The recommended way to use this functionality is through the Conc class. This provides an interface to create frequency lists, concordances, collocation tables, keyword tables and more.",
    "crumbs": [
      "API",
      "frequency"
    ]
  },
  {
    "objectID": "api/frequency.html#frequency-class-api-reference",
    "href": "api/frequency.html#frequency-class-api-reference",
    "title": "frequency",
    "section": "Frequency class API reference",
    "text": "Frequency class API reference\n\nsource\n\nFrequency\n\n Frequency (corpus:conc.corpus.Corpus|conc.listcorpus.ListCorpus)\n\nClass for frequency analysis reporting\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncorpus\nconc.corpus.Corpus | conc.listcorpus.ListCorpus\nCorpus instance\n\n\n\n\nsource\n\n\nFrequency.frequencies\n\n Frequency.frequencies (case_sensitive:bool=False, normalize_by:int=10000,\n                        page_size:int=20, page_current:int=1,\n                        show_token_id:bool=False,\n                        show_document_frequency:bool=False,\n                        exclude_tokens:list[str]=[],\n                        exclude_tokens_text:str='',\n                        restrict_tokens:list[str]=[],\n                        restrict_tokens_text:str='',\n                        exclude_punctuation:bool=True)\n\nReport frequent tokens.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncase_sensitive\nbool\nFalse\nfrequencies for tokens with or without case preserved\n\n\nnormalize_by\nint\n10000\nnormalize frequencies by a number (e.g. 10000)\n\n\npage_size\nint\n20\nnumber of rows to return, if 0 returns all\n\n\npage_current\nint\n1\ncurrent page, ignored if page_size is 0\n\n\nshow_token_id\nbool\nFalse\nshow token_id in output\n\n\nshow_document_frequency\nbool\nFalse\nshow document frequency in output\n\n\nexclude_tokens\nlist\n[]\nexclude specific tokens from frequency report, can be used to remove stopwords\n\n\nexclude_tokens_text\nstr\n\ntext to explain which tokens have been excluded, will be added to the report notes\n\n\nrestrict_tokens\nlist\n[]\nrestrict frequency report to return frequencies for a list of specific tokens\n\n\nrestrict_tokens_text\nstr\n\ntext to explain which tokens are included, will be added to the report notes\n\n\nexclude_punctuation\nbool\nTrue\nexclude punctuation tokens\n\n\nReturns\nResult\n\nreturn a Result object with the frequency table\n\n\n\n\nExamples\nSee the note above about accessing this functionality through the Conc class.\n\n# load the corpus\nbrown = Corpus().load(path_to_brown_corpus)\n\n\n# instantiate the Frequency class\nfreq_brown = Frequency(brown)\n\n\n# run the frequencies method and display the results\nfreq_brown.frequencies(normalize_by=10000, page_size=20).display()\n\n\n\n\n\n\n\nFrequencies\n\n\nFrequencies of word tokens, Brown Corpus\n\n\nRank\nToken\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe\n63,516\n648.03\n\n\n2\nof\n36,321\n370.57\n\n\n3\nand\n27,787\n283.50\n\n\n4\nto\n25,868\n263.92\n\n\n5\na\n22,190\n226.40\n\n\n6\nin\n19,751\n201.51\n\n\n7\nthat\n10,409\n106.20\n\n\n8\nis\n10,138\n103.43\n\n\n9\nwas\n9,931\n101.32\n\n\n10\nfor\n8,905\n90.85\n\n\n11\nwith\n7,043\n71.86\n\n\n12\nit\n6,991\n71.33\n\n\n13\nhe\n6,772\n69.09\n\n\n14\nas\n6,738\n68.75\n\n\n15\nhis\n6,523\n66.55\n\n\n16\non\n6,459\n65.90\n\n\n17\nbe\n6,365\n64.94\n\n\n18\n's\n5,285\n53.92\n\n\n19\nhad\n5,200\n53.05\n\n\n20\nby\n5,156\n52.60\n\n\n\nReport based on word tokens\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens: 980,144\n\n\nUnique word tokens: 42,907\n\n\nShowing 20 rows\n\n\nPage 1 of 2146\n\n\n\n\n\n\n\n\n\n\nfrom conc.core import get_stop_words\n\n\nstop_words = get_stop_words(save_path, spacy_model = 'en_core_web_sm')\nfreq_brown.frequencies(normalize_by=10000, show_document_frequency = True, exclude_tokens = stop_words, page_size=20).display()\n\n\n\n\n\n\n\nFrequencies\n\n\nFrequencies of word tokens, Brown Corpus\n\n\nRank\nToken\nFrequency\nDocument Frequency\nNormalized Frequency\n\n\n\n\n1\nsaid\n1,944\n315\n19.83\n\n\n2\ntime\n1,667\n450\n17.01\n\n\n3\nnew\n1,595\n390\n16.27\n\n\n4\nman\n1,346\n326\n13.73\n\n\n5\nlike\n1,287\n366\n13.13\n\n\n6\naf\n989\n49\n10.09\n\n\n7\nyears\n953\n346\n9.72\n\n\n8\nway\n925\n365\n9.44\n\n\n9\nstate\n883\n200\n9.01\n\n\n10\nlong\n863\n354\n8.80\n\n\n11\npeople\n851\n286\n8.68\n\n\n12\nworld\n848\n274\n8.65\n\n\n13\nyear\n831\n242\n8.48\n\n\n14\nlittle\n823\n322\n8.40\n\n\n15\ngood\n813\n320\n8.29\n\n\n16\nmen\n772\n248\n7.88\n\n\n17\nwork\n767\n310\n7.83\n\n\n18\nday\n767\n311\n7.83\n\n\n19\nold\n734\n278\n7.49\n\n\n20\nlife\n728\n284\n7.43\n\n\n\nReport based on word tokens\n\n\nTokens excluded from report: 302\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens: 980,144\n\n\nUnique word tokens: 42,601\n\n\nShowing 20 rows\n\n\nPage 1 of 2131",
    "crumbs": [
      "API",
      "frequency"
    ]
  },
  {
    "objectID": "api/listcorpus.html",
    "href": "api/listcorpus.html",
    "title": "listcorpus",
    "section": "",
    "text": "Note: to generate a frequency table for a corpus, see Conc.frequencies.",
    "crumbs": [
      "API",
      "listcorpus"
    ]
  },
  {
    "objectID": "api/listcorpus.html#listcorpus-class",
    "href": "api/listcorpus.html#listcorpus-class",
    "title": "listcorpus",
    "section": "ListCorpus class",
    "text": "ListCorpus class\n\nsource\n\nListCorpus\n\n ListCorpus (name:str='', description:str='')\n\nRepresention of a corpus based on frequency information, which can be loaded as a reference corpus.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nname of corpus\n\n\ndescription\nstr\n\ndescription of corpus\n\n\n\n\nsource\n\n\nListCorpus.build_from_corpus\n\n ListCorpus.build_from_corpus (source_corpus_path:str, save_path:str)\n\nBuild a List Corpus from a Conc corpus.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nsource_corpus_path\nstr\npath to a Conc corpus directory\n\n\nsave_path\nstr\ndirectory where corpus will be created, a subdirectory will be automatically created with the corpus content\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nListCorpus.load\n\n ListCorpus.load (corpus_path:str)\n\nLoad list corpus from disk.\n\n\n\n\nType\nDetails\n\n\n\n\ncorpus_path\nstr\npath to load corpus\n\n\n\n\nsource\n\n\nListCorpus.info\n\n ListCorpus.info (formatted:bool=True)\n\nReturn information about the list corpus.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nformatted\nbool\nTrue\nreturn formatted output\n\n\nReturns\nstr\n\nformatted information about the corpus\n\n\n\n\nsource\n\n\nListCorpus.report\n\n ListCorpus.report ()\n\nGet information about the list corpus as a result object.\n\nsource\n\n\nListCorpus.summary\n\n ListCorpus.summary (include_memory_usage:bool=False)\n\nPrint information about the list corpus in a formatted table.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninclude_memory_usage\nbool\nFalse\ninclude memory usage in output",
    "crumbs": [
      "API",
      "listcorpus"
    ]
  },
  {
    "objectID": "api/listcorpus.html#information-on-working-with-the-list-corpus-format",
    "href": "api/listcorpus.html#information-on-working-with-the-list-corpus-format",
    "title": "listcorpus",
    "section": "Information on working with the list corpus format",
    "text": "Information on working with the list corpus format\nTo create a list corpus you first need to create a standard corpus using Conc.corpus. See the recipes for examples.\nNote: if you intend to use the list corpus as a reference corpus for keyness analsis, it will probably be helpful to add standardize_word_token_punctuation_characters to the build method when building the source corpus. This will ensure that word tokens with punctuation (e.g. n’t) use the same apostrophe character and allow Conc to handle these differences when calculating keyness.\nOnce created you create a list corpus by creating in the path to the corpus directory …\n\nlistcorpus = ListCorpus().build_from_corpus(source_corpus_path = f'{save_path}garden-party.corpus', save_path = save_path)\n\nList corpus will copy some of the data from the corpus, and add document frequency information for each token. Conc uses the .listcorpus suffix on directories to differentiate standard corpora from list corpora. The directory for the list corpus will contain corpus information in listcorpus.json, the frequency information in the vocab.parquet file, and a human-readable README.md to aide sharing the data.\n\n\n├── garden-party.listcorpus\n│   ├── README.md\n│   ├── vocab.parquet\n│   └── listcorpus.json\n\n\nYou can access summary information, with the same methods as the Conc.corpus class.\nFor example …\n\nlistcorpus.summary()\n\n\n\n\n\n\n\nList Corpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nname\nGarden Party Corpus\n\n\ndescription\nA corpus of short stories from The Garden Party: and Other Stories by Katherine Mansfield. Texts downloaded from Project Gutenberg https://gutenberg.org/ and are in the public domain. The text files contain the short story without the title. https://github.com/ucdh/scraping-garden-party\n\n\ndate_created\n2025-07-23 22:15:21\n\n\nconc_version\n0.1.10\n\n\ncorpus_path\n/home/geoff/data/conc-test-corpora/garden-party.listcorpus\n\n\ndocument_count\n15\n\n\ntoken_count\n74,664\n\n\nword_token_count\n59,514\n\n\nunique_tokens\n5,410\n\n\nunique_word_tokens\n5,392\n\n\n\n\n\n\n\n\nThis preview of the vocab table shows the available columns in case you want to access the data directly. The anatomy page has information on the columns from the standard Conc corpus format that is relevant to working with a list corpus.\n\ndisplay(listcorpus.vocab.head(1000).collect().sample(10))\n\n\n\n\n\n\nrank\ntokens_sort_order\ntoken_id\ntoken\nfrequency_lower\nfrequency_orth\nis_punct\nis_space\ndocument_frequency_lower\ndocument_frequency_orth\n\n\n\n\n109\n35\n349\n\"about\"\n88\n88\nfalse\nfalse\n14\n14\n\n\n883\n5262\n4305\n\"strong\"\n9\n9\nfalse\nfalse\n5\n5\n\n\n797\n5474\n3022\n\"thank\"\n17\n10\nfalse\nfalse\n8\n7\n\n\n504\n4037\n2788\n\"played\"\n17\n17\nfalse\nfalse\n7\n7\n\n\n216\n6102\n1300\n\"who\"\n56\n44\nfalse\nfalse\n14\n14\n\n\n819\n868\n1720\n\"chairs\"\n9\n9\nfalse\nfalse\n5\n5\n\n\n27\n2383\n4423\n\"had\"\n469\n466\nfalse\nfalse\n14\n14\n\n\n209\n6128\n6234\n\"will\"\n47\n45\nfalse\nfalse\n13\n12\n\n\n986\n616\n2240\n\"brass\"\n7\n7\nfalse\nfalse\n4\n4\n\n\n251\n2410\n587\n\"hands\"\n38\n38\nfalse\nfalse\n13\n13\n\n\n\n\n\n\n\nsource\n\nListCorpus.get_token_count_text\n\n ListCorpus.get_token_count_text (exclude_punctuation:bool=False)\n\nGet the token count for the corpus with adjustments and text for output\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexclude_punctuation\nbool\nFalse\nexclude punctuation tokens from the count\n\n\nReturns\ntuple\n\ntoken count with adjustments based on exclusions, token descriptor, total descriptor",
    "crumbs": [
      "API",
      "listcorpus"
    ]
  },
  {
    "objectID": "api/conc.html",
    "href": "api/conc.html",
    "title": "conc",
    "section": "",
    "text": "source\n\nConc\n\n Conc (corpus)\n\nUnified interface to Conc reporting for analysis of frequency, ngrams, concordances, keyness, and collocates.\n\n\n\n\nDetails\n\n\n\n\ncorpus\nCorpus instance\n\n\n\n\n# load (or build) a corpus\nreuters = Corpus('reuters').load(path_to_reuters_corpus)\n\n\n#get a summary\nreuters.summary()\n\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nName\nReuters Corpus\n\n\nDescription\nReuters corpus (Reuters-21578, Distribution 1.0). \"The copyright for the text of newswire articles and Reuters annotations in the Reuters-21578 collection resides with Reuters Ltd. Reuters Ltd. and Carnegie Group, Inc. have agreed to allow the free distribution of this data *for research purposes only*. If you publish results based on this data set, please acknowledge its use, refer to the data set by the name (Reuters-21578, Distribution 1.0), and inform your readers of the current location of the data set.\" https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.\n\n\nDate Created\n2025-07-09 09:21:55\n\n\nConc Version\n0.1.6\n\n\nCorpus Path\n/home/geoff/data/conc-test-corpora/reuters.corpus\n\n\nDocument Count\n10,788\n\n\nToken Count\n1,552,919\n\n\nWord Token Count\n1,398,782\n\n\nUnique Tokens\n49,901\n\n\nUnique Word Tokens\n49,860\n\n\n\n\n\n\n\n\n\n# create a Conc report instance for the corpus\nconc = Conc(reuters)\n\n\nsource\n\n\nConc.frequencies\n\n Conc.frequencies (case_sensitive:bool=False, normalize_by:int=10000,\n                   page_size:int=20, page_current:int=1,\n                   show_token_id:bool=False,\n                   show_document_frequency:bool=False,\n                   exclude_tokens:list[str]=[],\n                   exclude_tokens_text:str='',\n                   restrict_tokens:list[str]=[],\n                   restrict_tokens_text:str='',\n                   exclude_punctuation:bool=True)\n\nReport frequent tokens.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncase_sensitive\nbool\nFalse\nfrequencies for tokens with or without case preserved\n\n\nnormalize_by\nint\n10000\nnormalize frequencies by a number (e.g. 10000)\n\n\npage_size\nint\n20\nnumber of rows to return, if 0 returns all\n\n\npage_current\nint\n1\ncurrent page, ignored if page_size is 0\n\n\nshow_token_id\nbool\nFalse\nshow token_id in output\n\n\nshow_document_frequency\nbool\nFalse\nshow document frequency in output\n\n\nexclude_tokens\nlist\n[]\nexclude specific tokens from frequency report, can be used to remove stopwords\n\n\nexclude_tokens_text\nstr\n\ntext to explain which tokens have been excluded, will be added to the report notes\n\n\nrestrict_tokens\nlist\n[]\nrestrict frequency report to return frequencies for a list of specific tokens\n\n\nrestrict_tokens_text\nstr\n\ntext to explain which tokens are included, will be added to the report notes\n\n\nexclude_punctuation\nbool\nTrue\nexclude punctuation tokens\n\n\nReturns\nResult\n\nreturn a Result object with the frequency table\n\n\n\n\nconc.frequencies(normalize_by=10000).display()\n\n\n\n\n\n\n\nFrequencies\n\n\nFrequencies of word tokens, Reuters Corpus\n\n\nRank\nToken\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe\n69,263\n495.17\n\n\n2\nof\n36,779\n262.94\n\n\n3\nto\n36,328\n259.71\n\n\n4\nin\n29,252\n209.12\n\n\n5\nand\n25,645\n183.34\n\n\n6\nsaid\n25,379\n181.44\n\n\n7\na\n24,844\n177.61\n\n\n8\nmln\n18,621\n133.12\n\n\n9\nvs\n14,332\n102.46\n\n\n10\nfor\n13,720\n98.09\n\n\n11\ndlrs\n12,411\n88.73\n\n\n12\nit\n11,104\n79.38\n\n\n13\npct\n9,810\n70.13\n\n\n14\n's\n9,627\n68.82\n\n\n15\non\n9,244\n66.09\n\n\n16\ncts\n8,357\n59.74\n\n\n17\nfrom\n8,216\n58.74\n\n\n18\nis\n7,673\n54.85\n\n\n19\nthat\n7,540\n53.90\n\n\n20\nyear\n7,523\n53.78\n\n\n\nReport based on word tokens\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens: 1,398,782\n\n\nUnique word tokens: 49,860\n\n\nShowing 20 rows\n\n\nPage 1 of 2494\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nConc.ngrams\n\n Conc.ngrams (token_str:str, ngram_length:int=2,\n              ngram_token_position:str='LEFT', normalize_by:int=10000,\n              page_size:int=20, page_current:int=1,\n              show_all_columns:bool=False, exclude_punctuation:bool=True,\n              use_cache:bool=True)\n\nReport ngram frequencies containing a token string.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntoken_str\nstr\n\ntoken string to get ngrams for\n\n\nngram_length\nint\n2\nlength of ngram\n\n\nngram_token_position\nstr\nLEFT\nspecify if token sequence is on LEFT or RIGHT or MIDDLE (support for other positions is in-development)\n\n\nnormalize_by\nint\n10000\nnormalize frequencies by a number (e.g. 10000)\n\n\npage_size\nint\n20\nnumber of results to display per results page\n\n\npage_current\nint\n1\ncurrent page of results\n\n\nshow_all_columns\nbool\nFalse\nreturn raw df with all columns or just ngram and frequency\n\n\nexclude_punctuation\nbool\nTrue\ndo not return ngrams with punctuation tokens\n\n\nuse_cache\nbool\nTrue\nretrieve the results from cache if available\n\n\nReturns\nResult\n\nreturn a Result object with ngram data\n\n\n\n\nconc.ngrams(token_str = 'said', ngram_length = 3, ngram_token_position = 'RIGHT', exclude_punctuation = True).display()\n\n\n\n\n\n\n\nNgrams for \"said\"\n\n\nReuters Corpus\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe company said\n1,173\n8.39\n\n\n2\nthe department said\n194\n1.39\n\n\n3\nthe sources said\n165\n1.18\n\n\n4\nof england said\n122\n0.87\n\n\n5\nthe spokesman said\n116\n0.83\n\n\n6\nthe bank said\n114\n0.81\n\n\n7\nagriculture department said\n106\n0.76\n\n\n8\ntrade sources said\n95\n0.68\n\n\n9\ncompany also said\n93\n0.66\n\n\n10\nthe report said\n93\n0.66\n\n\n11\nbut he said\n75\n0.54\n\n\n12\nit also said\n71\n0.51\n\n\n13\nthe official said\n71\n0.51\n\n\n14\nhe also said\n70\n0.50\n\n\n15\nindustry sources said\n68\n0.49\n\n\n16\nindustries inc said\n68\n0.49\n\n\n17\nthe group said\n66\n0.47\n\n\n18\nthe officials said\n64\n0.46\n\n\n19\nthe statement said\n59\n0.42\n\n\n20\ncompany spokesman said\n54\n0.39\n\n\n\nReport based on word tokens\n\n\nNgram length: 3, Token position: right\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 4,698\n\n\nTotal ngrams: 12,707\n\n\nShowing 20 rows\n\n\nPage 1 of 235\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nConc.ngram_frequencies\n\n Conc.ngram_frequencies (ngram_length:int=2, case_sensitive:bool=False,\n                         normalize_by:int=10000, page_size:int=20,\n                         page_current:int=1,\n                         show_document_frequency:bool=False,\n                         exclude_punctuation:bool=True)\n\nReport frequent ngrams.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nngram_length\nint\n2\nlength of ngram\n\n\ncase_sensitive\nbool\nFalse\nfrequencies for tokens lowercased or with case preserved\n\n\nnormalize_by\nint\n10000\nnormalize frequencies by a number (e.g. 10000)\n\n\npage_size\nint\n20\nnumber of rows to return\n\n\npage_current\nint\n1\ncurrent page\n\n\nshow_document_frequency\nbool\nFalse\nshow document frequency in output (slow to compute for large corpora)\n\n\nexclude_punctuation\nbool\nTrue\nexclude ngrams containing punctuation tokens\n\n\nReturns\nResult\n\nreturn a Result object with the frequency table\n\n\n\n\nconc.ngram_frequencies(ngram_length = 3, case_sensitive = False, exclude_punctuation = True, page_current = 1).display()\n\n\n\n\n\n\n\nNgram Frequencies\n\n\nReuters Corpus\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe company said\n1,173\n8.39\n\n\n2\nmln dlrs in\n795\n5.68\n\n\n3\ncts vs loss\n665\n4.75\n\n\n4\nsaid it has\n636\n4.55\n\n\n5\nmln avg shrs\n620\n4.43\n\n\n6\npct of the\n608\n4.35\n\n\n7\nthe united states\n603\n4.31\n\n\n8\nqtr net shr\n574\n4.10\n\n\n9\ndlrs a share\n546\n3.90\n\n\n10\ninc said it\n523\n3.74\n\n\n11\nthe company 's\n518\n3.70\n\n\n12\ncts net loss\n517\n3.70\n\n\n13\nthe end of\n501\n3.58\n\n\n14\ncts a share\n494\n3.53\n\n\n15\nis expected to\n429\n3.07\n\n\n16\ncorp said it\n412\n2.95\n\n\n17\nnine mths shr\n412\n2.95\n\n\n18\nsaid in a\n407\n2.91\n\n\n19\nthe bank of\n380\n2.72\n\n\n20\nbillion dlrs in\n373\n2.67\n\n\n\nReport based on word tokens\n\n\nNgram length: 3\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 684,778\n\n\nTotal ngrams: 1,128,352\n\n\nShowing 20 rows\n\n\nPage 1 of 34239\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nConc.concordance\n\n Conc.concordance (token_str:str, context_length:int=5,\n                   order:str='1R2R3R', page_size:int=20,\n                   page_current:int=1, show_all_columns:bool=False,\n                   use_cache:bool=True, ignore_punctuation:bool=True,\n                   filter_context_str:str|None=None,\n                   filter_context_length:int|tuple[int,int]=5)\n\nReport concordance for a token string.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntoken_str\nstr\n\ntoken string to get concordance for\n\n\ncontext_length\nint\n5\nnumber of words to show on left and right of token string\n\n\norder\nstr\n1R2R3R\norder of sort columns - one of 1L2L3L, 3L2L1L, 2L1L1R, 1L1R2R, 1R2R3R (default if ommitted), LEFT, RIGHT\n\n\npage_size\nint\n20\nnumber of results to display per results page\n\n\npage_current\nint\n1\ncurrent page of results\n\n\nshow_all_columns\nbool\nFalse\ndf with all columns or just essentials\n\n\nuse_cache\nbool\nTrue\nretrieve the results from cache if available (currently ignored)\n\n\nignore_punctuation\nbool\nTrue\nwhether to ignore punctuation in the concordance sort\n\n\nfilter_context_str\nstr | None\nNone\nif a string is provided, the concordance lines will be filtered to show lines with contexts containing this string\n\n\nfilter_context_length\nint | tuple[int, int]\n5\nignored if filter_context_str is None, otherwise this is the context window size per side in tokens - if an int (e.g. 5) context lengths on left and right will be the same, for independent control of left and right context length pass a tuple (context_length_left, context_left_right)\n\n\nReturns\nResult\n\nconcordance report results\n\n\n\n\nconc.concordance('the company said', context_length = 5, order='1R2R3R').display()\n\n\n\n\n\n\n\nConcordance for \"the company said\"\n\n\nReuters Corpus, Context tokens: 5, Order: 1R2R3R\n\n\nDoc Id\nLeft\nNode\nRight\n\n\n\n\n3754\ndividend payment since 1981 ,\nthe company said\n.\n\n\n10165\nsecond quarter of 1987 ,\nthe company said\n.\n\n\n7883\nof Metex common stock ,\nthe company said\n.\n\n\n4520\n110 dlrs per tonne ,\nthe company said\n.\n\n\n1801\nclose in early July ,\nthe company said\n.\n\n\n10681\nTransamerica Occidental Life subsidiary ,\nthe company said\n.\n\n\n7856\ngrades of crude oil ,\nthe company said\n.\n\n\n10162\nof record March 25 ,\nthe company said\n.\n\n\n8573\nadhesives for engineering plastics ,\nthe company said\n.\n\n\n5681\nsaleable for 13 months ,\nthe company said\n.\n\n\n4497\nCorp earlier this year ,\nthe company said\n.\n\n\n8740\nencouraging signs at yearend ,\nthe company said\n.\n\n\n10143\nclose of business today ,\nthe company said\n.\n\n\n10163\nheavy to 14.60 dlrs ,\nthe company said\n.\n\n\n7484\noperated by National Pizza ,\nthe company said\n.\n\n\n3651\nits sixth largest overall ,\nthe company said\n.\n\n\n9655\nMay one , 1987 ,\nthe company said\n.\n\n\n10741\napprovals and other conditions ,\nthe company said\n.\n\n\n8987\neastward from that point ,\nthe company said\n.\n\n\n7408\nof any other shareholder ,\nthe company said\n.\n\n\n\nTotal Concordance Lines: 1173\n\n\nTotal Documents: 911\n\n\nShowing 20 lines\n\n\nPage 1 of 59\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nConc.concordance_plot\n\n Conc.concordance_plot (token_str:str, page_size:int=10,\n                        append_info:bool=True)\n\nCreate a concordance plot.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntoken_str\nstr\n\ntoken string for concordance plot\n\n\npage_size\nint\n10\nnumber of plots per page\n\n\nappend_info\nbool\nTrue\nappend token position info to the concordance line preview screens visible when hover over the plot lines\n\n\nReturns\nPlot\n\nconcordance plot object, add .display() to view in notebook\n\n\n\n\nconc.concordance_plot('cause', page_size=10).display()\n\n\nConcordance Plot for \"cause\"Reuters CorpusTotal Documents: 121Total Concordance Lines: 135Page 1 of 13\n    \n    \n    \n    \n\n\n\nsource\n\n\nConc.set_reference_corpus\n\n Conc.set_reference_corpus\n                            (corpus:conc.corpus.Corpus|conc.listcorpus.Lis\n                            tCorpus)\n\nSet a reference corpus for keyness analysis.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncorpus\nconc.corpus.Corpus | conc.listcorpus.ListCorpus\nReference corpus\n\n\nReturns\nNone\n\n\n\n\n\n# load a corpus as a reference corpus\nbrown = Corpus('brown').load(path_to_brown_corpus)\n\n# set corpus as reference corpus\nconc.set_reference_corpus(brown)\n\n\nsource\n\n\nConc.keywords\n\n Conc.keywords (effect_size_measure:str='log_ratio',\n                statistical_significance_measure:str='log_likelihood',\n                order:str|None=None, order_descending:bool=True,\n                statistical_significance_cut:float|None=None,\n                apply_bonferroni:bool=False, min_document_frequency:int=0,\n                min_document_frequency_reference:int=0,\n                min_frequency:int=0, min_frequency_reference:int=0,\n                case_sensitive:bool=False, normalize_by:int=10000,\n                page_size:int=20, page_current:int=1,\n                show_document_frequency:bool=False,\n                exclude_tokens:list[str]=[], exclude_tokens_text:str='',\n                restrict_tokens:list[str]=[], restrict_tokens_text:str='',\n                exclude_punctuation:bool=True,\n                handle_common_typographic_differences:bool=True,\n                exclude_negative_keywords:bool=True)\n\nGet keywords for the corpus.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\neffect_size_measure\nstr\nlog_ratio\neffect size measure to use, currently only ‘log_ratio’ is supported\n\n\nstatistical_significance_measure\nstr\nlog_likelihood\nstatistical significance measure to use, currently only ‘log_likelihood’ is supported\n\n\norder\nstr | None\nNone\ndefault of None orders by effect size measure, results can also be ordered by: frequency, frequency_reference, document_frequency, document_frequency_reference, log_likelihood\n\n\norder_descending\nbool\nTrue\norder is descending or ascending\n\n\nstatistical_significance_cut\nfloat | None\nNone\nstatistical significance p-value to filter results, e.g. 0.05 or 0.01 or 0.001 - ignored if None or 0\n\n\napply_bonferroni\nbool\nFalse\napply Bonferroni correction to the statistical significance cut-off\n\n\nmin_document_frequency\nint\n0\nminimum document frequency in target for token to be included in the report\n\n\nmin_document_frequency_reference\nint\n0\nminimum document frequency in reference for token to be included in the report\n\n\nmin_frequency\nint\n0\nminimum frequency in target for token to be included in the report\n\n\nmin_frequency_reference\nint\n0\nminimum document frequency in reference for token to be included in the report\n\n\ncase_sensitive\nbool\nFalse\nfrequencies for tokens with or without case preserved\n\n\nnormalize_by\nint\n10000\nnormalize frequencies by a number (e.g. 10000)\n\n\npage_size\nint\n20\nnumber of rows to return, if 0 returns all\n\n\npage_current\nint\n1\ncurrent page, ignored if page_size is 0\n\n\nshow_document_frequency\nbool\nFalse\nshow document frequency in output\n\n\nexclude_tokens\nlist\n[]\nexclude specific tokens from report results\n\n\nexclude_tokens_text\nstr\n\ntext to explain which tokens have been excluded, will be added to the report notes\n\n\nrestrict_tokens\nlist\n[]\nrestrict report to return results for a list of specific tokens\n\n\nrestrict_tokens_text\nstr\n\ntext to explain which tokens are included, will be added to the report notes\n\n\nexclude_punctuation\nbool\nTrue\nexclude punctuation tokens\n\n\nhandle_common_typographic_differences\nbool\nTrue\nwhether to detect and normalize common differences in word tokens due to typographic differences (i.e. currently focused on apostrophes in common English contractions), ignored when exclude_punctuation is False\n\n\nexclude_negative_keywords\nbool\nTrue\nwhether to exclude negative keywords from the report\n\n\nReturns\nResult\n\nreturn a Result object with the frequency table\n\n\n\n\nconc.keywords(statistical_significance_cut = 0.0001, min_document_frequency_reference = 5).display()\n\n\n\n\n\n\n\nKeywords\n\n\nTarget corpus: Reuters Corpus, Reference corpus: Brown Corpus\n\n\nRank\nToken\nFrequency\nFrequency Reference\nNormalized Frequency\nNormalized Frequency Reference\nRelative Risk\nLog Ratio\nLog Likelihood\n\n\n\n\n1\nsaid\n25,379\n1,944\n181.44\n19.83\n9.15\n3.19\n16,380.21\n\n\n2\nnet\n6,988\n31\n49.96\n0.32\n157.95\n7.30\n7,078.84\n\n\n3\nbillion\n5,828\n65\n41.66\n0.66\n62.83\n5.97\n5,589.95\n\n\n4\nloss\n5,124\n85\n36.63\n0.87\n42.24\n5.40\n4,724.67\n\n\n5\nu.s.\n5,496\n155\n39.29\n1.58\n24.85\n4.63\n4,691.63\n\n\n6\nyear\n7,523\n831\n53.78\n8.48\n6.34\n2.67\n4,051.72\n\n\n7\nbank\n3,640\n83\n26.02\n0.85\n30.73\n4.94\n3,217.71\n\n\n8\ncompany\n4,670\n319\n33.39\n3.25\n10.26\n3.36\n3,154.17\n\n\n9\nprofit\n2,960\n36\n21.16\n0.37\n57.61\n5.85\n2,817.73\n\n\n10\noil\n3,262\n100\n23.32\n1.02\n22.86\n4.51\n2,741.87\n\n\n11\nshare\n3,146\n99\n22.49\n1.01\n22.27\n4.48\n2,631.00\n\n\n12\nshares\n2,652\n45\n18.96\n0.46\n41.30\n5.37\n2,438.84\n\n\n13\ntrade\n3,094\n143\n22.12\n1.46\n15.16\n3.92\n2,367.94\n\n\n14\nmarket\n2,810\n158\n20.09\n1.61\n12.46\n3.64\n2,030.41\n\n\n15\nits\n7,402\n1,780\n52.92\n18.16\n2.91\n1.54\n1,987.46\n\n\n16\nstock\n2,629\n146\n18.79\n1.49\n12.62\n3.66\n1,907.10\n\n\n17\nprices\n2,194\n61\n15.69\n0.62\n25.20\n4.66\n1,877.65\n\n\n18\njapan\n1,854\n34\n13.25\n0.35\n38.21\n5.26\n1,688.89\n\n\n19\napril\n2,039\n71\n14.58\n0.72\n20.12\n4.33\n1,670.31\n\n\n20\nquarter\n1,852\n44\n13.24\n0.45\n29.49\n4.88\n1,626.89\n\n\n\nReport based on word tokens\n\n\nFiltered tokens by minimum document frequency in reference corpus (5)\n\n\nKeywords filtered based on p-value: 0.0001\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens in target corpus: 1,398,782\n\n\nTotal word tokens in reference corpus: 980,144\n\n\nKeywords: 3,691\n\n\nShowing 20 rows\n\n\nPage 1 of 185\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nConc.collocates\n\n Conc.collocates (token_str:str, effect_size_measure:str='logdice',\n                  statistical_significance_measure:str='log_likelihood',\n                  order:str|None=None, order_descending:bool=True,\n                  statistical_significance_cut:float|None=None,\n                  apply_bonferroni:bool=False,\n                  context_length:int|tuple[int,int]=5,\n                  min_collocate_frequency:int=5, page_size:int=20,\n                  page_current:int=1, exclude_punctuation:bool=True)\n\nReport collocates for a given token string.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntoken_str\nstr\n\nToken to search for\n\n\neffect_size_measure\nstr\nlogdice\nstatistical measure to use for collocation calculation: logdice, mutual_information\n\n\nstatistical_significance_measure\nstr\nlog_likelihood\nstatistical significance measure to use, currently only ‘log_likelihood’ is supported\n\n\norder\nstr | None\nNone\ndefault of None orders by collocation_measure, results can also be ordered by: collocate_frequency, frequency, log_likelihood\n\n\norder_descending\nbool\nTrue\norder is descending or ascending\n\n\nstatistical_significance_cut\nfloat | None\nNone\nstatistical significance p-value to filter results, e.g. 0.05 or 0.01 or 0.001 - ignored if None or 0\n\n\napply_bonferroni\nbool\nFalse\napply Bonferroni correction to the statistical significance cut-off\n\n\ncontext_length\nint | tuple[int, int]\n5\nWindow size per side in tokens - if an int (e.g. 5) context lengths on left and right will be the same, for independent control of left and right context length pass a tuple (context_length_left, context_left_right) (e.g. (0, 5))\n\n\nmin_collocate_frequency\nint\n5\nMinimum count of collocates\n\n\npage_size\nint\n20\nnumber of rows to return, if 0 returns all\n\n\npage_current\nint\n1\ncurrent page, ignored if page_size is 0\n\n\nexclude_punctuation\nbool\nTrue\nexclude punctuation tokens\n\n\nReturns\nResult\n\n\n\n\n\n\nconc.collocates('the company', context_length = (0, 1), exclude_punctuation = False).display()\n\n\n\n\n\n\n\nCollocates of \"the company\"\n\n\nReuters Corpus\n\n\nRank\nToken\nCollocate Frequency\nFrequency\nLogdice\nLog Likelihood\n\n\n\n\n1\nsaid\n1,173\n25,379\n10.40\n5,149.40\n\n\n2\n's\n518\n9,627\n10.38\n2,429.14\n\n\n3\nalso\n107\n2,532\n9.28\n450.99\n\n\n4\nreported\n51\n775\n8.74\n259.63\n\n\n5\nhas\n69\n4,874\n8.14\n151.05\n\n\n6\nhad\n47\n2,975\n7.98\n111.88\n\n\n7\nearned\n22\n159\n7.78\n145.70\n\n\n8\nwould\n43\n4,688\n7.49\n63.27\n\n\n9\nis\n59\n7,673\n7.48\n70.93\n\n\n10\nwill\n49\n5,951\n7.47\n63.94\n\n\n11\ndid\n20\n673\n7.43\n70.77\n\n\n12\nwas\n46\n5,826\n7.40\n57.12\n\n\n13\ntoday\n15\n1,445\n6.75\n25.03\n\n\n14\n.\n165\n49,406\n6.69\n35.53\n\n\n15\nadded\n13\n1,116\n6.65\n24.16\n\n\n16\nexpects\n11\n628\n6.59\n28.20\n\n\n17\nmight\n10\n402\n6.54\n32.04\n\n\n18\ndoes\n9\n408\n6.38\n26.84\n\n\n19\nlost\n8\n150\n6.32\n37.38\n\n\n20\nearlier\n10\n1,100\n6.28\n14.57\n\n\n\nReport based on word and punctuation tokens\n\n\nContext tokens left: 0, context tokens right: 1\n\n\nFiltered tokens by minimum collocation frequency (5)\n\n\nUnique collocates: 49\n\n\nShowing 20 rows\n\n\nPage 1 of 3",
    "crumbs": [
      "API",
      "conc"
    ]
  },
  {
    "objectID": "api/concordance.html",
    "href": "api/concordance.html",
    "title": "concordance",
    "section": "",
    "text": "There are examples below showing how to use the Concordance class directly to output concordances or concordance plots. The recommended way to use this functionality is through the Conc class. This provides an interface to create frequency lists, concordances, collocation tables, keyword tables and more.",
    "crumbs": [
      "API",
      "concordance"
    ]
  },
  {
    "objectID": "api/concordance.html#using-the-concordance-class",
    "href": "api/concordance.html#using-the-concordance-class",
    "title": "concordance",
    "section": "",
    "text": "There are examples below showing how to use the Concordance class directly to output concordances or concordance plots. The recommended way to use this functionality is through the Conc class. This provides an interface to create frequency lists, concordances, collocation tables, keyword tables and more.",
    "crumbs": [
      "API",
      "concordance"
    ]
  },
  {
    "objectID": "api/concordance.html#concordance-class-api-reference",
    "href": "api/concordance.html#concordance-class-api-reference",
    "title": "concordance",
    "section": "Concordance class API reference",
    "text": "Concordance class API reference\n\nsource\n\nConcordance\n\n Concordance (corpus:conc.corpus.Corpus)\n\nClass for concordancing.\n\n\n\n\nType\nDetails\n\n\n\n\ncorpus\nCorpus\nCorpus instance\n\n\n\n\n# load the corpus\nbrown = Corpus('brown').load(path_to_brown_corpus)\n\n\n# instantiate the Concordance class\nreport_brown = Concordance(brown)\n\n\nsource\n\n\nConcordance.concordance\n\n Concordance.concordance (token_str:str, context_length:int=5,\n                          order:str='1R2R3R', page_size:int=20,\n                          page_current:int=1, show_all_columns:bool=False,\n                          use_cache:bool=True,\n                          ignore_punctuation:bool=True,\n                          filter_context_str:str|None=None,\n                          filter_context_length:int|tuple[int,int]=5)\n\nReport concordance for a token string.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntoken_str\nstr\n\ntoken string to get concordance for\n\n\ncontext_length\nint\n5\nnumber of words to show on left and right of token string\n\n\norder\nstr\n1R2R3R\norder of sort columns - one of 1L2L3L, 3L2L1L, 2L1L1R, 1L1R2R, 1R2R3R, LEFT, RIGHT\n\n\npage_size\nint\n20\nnumber of results to display per results page\n\n\npage_current\nint\n1\ncurrent page of results\n\n\nshow_all_columns\nbool\nFalse\ndf with all columns or just essentials\n\n\nuse_cache\nbool\nTrue\nretrieve the results from cache if available (currently ignored)\n\n\nignore_punctuation\nbool\nTrue\nwhether to ignore punctuation in the concordance sort\n\n\nfilter_context_str\nstr | None\nNone\nif a string is provided, the concordance lines will be filtered to show lines with contexts containing this string\n\n\nfilter_context_length\nint | tuple[int, int]\n5\nignored if filter_context_str is None, otherwise this is the context window size per side in tokens - if an int (e.g. 5) context lengths on left and right will be the same, for independent control of left and right context length pass a tuple (context_length_left, context_left_right)\n\n\nReturns\nResult\n\nconcordance report results\n\n\n\nWhen configuring order, you can use LEFT as a shortcut for 1L2L3L and RIGHT as a shortcut for 1R2R3R.\n\nExamples\nSee the note above about accessing this functionality through the Conc class.\n\nreport_brown.concordance('good at', context_length = 10, order='1R2R3R').display()\n\n\n\n\n\n\n\nConcordance for \"good at\"\n\n\nBrown Corpus, Context tokens: 10, Order: 1R2R3R\n\n\nDoc Id\nLeft\nNode\nRight\n\n\n\n\n484\nabout twenty miles away , and he was also pretty\ngood at\nanything in the carpentry line . was a vivid ,\n\n\n263\nhe says , ' as a storyteller and was precociously\ngood at\ndescription , dialogue , and most of the other staples\n\n\n479\nand not a method of passing the day . was\ngood at\nhis job . probably was n't hard for him to\n\n\n474\ntrying to flatter her vanity . You must have been\ngood at\nhistory at school . did you go to school ''\n\n\n82\nenough of unequal merit , but all of them pretty\ngood at\nthat . consisted of a new arrangement of ` `\n\n\n474\nWhy not '' ? ? said . I 'm not\ngood at\nthat kind of thing '' . This afternoon let 's\n\n\n\nTotal Concordance Lines: 6\n\n\nTotal Documents: 5\n\n\nShowing 6 lines\n\n\nPage 1 of 1\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\nConcordance.concordance_plot\n\n Concordance.concordance_plot (token_str:str, page_size:int=10,\n                               append_info:bool=True)\n\nCreate a concordance plot.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntoken_str\nstr\n\ntoken string for concordance plot\n\n\npage_size\nint\n10\nnumber of plots per page\n\n\nappend_info\nbool\nTrue\nappend token position info to the concordance line preview screens visible when hover over the plot lines\n\n\nReturns\nPlot\n\nconcordance plot object, add .display() to view in notebook\n\n\n\n\nExamples\nSee the note above about accessing this functionality through the Conc class.\n\nconc_reuters.concordance_plot(token_str='cause', page_size=10, append_info=True).display()\n\n\nConcordance Plot for \"cause\"Reuters CorpusTotal Documents: 121Total Concordance Lines: 135Page 1 of 13",
    "crumbs": [
      "API",
      "concordance"
    ]
  },
  {
    "objectID": "api/ngrams.html",
    "href": "api/ngrams.html",
    "title": "ngrams",
    "section": "",
    "text": "There are examples below showing how to use the Ngrams class directly to output Ngram clusters or frequency lists based on ngrams. The recommended way to use this functionality is through the Conc class. This provides an interface to create frequency lists, concordances, collocation tables, keyword tables and more.",
    "crumbs": [
      "API",
      "ngrams"
    ]
  },
  {
    "objectID": "api/ngrams.html#using-the-ngrams-class",
    "href": "api/ngrams.html#using-the-ngrams-class",
    "title": "ngrams",
    "section": "",
    "text": "There are examples below showing how to use the Ngrams class directly to output Ngram clusters or frequency lists based on ngrams. The recommended way to use this functionality is through the Conc class. This provides an interface to create frequency lists, concordances, collocation tables, keyword tables and more.",
    "crumbs": [
      "API",
      "ngrams"
    ]
  },
  {
    "objectID": "api/ngrams.html#ngrams-class-api-reference",
    "href": "api/ngrams.html#ngrams-class-api-reference",
    "title": "ngrams",
    "section": "Ngrams class API reference",
    "text": "Ngrams class API reference\n\nsource\n\nNgrams\n\n Ngrams (corpus:conc.corpus.Corpus)\n\nClass for n-gram analysis reporting.\n\n\n\n\nType\nDetails\n\n\n\n\ncorpus\nCorpus\nCorpus instance\n\n\n\n\nsource\n\n\nNgrams.ngrams\n\n Ngrams.ngrams (token_str:str, ngram_length:int|None=2,\n                ngram_token_position:str='LEFT', normalize_by:int=10000,\n                page_size:int=20, page_current:int=1,\n                show_all_columns:bool=False,\n                exclude_punctuation:bool=True, use_cache:bool=True)\n\nReport ngram frequencies containing a token string.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntoken_str\nstr\n\ntoken string to get ngrams for\n\n\nngram_length\nint | None\n2\nlength of ngram, if set to None it will use the number of tokens in the token_str + 1\n\n\nngram_token_position\nstr\nLEFT\nspecify if token sequence is on LEFT or RIGHT or MIDDLE (support for other positions is in-development)\n\n\nnormalize_by\nint\n10000\nnormalize frequencies by a number (e.g. 10000)\n\n\npage_size\nint\n20\nnumber of results to display per results page\n\n\npage_current\nint\n1\ncurrent page of results\n\n\nshow_all_columns\nbool\nFalse\nreturn raw df with all columns or just ngram and frequency\n\n\nexclude_punctuation\nbool\nTrue\ndo not return ngrams with punctuation tokens\n\n\nuse_cache\nbool\nTrue\nretrieve the results from cache if available (currently ignored)\n\n\nReturns\nResult\n\nreturn a Result object with ngram data\n\n\n\n\nExamples\nSee the note above about accessing this functionality through the Conc class.\n\n# load the corpus\nreuters = Corpus().load(path_to_reuters_corpus)\n\n# instantiate the Ngrams class\nngrams_reuters = Ngrams(reuters)\n\n\n# run the ngrams method and display the results\nngrams_reuters.ngrams('environmental', ngram_length = 2, ngram_token_position = 'LEFT').display()\n\n\n\n\n\n\n\nNgrams for \"environmental\"\n\n\nReuters Corpus\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nenvironmental protection\n4\n0.03\n\n\n2\nenvironmental systems\n4\n0.03\n\n\n3\nenvironmental services\n3\n0.02\n\n\n4\nenvironmental damage\n2\n0.01\n\n\n5\nenvironmental regulations\n2\n0.01\n\n\n6\nenvironmental impact\n2\n0.01\n\n\n7\nenvironmental controls\n1\n0.01\n\n\n8\nenvironmental approval\n1\n0.01\n\n\n9\nenvironmental and\n1\n0.01\n\n\n10\nenvironmental sciences\n1\n0.01\n\n\n11\nenvironmental service\n1\n0.01\n\n\n12\nenvironmental concerns\n1\n0.01\n\n\n13\nenvironmental issues\n1\n0.01\n\n\n14\nenvironmental power\n1\n0.01\n\n\n15\nenvironmental plan\n1\n0.01\n\n\n16\nenvironmental had\n1\n0.01\n\n\n17\nenvironmental control\n1\n0.01\n\n\n18\nenvironmental management\n1\n0.01\n\n\n19\nenvironmental subsidiary\n1\n0.01\n\n\n20\nenvironmental was\n1\n0.01\n\n\n\nReport based on word tokens\n\n\nNgram length: 2, Token position: left\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 21\n\n\nTotal ngrams: 32\n\n\nShowing 20 rows\n\n\nPage 1 of 2\n\n\n\n\n\n\n\n\n\n\n# run the ngrams method and display the results\nngrams_reuters.ngrams('the highest', ngram_length = 3, ngram_token_position = 'LEFT', page_size = 10).display()\n\n\n\n\n\n\n\nNgrams for \"the highest\"\n\n\nReuters Corpus\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe highest since\n8\n0.06\n\n\n2\nthe highest level\n4\n0.03\n\n\n3\nthe highest in\n3\n0.02\n\n\n4\nthe highest rate\n2\n0.01\n\n\n5\nthe highest interest\n2\n0.01\n\n\n6\nthe highest priority\n2\n0.01\n\n\n7\nthe highest number\n2\n0.01\n\n\n8\nthe highest agriculture\n2\n0.01\n\n\n9\nthe highest such\n2\n0.01\n\n\n10\nthe highest positive\n2\n0.01\n\n\n\nReport based on word tokens\n\n\nNgram length: 3, Token position: left\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 31\n\n\nTotal ngrams: 50\n\n\nShowing 10 rows\n\n\nPage 1 of 4\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\nNgrams.ngram_frequencies\n\n Ngrams.ngram_frequencies (ngram_length:int=2, case_sensitive:bool=False,\n                           normalize_by:int=10000, page_size:int=20,\n                           page_current:int=1,\n                           show_document_frequency:bool=False,\n                           exclude_punctuation:bool=True)\n\nReport frequent ngrams.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nngram_length\nint\n2\nlength of ngram\n\n\ncase_sensitive\nbool\nFalse\nfrequencies for tokens lowercased or with case preserved\n\n\nnormalize_by\nint\n10000\nnormalize frequencies by a number (e.g. 10000)\n\n\npage_size\nint\n20\nnumber of rows to return\n\n\npage_current\nint\n1\ncurrent page\n\n\nshow_document_frequency\nbool\nFalse\nshow document frequency in output\n\n\nexclude_punctuation\nbool\nTrue\nexclude ngrams containing punctuation tokens\n\n\nReturns\nResult\n\nreturn a Result object with the frequency table\n\n\n\nNgram frequencies is the slowest operation in Conc currently and will be optimised in the future.\n\nExamples\nSee the note above about accessing this functionality through the Conc class.\n\nngrams_reuters.ngram_frequencies(ngram_length = 3, case_sensitive = False).display()\n\n\n\n\n\n\n\nNgram Frequencies\n\n\nReuters Corpus\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe company said\n1,173\n8.39\n\n\n2\nmln dlrs in\n795\n5.68\n\n\n3\ncts vs loss\n665\n4.75\n\n\n4\nsaid it has\n636\n4.55\n\n\n5\nmln avg shrs\n620\n4.43\n\n\n6\npct of the\n608\n4.35\n\n\n7\nthe united states\n603\n4.31\n\n\n8\nqtr net shr\n574\n4.10\n\n\n9\ndlrs a share\n546\n3.90\n\n\n10\ninc said it\n523\n3.74\n\n\n11\nthe company 's\n518\n3.70\n\n\n12\ncts net loss\n517\n3.70\n\n\n13\nthe end of\n501\n3.58\n\n\n14\ncts a share\n494\n3.53\n\n\n15\nis expected to\n429\n3.07\n\n\n16\ncorp said it\n412\n2.95\n\n\n17\nnine mths shr\n412\n2.95\n\n\n18\nsaid in a\n407\n2.91\n\n\n19\nthe bank of\n380\n2.72\n\n\n20\nbillion dlrs in\n373\n2.67\n\n\n\nReport based on word tokens\n\n\nNgram length: 3\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 684,778\n\n\nTotal ngrams: 1,128,352\n\n\nShowing 20 rows\n\n\nPage 1 of 34239\n\n\n\n\n\n\n\n\n\n\nngrams_reuters.ngram_frequencies(ngram_length = 3, case_sensitive = True).display()\n\n\n\n\n\n\n\nNgram Frequencies\n\n\nReuters Corpus\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nThe company said\n747\n5.34\n\n\n2\nmln dlrs in\n726\n5.19\n\n\n3\ncts vs loss\n645\n4.61\n\n\n4\nsaid it has\n632\n4.52\n\n\n5\nmln Avg shrs\n615\n4.40\n\n\n6\npct of the\n608\n4.35\n\n\n7\nQTR NET Shr\n559\n4.00\n\n\n8\nthe United States\n524\n3.75\n\n\n9\ndlrs a share\n519\n3.71\n\n\n10\nInc said it\n514\n3.67\n\n\n11\ncts Net loss\n509\n3.64\n\n\n12\nthe end of\n501\n3.58\n\n\n13\ncts a share\n490\n3.50\n\n\n14\nthe company 's\n476\n3.40\n\n\n15\nthe company said\n426\n3.05\n\n\n16\nis expected to\n426\n3.05\n\n\n17\nsaid in a\n407\n2.91\n\n\n18\nCorp said it\n392\n2.80\n\n\n19\nNine mths Shr\n370\n2.65\n\n\n20\ncts Oper net\n363\n2.60\n\n\n\nReport based on word tokens\n\n\nNgram length: 3\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 702,051\n\n\nTotal ngrams: 1,128,352\n\n\nShowing 20 rows\n\n\nPage 1 of 35103\n\n\n\n\n\n\n\n\n\n\nngrams_reuters.ngram_frequencies(ngram_length = 3, case_sensitive = False, show_document_frequency = True).display()\n\n\n\n\n\n\n\nNgram Frequencies\n\n\nReuters Corpus\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\nDocument Frequency\n\n\n\n\n1\nthe company said\n1,173\n8.39\n911\n\n\n2\nmln dlrs in\n795\n5.68\n549\n\n\n3\ncts vs loss\n665\n4.75\n474\n\n\n4\nsaid it has\n636\n4.55\n586\n\n\n5\nmln avg shrs\n620\n4.43\n450\n\n\n6\npct of the\n608\n4.35\n508\n\n\n7\nthe united states\n603\n4.31\n391\n\n\n8\nqtr net shr\n574\n4.10\n573\n\n\n9\ndlrs a share\n546\n3.90\n375\n\n\n10\ninc said it\n523\n3.74\n521\n\n\n11\nthe company 's\n518\n3.70\n406\n\n\n12\ncts net loss\n517\n3.70\n389\n\n\n13\nthe end of\n501\n3.58\n384\n\n\n14\ncts a share\n494\n3.53\n290\n\n\n15\nis expected to\n429\n3.07\n381\n\n\n16\ncorp said it\n412\n2.95\n410\n\n\n17\nnine mths shr\n412\n2.95\n412\n\n\n18\nsaid in a\n407\n2.91\n393\n\n\n19\nthe bank of\n380\n2.72\n304\n\n\n20\nbillion dlrs in\n373\n2.67\n235\n\n\n\nReport based on word tokens\n\n\nNgram length: 3\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 684,778\n\n\nTotal ngrams: 1,128,352\n\n\nShowing 20 rows\n\n\nPage 1 of 34239",
    "crumbs": [
      "API",
      "ngrams"
    ]
  },
  {
    "objectID": "api/corpus.html",
    "href": "api/corpus.html",
    "title": "corpus",
    "section": "",
    "text": "source\n\n\n\n Corpus (name:str='', description:str='')\n\nRepresention of text corpus, with methods to build, load and save a corpus from a variety of formats and to work with the corpus data.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nname of corpus\n\n\ndescription\nstr\n\ndescription of corpus",
    "crumbs": [
      "API",
      "corpus"
    ]
  },
  {
    "objectID": "api/corpus.html#corpus-class",
    "href": "api/corpus.html#corpus-class",
    "title": "corpus",
    "section": "",
    "text": "source\n\n\n\n Corpus (name:str='', description:str='')\n\nRepresention of text corpus, with methods to build, load and save a corpus from a variety of formats and to work with the corpus data.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nname of corpus\n\n\ndescription\nstr\n\ndescription of corpus",
    "crumbs": [
      "API",
      "corpus"
    ]
  },
  {
    "objectID": "api/corpus.html#build-and-save-a-corpus",
    "href": "api/corpus.html#build-and-save-a-corpus",
    "title": "corpus",
    "section": "Build and save a corpus",
    "text": "Build and save a corpus\nConc defines a punctuation token as a token consisting only of punctuation characters. Punctuation characters are defined by combining the Python string.punctuation characters with unicode characters categorised as punctuation (i.e. unicode characters with general category starting with P) or currency symbols (i.e. general category Sc). This means, for example, that various versions of a dashes or quotation marks can be identified as punctuation. This also means that any emoticons that are based on sequences of punctuation characters, like :), will be defined as a punctuation token. To access reporting on punctuation is still possible in Conc reports using the relevant parameters. There are still many unicode symbol characters that are not defined as punctuation by Conc. This may change in future versions of Conc, including the ability to define punctuation strings or exceptions. Any changes will be documented.\n\nprint(len(PUNCTUATION_STRINGS))\nprint(PUNCTUATION_STRINGS)\n\n890\n𐩿᭟£﹚”᰿₢𑜾᳁𐫳𑩅᪩﹝𐩓꣺᭝︖‹⁓❵﹏⸰𑅂߸﹣፨꧉‗᠂–︰⸎𐮛⸻‰𑑎⹘𑗆𑩂€՜𑃀𑅵࠻﹨𑓆𑱄⁅𒑱&lt;⟦⸹︶𐮜᳅߿⦗}｛〕𖩮⁖⳻—᨟„𑅁'᭜࠸𐽗𐬼᰼𖺚؋࿒𝪉）𑙥፡𑑍︓⦑〜゠𒑴𖺘𑪞؍𑥄𖬹꧞₣₫𑅴𑗍⸦࠼/᠀࠶❨⸟𑗗။᳂⧽⸢᠈𑙡𐬻⸤⟬*／𑇞⹅､︼꧇𑜽𖭄𑥆•︳꧄＄𑈽⹗᙮᛫𑗋᜵⧙𑈼₲﹋？༈⹊⦎·=]︑﹆၏﹛﹠𑻷࠽𑧢𑿠؛܈᭚﹖𐏐꧂᐀【‿꧃𑗑፠𑱁⸂𑇛₦❮︿𑗇𐺭־⸇𑇅⸲𑗅⦋⸙〉‸׆𐄁[٭᠅︻᳓〰﹙⸽𑪢𖬸❲𑙃࠺𒿱༑⸃﹅༆❪⦊⟨꧈՛₧𐩔﹕¤―𐡗₶࠹⁏₹＂⸶𐤿𑙫･﹪꧌₠᳇𐩖꛷࠱᱾〔・𐄀𑗂𞥟︱𑗏៛܍៘𑗕❬꧁꡴›₤࠴⁃׃꫱׳…⸋؊❴〈؝⁆⁑꤮𐩗𑁇࠰⸺『𒑳𑙠⸒￥〖﷼⁜⟯⸍𖫵៖﹁׀《᚛꧊⹍𖬻⦆⁌܆⸧𑗉︷𐬺᨞⸖⹔⸫⸵𐫶⹃𑇇𝪊𑜼!࿔⁍⦔⹛꙳𑨿⦌″⹏꡷︔༇⁉՝︐⸌𑿿𑿟|᭠⁀𐩑་｢⦉၌᥅❱𐫱︴𑗒𐤟꤯₩₵¡၎‐⦄₴†⸨❯𐩕₨𐬾⸆⌊₪՟𑱂༊﹇𖺙［⹒❫⸓༽𐩒᠇𑪛⁕𑑚᠃⹇౷。₾𐮚᯿⸝⧚᯽᪨𑑌』𒑲᪠·₼꧟％)۔⸿⹌-︒￦❰𛲟⸾᚜⁾𑗌𑪠𖺗༄𑱃﹐𖬷၊᛬﹡⹂‑❩﴾𑱰‟꫰᠄⧛⌈𑻸᳆꛵¿࠷𑇆𐬿‾;֊༉༏՞⧘⸣𑑝𑙩₍࿓៕〟፥⟪༌𑩃᜶〃܃，𑙦‶⸪᭾⦓︸๚𑙪;𑗖𐾆᪭𑇈᳃₮𑗔￡𑊩৻฿។⦐៚﹉⸑꡵𐽙𐬹﹑₯‴⸳꫞⸛⸔\\༐﴿𑗓𑩀※𐾈܋⁙⸀𑑛｝𑁉⸞⁊〛〝꛶’〚⁽࠲𑩁꛲܀༅⸭྅؞\"᰽᪤⁁﹍﹟༻܅։⸱।#⸊⁈︙૰৲‥︕࿑⸕𐽖⸠𐩘𐽘𐽕꫟𑗁᱿&꩝᪬⹀᳀`⸏⸄꧋፣؉܂】࠵꣎꣸﹞₺₻‘৳⟆𐾉፦⹙٬𑠻𑿝࿐܉「᪣〘᯾⸜〞჻𑁌𖩯﹌༺𑇝§𑗎༒𑙬（.𐾇࠾⸩𖿢′︽﹊།॰¢‡𐬽｟⳾꓾⹉＃$‷⸘‒〽．𑅃｡］᯼⸷₷{꣹𐫵᰾܄⹈꣏＆૱꧆⦃₡‱‧〙｠﹔᪫‣﹜𐫲𑂿𖬺๛⹁꛳⦕𒿲₳⦖᰻𑁍⟧⦇‽?‛⌋₿⸅＊᠉৽᠁𑪜；․⁋⦍⹄߾॥࿙»⹝᭽﹎‖꧅︺՚＿಄‚_๏⁂𑑏⳿꣼༔₰⸸﹗܊᠊𑩆₎᪥𑈸߹⧼⸡〈❭𑁋𑇟״(𑪟٪𐩐꯫⹖꩞‼࿚⳺^﹃⦅⸉꘎༎𞋿⁗𑙧︵꩜𑈺₥꘏߷၍⁐،,⁛︘⟅⸗᪢⳹¶⌉꓿⸮꛴⸈₸：︾𑙁࡞𝪇⁎⟭¥᭛𝪋੶︹𐫴𞥞𐄂𑂼‵❳𑅀﹫𝪈«、𐫰⁞꥟፤𐎟⸁⦏༼𑪚￠𑗊⁔᪡₭᛭᥄𑂻﹈𑁈𑥅܌₽𑗄－𑗃𑗐꧍﹄𑗈𑱅᠆᭞﹂&gt;﹀﹘⁇%⵰𑿞⟮》⹋﹩﹒܇⟫+⦘᳄」⁚᪪᪦⹆＼⸥@꘍︲:࠳𑙤꡶⹜𑃁؟𐮙𑈻௹𑪡𑑋៙~⟩₱⹎〉𑱱꙾𞲰⸬⦈⃀⸴𑚹⸐〗︗＠𑁊𑙨٫𑂾⦒⹚𑙢⹓෴⸼𐕯⁘𑈹“𑙣֏⳼𑇍！𒑰⸚⁝𑩄𑙂⹕።｣＇꠸፧܁꩟\n\n\nSpacy includes space tokens in the vocab for non-destructive tokenisation. Positions of space tokens are stored so they can be filtered out for analysis and reporting.\nTokens consisting of only punctuation are defined as punctuation tokens. These can be removed or included in analysis and reporting.\nNOTE: currently streaming either with sink_parquet or collect(engine=‘streaming’) can break the order of the dataframe (not just whole rows, but within specific columns leading to misaligned data). Streaming is not being used for the build, this will be reassessed in the future as the new Polars streaming functionality matures.\n\nsource\n\nCorpus.save_corpus_metadata\n\n Corpus.save_corpus_metadata (template:str='# {name}\\n\\n## About\\n\\nThis\n                              directory contains a corpus created using\n                              the [Conc]({REPOSITORY_URL}) Python library.\n                              \\n\\n## Corpus\n                              Information\\n\\n{description}\\n\\nDate\n                              created: {date_created}  \\nDocument count:\n                              {document_count}  \\nToken count:\n                              {token_count}  \\nWord token count:\n                              {word_token_count}  \\nUnique tokens:\n                              {unique_tokens}  \\nUnique word tokens:\n                              {unique_word_tokens}  \\nConc Version Number:\n                              {conc_version}  \\nspaCy model:\n                              {SPACY_MODEL}, version {SPACY_MODEL_VERSION}\n                              \\n\\n## Using this corpus\\n \\nConc can be\n                              installed [via pip]({PYPI_URL}). The [Conc\n                              documentation site]({DOCUMENTATION_URL})\n                              \\nhas tutorials and detailed information to\n                              get you started with Conc or to work with\n                              the corpus \\ndata directly.  \\n\\n## Cite\n                              Conc\\n\\n{CITATION_STR}\\n\\n')\n\nSave corpus metadata.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntemplate\nstr\n# {name}## AboutThis directory contains a corpus created using the Conc Python library. ## Corpus Information{description}Date created: {date_created} Document count: {document_count} Token count: {token_count} Word token count: {word_token_count} Unique tokens: {unique_tokens} Unique word tokens: {unique_word_tokens} Conc Version Number: {conc_version} spaCy model: {SPACY_MODEL}, version {SPACY_MODEL_VERSION} ## Using this corpus Conc can be installed via pip. The Conc documentation site has tutorials and detailed information to get you started with Conc or to work with the corpus data directly. ## Cite Conc{CITATION_STR}\ntemplate for the README file\n\n\n\n\nsource\n\n\nCorpus.build_from_files\n\n Corpus.build_from_files (source_path:str, save_path:str,\n                          file_mask:str='*.txt',\n                          metadata_file:str|None=None,\n                          metadata_file_column:str='file',\n                          metadata_columns:list[str]=[],\n                          encoding:str='utf-8',\n                          model:str='en_core_web_sm',\n                          spacy_batch_size:int=1000,\n                          build_process_batch_size:int=5000,\n                          build_process_cleanup:bool=True, standardize_wor\n                          d_token_punctuation_characters:bool=False)\n\nBuild a corpus from text files in a folder.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_path\nstr\n\npath to folder with text files, path can be a directory, zip or tar/tar.gz file\n\n\nsave_path\nstr\n\ndirectory where corpus will be created, a subdirectory will be automatically created with the corpus content\n\n\nfile_mask\nstr\n*.txt\nmask to select files\n\n\nmetadata_file\nstr | None\nNone\npath to a CSV with metadata\n\n\nmetadata_file_column\nstr\nfile\ncolumn in metadata file with file names to align texts with metadata\n\n\nmetadata_columns\nlist\n[]\nlist of column names to import from metadata\n\n\nencoding\nstr\nutf-8\nencoding of text files\n\n\nmodel\nstr\nen_core_web_sm\nspacy model to use for tokenisation\n\n\nspacy_batch_size\nint\n1000\nbatch size for spacy tokenizer\n\n\nbuild_process_batch_size\nint\n5000\nsave in-progress build to disk every n docs\n\n\nbuild_process_cleanup\nbool\nTrue\nRemove the build files after build is complete, retained for development and testing purposes\n\n\nstandardize_word_token_punctuation_characters\nbool\nFalse\nwhether to standardize apostrophes in word tokens\n\n\n\n\nsource\n\n\nCorpus.build_from_csv\n\n Corpus.build_from_csv (source_path:str, save_path:str,\n                        text_column:str='text',\n                        metadata_columns:list[str]=[],\n                        encoding:str='utf8', model:str='en_core_web_sm',\n                        spacy_batch_size:int=1000,\n                        build_process_batch_size:int=5000,\n                        build_process_cleanup:bool=True, standardize_word_\n                        token_punctuation_characters:bool=False)\n\nBuild a corpus from a csv file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_path\nstr\n\npath to csv file\n\n\nsave_path\nstr\n\ndirectory where corpus will be created, a subdirectory will be automatically created with the corpus content\n\n\ntext_column\nstr\ntext\ncolumn in csv with text\n\n\nmetadata_columns\nlist\n[]\nlist of column names to import from csv\n\n\nencoding\nstr\nutf8\nencoding of csv passed to Polars read_csv, see their documentation\n\n\nmodel\nstr\nen_core_web_sm\nspacy model to use for tokenisation\n\n\nspacy_batch_size\nint\n1000\nbatch size for Spacy tokenizer\n\n\nbuild_process_batch_size\nint\n5000\nsave in-progress build to disk every n docs\n\n\nbuild_process_cleanup\nbool\nTrue\nRemove the build files after build is complete, retained for development and testing purposes\n\n\nstandardize_word_token_punctuation_characters\nbool\nFalse\nwhether to standardize apostrophes in word tokens",
    "crumbs": [
      "API",
      "corpus"
    ]
  },
  {
    "objectID": "api/corpus.html#load-a-corpus",
    "href": "api/corpus.html#load-a-corpus",
    "title": "corpus",
    "section": "Load a corpus",
    "text": "Load a corpus\n\nsource\n\nCorpus.load\n\n Corpus.load (corpus_path:str)\n\nLoad corpus from disk and load the corresponding spaCy model.\n\n\n\n\nType\nDetails\n\n\n\n\ncorpus_path\nstr\npath to load corpus",
    "crumbs": [
      "API",
      "corpus"
    ]
  },
  {
    "objectID": "api/corpus.html#information-about-the-corpus",
    "href": "api/corpus.html#information-about-the-corpus",
    "title": "corpus",
    "section": "Information about the corpus",
    "text": "Information about the corpus\n\nsource\n\nCorpus.info\n\n Corpus.info (include_disk_usage:bool=False, formatted:bool=True)\n\nReturn information about the corpus.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninclude_disk_usage\nbool\nFalse\ninclude information of size on disk in output\n\n\nformatted\nbool\nTrue\nreturn formatted output\n\n\nReturns\nstr\n\nformatted information about the corpus\n\n\n\n\nsource\n\n\nCorpus.report\n\n Corpus.report (include_memory_usage:bool=False)\n\nGet information about the corpus as a result object.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninclude_memory_usage\nbool\nFalse\ninclude memory usage in output\n\n\nReturns\nResult\n\nreturns Result object with corpus summary information\n\n\n\n\nsource\n\n\nCorpus.summary\n\n Corpus.summary (include_memory_usage:bool=False)\n\nPrint information about the corpus in a formatted table.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninclude_memory_usage\nbool\nFalse\ninclude memory usage in output\n\n\n\nYou can get summary information on your corpus, including the number of documents, the token count and the number of unique tokens as a dataframe using the info method. You can also just print the corpus itself.\n\nprint(brown) # equivalent to print(brown.info())\n\n┌────────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│ Attribute          ┆ Value                                                                                                                                                                                                                                              │\n╞════════════════════╪════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╡\n│ Name               ┆ Brown Corpus                                                                                                                                                                                                                                       │\n│ Description        ┆ A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 │\n│                    ┆ http://www.hit.uib.no/icame/brown/bcm.html. This version …                                                                                                                                                                                         │\n│ Date Created       ┆ 2025-07-02 15:00:59                                                                                                                                                                                                                                │\n│ Conc Version       ┆ 0.1.5                                                                                                                                                                                                                                              │\n│ Corpus Path        ┆ /home/geoff/data/conc-test-corpora/brown.corpus                                                                                                                                                                                                    │\n│ Document Count     ┆ 500                                                                                                                                                                                                                                                │\n│ Token Count        ┆ 1,138,566                                                                                                                                                                                                                                          │\n│ Word Token Count   ┆ 980,144                                                                                                                                                                                                                                            │\n│ Unique Tokens      ┆ 42,930                                                                                                                                                                                                                                             │\n│ Unique Word Tokens ┆ 42,907                                                                                                                                                                                                                                             │\n└────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\nThe info method can also provide information on the disk usage of the corpus setting the include_disk_usage parameter to True.\n\nprint(brown.info(include_disk_usage=True))\n\n┌────────────────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│ Attribute                  ┆ Value                                                                                                                                                                                                                                              │\n╞════════════════════════════╪════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╡\n│ Name                       ┆ Brown Corpus                                                                                                                                                                                                                                       │\n│ Description                ┆ A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 │\n│                            ┆ http://www.hit.uib.no/icame/brown/bcm.html. This version …                                                                                                                                                                                         │\n│ Date Created               ┆ 2025-07-02 15:00:59                                                                                                                                                                                                                                │\n│ Conc Version               ┆ 0.1.5                                                                                                                                                                                                                                              │\n│ Corpus Path                ┆ /home/geoff/data/conc-test-corpora/brown.corpus                                                                                                                                                                                                    │\n│ Document Count             ┆ 500                                                                                                                                                                                                                                                │\n│ Token Count                ┆ 1,138,566                                                                                                                                                                                                                                          │\n│ Word Token Count           ┆ 980,144                                                                                                                                                                                                                                            │\n│ Unique Tokens              ┆ 42,930                                                                                                                                                                                                                                             │\n│ Unique Word Tokens         ┆ 42,907                                                                                                                                                                                                                                             │\n│ Corpus Metadata (Mb)       ┆ 0.001                                                                                                                                                                                                                                              │\n│ Document Metadata (Mb)     ┆ 0.001                                                                                                                                                                                                                                              │\n│ Tokens (Mb)                ┆ 4.468                                                                                                                                                                                                                                              │\n│ Vocab (Mb)                 ┆ 0.678                                                                                                                                                                                                                                              │\n│ Punctuation Positions (Mb) ┆ 0.425                                                                                                                                                                                                                                              │\n│ Space Positions (Mb)       ┆ 0.012                                                                                                                                                                                                                                              │\n└────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\nYou can get the same information in a table format by using the summary method.\n\nbrown.summary()\n\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nName\nBrown Corpus\n\n\nDescription\nA Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.\n\n\nDate Created\n2025-07-02 15:00:59\n\n\nConc Version\n0.1.5\n\n\nCorpus Path\n/home/geoff/data/conc-test-corpora/brown.corpus\n\n\nDocument Count\n500\n\n\nToken Count\n1,138,566\n\n\nWord Token Count\n980,144\n\n\nUnique Tokens\n42,930\n\n\nUnique Word Tokens\n42,907",
    "crumbs": [
      "API",
      "corpus"
    ]
  },
  {
    "objectID": "api/corpus.html#working-with-tokens",
    "href": "api/corpus.html#working-with-tokens",
    "title": "corpus",
    "section": "Working with tokens",
    "text": "Working with tokens\nInternally, Conc uses Polars and Numpy vector operations where possible to speed up processing.\n\nsource\n\nCorpus.token_ids_to_tokens\n\n Corpus.token_ids_to_tokens (token_ids:numpy.ndarray|list)\n\nGet token strings for a list of token ids.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntoken_ids\nnumpy.ndarray | list\ntoken ids to return token strings for\n\n\nReturns\nndarray\nreturn token strings for token ids\n\n\n\n\nsource\n\n\nCorpus.tokens_to_token_ids\n\n Corpus.tokens_to_token_ids (tokens:list[str]|numpy.ndarray[str])\n\nConvert a list or np.array of token string to token ids\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntokens\nlist[str] | numpy.ndarray[str]\nlist of tokens to get ids for\n\n\nReturns\nndarray\narray of token ids, 0 for unknown tokens\n\n\n\n\nsource\n\n\nCorpus.token_to_id\n\n Corpus.token_to_id (token:str)\n\nGet the token id of a token string.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntoken\nstr\ntoken to get id for\n\n\nReturns\nint\nreturn token id (0 if token not found in the corpus)\n\n\n\nA list or numpy array of token strings can be converted to a numpy array of token ids like this using tokens_to_token_ids …\n\ntokens = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\ntoken_ids = brown.tokens_to_token_ids(tokens)\nprint(token_ids)\n\n[15682 37698 47121 13458   526 16875 22848 25923 23289]\n\n\nTo reverse this use token_ids_to_tokens …\n\ntokens = brown.token_ids_to_tokens(token_ids) # token_ids was set above\nprint(tokens)\n\n['The' 'quick' 'brown' 'fox' 'jumps' 'over' 'the' 'lazy' 'dog']\n\n\nThe tokens_to_token_ids method will return a 0 for any tokens not in the corpus vocabulary.\n\ntokens = ['some', 'random', 'gazupinfava', 'words']\nbrown.tokens_to_token_ids(tokens)\n\narray([21572, 28602,     0, 31327])\n\n\nIf zero is passed to token_ids_to_tokens it will return an error token as shown below. A negative value will raise a ValueError.\n\nbrown.token_ids_to_tokens([0])\n\narray(['ERROR: not a token'], dtype=object)\n\n\nThe token_to_id method wraps tokens_to_token_ids. You can pass a single token string and get the token id back. As with tokens_to_token_ids, if the token is not in the vocabulary it will return 0.\n\nprint(brown.token_to_id('brown')) # returns token id\nprint(brown.token_to_id('Supercalifragilisticexpialidocious')) # returns 0 if token not in corpus\n\n47121\n0\n\n\n\nsource\n\n\nCorpus.token_ids_to_sort_order\n\n Corpus.token_ids_to_sort_order (token_ids:numpy.ndarray|list)\n\nGet the sort order of token strings corresponding to token ids\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntoken_ids\nnumpy.ndarray | list\ntoken ids to return token strings for\n\n\nReturns\nndarray\nrank of token ids\n\n\n\n\ntokens = np.array(['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'])\ntoken_ids = brown.tokens_to_token_ids(tokens)\nsort_order = brown.token_ids_to_sort_order(token_ids)\nsorted_tokens = tokens[np.argsort(sort_order)]\n\nprint(tokens)\nprint(token_ids)\nprint(sort_order)\nprint(sorted_tokens)\n\n['The' 'quick' 'brown' 'fox' 'jumps' 'over' 'the' 'lazy' 'dog']\n[15682 37698 47121 13458   526 16875 22848 25923 23289]\n[50086 40359  7940 20497 27663 35982 50087 29054 15849]\n['brown' 'dog' 'fox' 'jumps' 'lazy' 'over' 'quick' 'The' 'the']\n\n\n\nsource\n\n\nCorpus.get_token_count_text\n\n Corpus.get_token_count_text (exclude_punctuation:bool=False)\n\nGet the token count for the corpus with adjustments and text for output\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexclude_punctuation\nbool\nFalse\nexclude punctuation tokens from the count\n\n\nReturns\ntuple\n\ntoken count with adjustments based on exclusions, token descriptor, total descriptor",
    "crumbs": [
      "API",
      "corpus"
    ]
  },
  {
    "objectID": "api/corpus.html#tokenization",
    "href": "api/corpus.html#tokenization",
    "title": "corpus",
    "section": "Tokenization",
    "text": "Tokenization\n\nsource\n\nCorpus.tokenize\n\n Corpus.tokenize (string:str, simple_indexing=False)\n\nTokenize a string using the Spacy tokenizer.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstring\nstr\n\nstring to tokenize\n\n\nsimple_indexing\nbool\nFalse\nuse simple indexing",
    "crumbs": [
      "API",
      "corpus"
    ]
  },
  {
    "objectID": "api/corpus.html#work-with-specific-texts-in-the-corpus",
    "href": "api/corpus.html#work-with-specific-texts-in-the-corpus",
    "title": "corpus",
    "section": "Work with specific texts in the corpus",
    "text": "Work with specific texts in the corpus\n\nsource\n\nCorpus.text\n\n Corpus.text (doc_id:int)\n\nGet a text document\n\n\n\n\nType\nDetails\n\n\n\n\ndoc_id\nint\nthe id of the document",
    "crumbs": [
      "API",
      "corpus"
    ]
  },
  {
    "objectID": "api/corpus.html#find-positions-of-tokens",
    "href": "api/corpus.html#find-positions-of-tokens",
    "title": "corpus",
    "section": "Find positions of tokens",
    "text": "Find positions of tokens\n\nsource\n\nCorpus.get_tokens_by_index\n\n Corpus.get_tokens_by_index (index:str='orth_index',\n                             exclude_punctuation:bool=False)\n\nGet tokens for a given index.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nindex\nstr\north_index\nindex to get tokens from i.e. ‘orth_index’ ‘lower_index’ ‘token2doc_index’\n\n\nexclude_punctuation\nbool\nFalse\nexclude punctuation tokens from the result (unused currently)\n\n\nReturns\nndarray\n\n\n\n\n\n\nsource\n\n\nCorpus.get_ngrams_by_index\n\n Corpus.get_ngrams_by_index (ngram_length:int, index:str,\n                             exclude_punctuation:bool=False)\n\nGet ngrams for a given index and ngram length.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nngram_length\nint\n\nlength of ngrams to get\n\n\nindex\nstr\n\nindex to get tokens from, e.g. ‘orth_index’ ‘lower_index’\n\n\nexclude_punctuation\nbool\nFalse\nexclude punctuation tokens from the result (unused currently)\n\n\nReturns\nndarray\n\n\n\n\n\n\ntoy.get_ngrams_by_index(ngram_length=2, index='lower_index')[100:110]\n\narray([[10,  6],\n       [ 6, 12],\n       [12,  8],\n       [ 8, 10],\n       [10, 13],\n       [13, 15],\n       [15, 17],\n       [17, 10],\n       [10, 11],\n       [11, 12]], dtype=uint32)\n\n\n\nsource\n\n\nCorpus.get_token_positions\n\n Corpus.get_token_positions (token_sequence:list[numpy.ndarray],\n                             index_id:int, exclude_punctuation:bool=False)\n\nGet the positions of a token sequence in the corpus.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntoken_sequence\nlist\n\ntoken sequence to get index for\n\n\nindex_id\nint\n\nindex to search (i.e. ORTH, LOWER)\n\n\nexclude_punctuation\nbool\nFalse\nexclude punctuation tokens from the result (unused currently)\n\n\nReturns\nndarray\n\npositions of token sequence\n\n\n\n\ntoken_str = 'dog'\ntoken_sequence, index_id = brown.tokenize(token_str, simple_indexing=True)\ntoken_positions = brown.get_token_positions(token_sequence, index_id)\nprint(token_positions)\n\n[array([  18833,   18870,   18880,   18950,   18957,   37578,   88691,\n        125019,  137037,  137687,  137722,  137731,  137775,  143860,\n        188374,  248842,  248982,  249204,  249217,  249243,  249311,\n        249337,  249397,  249425,  249535,  250476,  250495,  250554,\n        250613,  250645,  250699,  250709,  251033,  252740,  253700,\n        255256,  255360,  255532,  330282,  359785,  437987,  437991,\n        438046,  438051,  463456,  463485,  463507,  521175,  648316,\n        694080,  694129,  694289,  694481,  694760,  695139,  695216,\n        695313,  861865,  861872,  863503,  863521,  875531,  875573,\n        875660,  887598,  994901, 1012130, 1028088, 1050598, 1050607,\n       1052032, 1074911, 1084765, 1086020, 1086052, 1086639, 1104994,\n       1128317, 1137426])]\n\n\n\nsource\n\n\nCorpus.get_tokens_in_context\n\n Corpus.get_tokens_in_context (token_positions:numpy.ndarray, index:str,\n                               context_length:int=5,\n                               position_offset:int=1,\n                               position_offset_step:int=1,\n                               exclude_punctuation:bool=True,\n                               convert_eof:bool=True)\n\nGet tokens in context for given token positions, context length and direction, operates one side at a time.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntoken_positions\nndarray\n\nNumpy array of token positions in the corpus\n\n\nindex\nstr\n\nIndex to use - lower_index, orth_index\n\n\ncontext_length\nint\n5\nNumber of context words to consider on each side of the token\n\n\nposition_offset\nint\n1\noffset to start retrieving context words - negatve is left of node, positive for right - may want to adjust if sequence_len &gt; 1\n\n\nposition_offset_step\nint\n1\nstep to move position offset by, this sets direct, -1 for left, 1 for right\n\n\nexclude_punctuation\nbool\nTrue\nignore punctuation from context retrieved\n\n\nconvert_eof\nbool\nTrue\nif True (for collocation functionality), contexts with end of file tokens will have eof token and tokens after set to zero, otherwise EOF retained (e.g. False used for ngrams)\n\n\nReturns\nResult\n\n\n\n\n\n\nsource\n\n\nbuild_test_corpora\n\n build_test_corpora (source_path:str, save_path:str,\n                     force_rebuild:bool=False)\n\n(Deprecated - moved to conc.corpora) Build all test corpora from source files.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_path\nstr\n\npath to folder with corpora\n\n\nsave_path\nstr\n\npath to save corpora\n\n\nforce_rebuild\nbool\nFalse\nforce rebuild of corpora, useful for development and testing\n\n\n\nNote: build_sample_corpora was accessible via conc.corpus as build_test_corpora up to version 0.1.1. Calling it this way will raise a deprecation warning. It will be removed for version 1.0.",
    "crumbs": [
      "API",
      "corpus"
    ]
  },
  {
    "objectID": "development/roadmap.html",
    "href": "development/roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "Conc is in active development and support. The current releases of Conc are Beta releases.\nIt is currently released as a pip-installable Python package. For the latest release, install from Pypi (pip install conc). Add the -U flag to upgrade if you are already running Conc.\nThe code available on Github may be pre-release and may be ahead of the Pypi release. The code on Github may include bug fixes and new functionality, including development that is in-progress, potentially incomplete or subject to change. For the latest pre-release functionality install from Github (pip install git+https://github.com/polsci/conc.git).\nThe documentation reflects the most recent functionality. See the Roadmap below for upcoming features and the CHANGELOG for notes on previous releases.\n\nShort-term\n\nadd tutorial/walkthrough/ getting started notebook\nadd citation information\ndocument use of Conc results in other Python libraries (e.g. Pandas) via tutorial/walkthrough, results module (include link on README principles)\nCorpus tokenize support for functionality from earlier versions of Conc for wildcards, multiple strings, case insensitive tokenization\nextend caching support to all intensive reports, revise storage of cached results for in-memory/disk option\nrelegate some logger warnings to debug level and audit logger messages for consistency and clarity for users\nadd support for build from datasets library\nraise errors for obvious problems with source data when building corpora (e.g. empty column in csv that is supposed to contain text)\nngrams method - implement case handling\nget_ngrams_by_index - implement case handling\nimprove concordance ordering so not fixed options e.g. include 3R1R2R\nimprove ngram support for ngram token position beyond LEFT/RIGHT (i.e. define positions relative to ngram, or ANY)\nconcordancing - add in ordering by metadata columns or doc\nannotations support for spaCy POS, TAG, SENT_START, LEMMA\nshift more processing from in-memory to polars with support for streaming or in-memory processing\nrevisit polars streaming - potentially implement a batched write for very large files i.e. splitting vocab/tokens files into smaller chunks to reduce memory usage.\ncommand line corpus building functionality leveraging nbdev’s CLI functionality\n\n\n\nMedium-term\n\nSupport for processing backends other than spaCy (i.e. other tokenizers)",
    "crumbs": [
      "Development",
      "Roadmap"
    ]
  },
  {
    "objectID": "development/releases.html",
    "href": "development/releases.html",
    "title": "Releases",
    "section": "",
    "text": "This page duplicates information from Conc’s CHANGELOG in the Github repository.\n[0.1.13] - 2025-08-12 - restrict/exclude tokens improvements for keyness, bug fix\n\nChanged\n\nreplacing call to deprecated function in frequency tests\nremoving double filtering of restrict/excluded tokens keyness\nadded tests for restrict tokens in keyness\nadd note on restrict_tokens interaction with other arguments\npinning polars to 1.31.0 based on is_in bug in 1.32.0\nensuring force rebuild of sample corpora forces redownload of source data\n\n\n\nFixed\n\ncorrecting attempt to collect already collected lazyframe in keyness\n\n\n\n\n[0.1.12] - 2025-07-24 - documentation updates, bug fix\n\nAdded\n\nexplanation of keyness and collocate statistics implemented in Conc\n\n\n\nChanged\n\nupdated optional dependencies install instructions\ninstructions for installing on older machines now has pinned Polars version to 1.30.0\n\n\n\nFixed\n\nfix ngrams not returning correct ngram range when token position RIGHT and ngram_length &gt; 1 (includes new tests)\n\n\n\n\n[0.1.11] - 2025-07-23 - Zenodo integrated, citation information added\n\nAdded\n\nadded badge for Zenodo DOI\nadded release badge\nadded citation file\n\n\n\nChanged\n\nupdated citation information to README files generated by Conc on corpus build\n\n\n\n\n[0.1.10] - 2025-07-23 - documentation improvements, initiating Zenodo integration\nThis version is the first released via Github releases. Previous releases were via Pypi only. Github releases are tracked automatically by Zenodo.\n\nChanged\n\nimprovements to API documentation with headings and direction to use Conc class as main interface to reports\n\n\n\n\n[0.1.9] - 2025-07-15 - compatibility with setup on Google Colab\n\nChanged\n\nrequiring numpy&gt;=2.0.0 and polars&gt;=1.30.0, thanks @karinstahel for the feedback about Colab compatibility\n\n\n\n\n[0.1.8] - 2025-07-10 - bug fix so list corpora ready for use after build\n\nFixed\n\ninit the corpus after build so vocab is ready for use\n\n\n\n\n[0.1.7] - 2025-07-09 - keywords report improvements, BNC XML parsing, lightweight ListCorpus format for reference corpora\n\nAdded\n\nadded BNC xml parsing to conc.corpora\nListCorpus format - lightweight format with frequency information for use as a reference corpus\nsupport for ListCorpus added to keywords, raising errors in non-supported report classes\ndocumented ListCorpus in recipes\nadded document frequency reporting to ngram_frequencies\nadded optional standardisation of apostrophes when building corpora\nadded support to exclude/include negative keywords in keywords report\n\n\n\nChanged\n\nchanged Corpus.build to _build, added flexibility to expected/required files, and improved docs\nstop words via Conc.core are now sorted\nrequiring lxml dependency for BNC XML parsing\nkeywords report defaults to trying to resolve apostrophe character differences in word tokens\nchanged default keywords order to log likelihood\n\n\n\nFixed\n\nensure page size 0 (all data) works in keywords method\nfix issue with 0 frequencies not using observed frequency correction in keywords report\n\n\n\n\n[0.1.6] - 2025-07-02 - concordance filtering based on context words, improved display of texts\n\nAdded\n\nadded concordancing functionality to allow specifying a context word within a specific span\nnew parameters to control reflow and wrapping of text via Text class (or Corpus.text)\n\n\n\nChanged\n\ntweaks to documentation (recipes) for new functionality in 0.1.5 and 0.1.6\nusing ignore punctuation True as default for concordance, so concordance sort defaults to word token sorting\n\n\n\n\n[0.1.5] - 2025-06-30 - improved concordance sorting, new features to support release of ConText\n\nAdded\n\nCorpus.report - allows accessing Corpus summary as a Result object\nadded Plot class\nadded functionality for auto-calculating ngram_length based on input\nhighlighting in text output\nadded flag to allow/prevent multiple formats for sample corpora sources\nadded MIDDLE ngram_token_position\nsupport for ignoring punctuation in concordance sorts\nsupport for scaling of concordance plot\ndoc_position_to_corpus_position in Text to allow translating positions\n\n\n\nChanged\n\nexpanded punctuation list to unicode tokens to allow better punctuation token detection\nimprove formatting of concordances and other tables\nimproved plot rendering to allow multiple concordance plots per page\nrevised sample corpora processes to ensure archives compressed, metadata used\nprevented possible many iterations when scanning for sort tokens\npassing doc/offset in svg element attributes (support for ConText clickable plots)\n\n\n\nFixed\n\nfix to prevent negative start indexes and associated weirdness\n\n\n\n\n[0.1.4] - 2025-06-18 - documentation and test improvements, Result/Text class updates\n\nAdded\n\nadded tutorials, recipes, install page and other documentation\nnox-based pre-release test script for Pythons 3.10+\nrenderpng.py in scripts to render screenshots for docs\nadded Result.to_html method to support Text revisions and more flexible output options\n\n\n\nChanged\n\nrestructured and updated documentation\nminor code changes related to documentation clarity (e.g. formatting of corpus_path values from build)\nimproved Text class HTML output, new functionality in Text to support this\nsimplified Result class internal structure for more flexible output\n\n\n\n\n[0.1.3] - 2025-06-13 - concordance plot improvements\n\nChanged\n\nconcordance plot rewritten in pure HTML/JS/CSS without Plotly/AnyWidget for better performance and portability across Jupyter environments\n\n\n\nRemoved\n\nAnyWidget dependency (due to rewrite of concordance plot)\n\n\n\n\n[0.1.2] - 2025-06-12 - CI build improvements and new corpora module\n\nAdded\n\nexposed concordance plot via Conc interface class\nadded conc.corpora module\nadded console script conc_build_sample_corpora to be used in CI\nadded anywidget dependency for concordance plot\n\n\n\nChanged\n\nmoved build_test_corpora from conc.core to conc.corpora as build_sample_corpora\nraise deprecation error for build_test_corpora\nmove function from conc.core related to corpus source downloads to conc.corpora\ndeprecated calls in conc.core log warnings and run new functions in conc.corpora\nchanged CI workflow to use conc_build_sample_corpora\n\n\n\nFixed\n\nresolve error related to column not found when changing concordance context lengths\n\n\n\n\n[0.1.1] - 2025-06-10 - Dependency fix\n\nFixed\n\nmove memory_profiler from dev to main dependencies\n\n\n\n\n[0.1.0] - 2025-06-09 - Initial release\nInitial release of Conc",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#restrictexclude-tokens-improvements-for-keyness-bug-fix",
    "href": "development/releases.html#restrictexclude-tokens-improvements-for-keyness-bug-fix",
    "title": "Releases",
    "section": "[0.1.13] - 2025-08-12 - restrict/exclude tokens improvements for keyness, bug fix",
    "text": "[0.1.13] - 2025-08-12 - restrict/exclude tokens improvements for keyness, bug fix\n\nChanged\n\nreplacing call to deprecated function in frequency tests\nremoving double filtering of restrict/excluded tokens keyness\nadded tests for restrict tokens in keyness\nadd note on restrict_tokens interaction with other arguments\npinning polars to 1.31.0 based on is_in bug in 1.32.0\nensuring force rebuild of sample corpora forces redownload of source data\n\n\n\nFixed\n\ncorrecting attempt to collect already collected lazyframe in keyness",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#documentation-updates-bug-fix",
    "href": "development/releases.html#documentation-updates-bug-fix",
    "title": "Releases",
    "section": "[0.1.12] - 2025-07-24 - documentation updates, bug fix",
    "text": "[0.1.12] - 2025-07-24 - documentation updates, bug fix\n\nAdded\n\nexplanation of keyness and collocate statistics implemented in Conc\n\n\n\nChanged\n\nupdated optional dependencies install instructions\ninstructions for installing on older machines now has pinned Polars version to 1.30.0\n\n\n\nFixed\n\nfix ngrams not returning correct ngram range when token position RIGHT and ngram_length &gt; 1 (includes new tests)",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#zenodo-integrated-citation-information-added",
    "href": "development/releases.html#zenodo-integrated-citation-information-added",
    "title": "Releases",
    "section": "[0.1.11] - 2025-07-23 - Zenodo integrated, citation information added",
    "text": "[0.1.11] - 2025-07-23 - Zenodo integrated, citation information added\n\nAdded\n\nadded badge for Zenodo DOI\nadded release badge\nadded citation file\n\n\n\nChanged\n\nupdated citation information to README files generated by Conc on corpus build",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#documentation-improvements-initiating-zenodo-integration",
    "href": "development/releases.html#documentation-improvements-initiating-zenodo-integration",
    "title": "Releases",
    "section": "[0.1.10] - 2025-07-23 - documentation improvements, initiating Zenodo integration",
    "text": "[0.1.10] - 2025-07-23 - documentation improvements, initiating Zenodo integration\nThis version is the first released via Github releases. Previous releases were via Pypi only. Github releases are tracked automatically by Zenodo.\n\nChanged\n\nimprovements to API documentation with headings and direction to use Conc class as main interface to reports",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#compatibility-with-setup-on-google-colab",
    "href": "development/releases.html#compatibility-with-setup-on-google-colab",
    "title": "Releases",
    "section": "[0.1.9] - 2025-07-15 - compatibility with setup on Google Colab",
    "text": "[0.1.9] - 2025-07-15 - compatibility with setup on Google Colab\n\nChanged\n\nrequiring numpy&gt;=2.0.0 and polars&gt;=1.30.0, thanks @karinstahel for the feedback about Colab compatibility",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#bug-fix-so-list-corpora-ready-for-use-after-build",
    "href": "development/releases.html#bug-fix-so-list-corpora-ready-for-use-after-build",
    "title": "Releases",
    "section": "[0.1.8] - 2025-07-10 - bug fix so list corpora ready for use after build",
    "text": "[0.1.8] - 2025-07-10 - bug fix so list corpora ready for use after build\n\nFixed\n\ninit the corpus after build so vocab is ready for use",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#keywords-report-improvements-bnc-xml-parsing-lightweight-listcorpus-format-for-reference-corpora",
    "href": "development/releases.html#keywords-report-improvements-bnc-xml-parsing-lightweight-listcorpus-format-for-reference-corpora",
    "title": "Releases",
    "section": "[0.1.7] - 2025-07-09 - keywords report improvements, BNC XML parsing, lightweight ListCorpus format for reference corpora",
    "text": "[0.1.7] - 2025-07-09 - keywords report improvements, BNC XML parsing, lightweight ListCorpus format for reference corpora\n\nAdded\n\nadded BNC xml parsing to conc.corpora\nListCorpus format - lightweight format with frequency information for use as a reference corpus\nsupport for ListCorpus added to keywords, raising errors in non-supported report classes\ndocumented ListCorpus in recipes\nadded document frequency reporting to ngram_frequencies\nadded optional standardisation of apostrophes when building corpora\nadded support to exclude/include negative keywords in keywords report\n\n\n\nChanged\n\nchanged Corpus.build to _build, added flexibility to expected/required files, and improved docs\nstop words via Conc.core are now sorted\nrequiring lxml dependency for BNC XML parsing\nkeywords report defaults to trying to resolve apostrophe character differences in word tokens\nchanged default keywords order to log likelihood\n\n\n\nFixed\n\nensure page size 0 (all data) works in keywords method\nfix issue with 0 frequencies not using observed frequency correction in keywords report",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#concordance-filtering-based-on-context-words-improved-display-of-texts",
    "href": "development/releases.html#concordance-filtering-based-on-context-words-improved-display-of-texts",
    "title": "Releases",
    "section": "[0.1.6] - 2025-07-02 - concordance filtering based on context words, improved display of texts",
    "text": "[0.1.6] - 2025-07-02 - concordance filtering based on context words, improved display of texts\n\nAdded\n\nadded concordancing functionality to allow specifying a context word within a specific span\nnew parameters to control reflow and wrapping of text via Text class (or Corpus.text)\n\n\n\nChanged\n\ntweaks to documentation (recipes) for new functionality in 0.1.5 and 0.1.6\nusing ignore punctuation True as default for concordance, so concordance sort defaults to word token sorting",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#improved-concordance-sorting-new-features-to-support-release-of-context",
    "href": "development/releases.html#improved-concordance-sorting-new-features-to-support-release-of-context",
    "title": "Releases",
    "section": "[0.1.5] - 2025-06-30 - improved concordance sorting, new features to support release of ConText",
    "text": "[0.1.5] - 2025-06-30 - improved concordance sorting, new features to support release of ConText\n\nAdded\n\nCorpus.report - allows accessing Corpus summary as a Result object\nadded Plot class\nadded functionality for auto-calculating ngram_length based on input\nhighlighting in text output\nadded flag to allow/prevent multiple formats for sample corpora sources\nadded MIDDLE ngram_token_position\nsupport for ignoring punctuation in concordance sorts\nsupport for scaling of concordance plot\ndoc_position_to_corpus_position in Text to allow translating positions\n\n\n\nChanged\n\nexpanded punctuation list to unicode tokens to allow better punctuation token detection\nimprove formatting of concordances and other tables\nimproved plot rendering to allow multiple concordance plots per page\nrevised sample corpora processes to ensure archives compressed, metadata used\nprevented possible many iterations when scanning for sort tokens\npassing doc/offset in svg element attributes (support for ConText clickable plots)\n\n\n\nFixed\n\nfix to prevent negative start indexes and associated weirdness",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#documentation-and-test-improvements-resulttext-class-updates",
    "href": "development/releases.html#documentation-and-test-improvements-resulttext-class-updates",
    "title": "Releases",
    "section": "[0.1.4] - 2025-06-18 - documentation and test improvements, Result/Text class updates",
    "text": "[0.1.4] - 2025-06-18 - documentation and test improvements, Result/Text class updates\n\nAdded\n\nadded tutorials, recipes, install page and other documentation\nnox-based pre-release test script for Pythons 3.10+\nrenderpng.py in scripts to render screenshots for docs\nadded Result.to_html method to support Text revisions and more flexible output options\n\n\n\nChanged\n\nrestructured and updated documentation\nminor code changes related to documentation clarity (e.g. formatting of corpus_path values from build)\nimproved Text class HTML output, new functionality in Text to support this\nsimplified Result class internal structure for more flexible output",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#concordance-plot-improvements",
    "href": "development/releases.html#concordance-plot-improvements",
    "title": "Releases",
    "section": "[0.1.3] - 2025-06-13 - concordance plot improvements",
    "text": "[0.1.3] - 2025-06-13 - concordance plot improvements\n\nChanged\n\nconcordance plot rewritten in pure HTML/JS/CSS without Plotly/AnyWidget for better performance and portability across Jupyter environments\n\n\n\nRemoved\n\nAnyWidget dependency (due to rewrite of concordance plot)",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#ci-build-improvements-and-new-corpora-module",
    "href": "development/releases.html#ci-build-improvements-and-new-corpora-module",
    "title": "Releases",
    "section": "[0.1.2] - 2025-06-12 - CI build improvements and new corpora module",
    "text": "[0.1.2] - 2025-06-12 - CI build improvements and new corpora module\n\nAdded\n\nexposed concordance plot via Conc interface class\nadded conc.corpora module\nadded console script conc_build_sample_corpora to be used in CI\nadded anywidget dependency for concordance plot\n\n\n\nChanged\n\nmoved build_test_corpora from conc.core to conc.corpora as build_sample_corpora\nraise deprecation error for build_test_corpora\nmove function from conc.core related to corpus source downloads to conc.corpora\ndeprecated calls in conc.core log warnings and run new functions in conc.corpora\nchanged CI workflow to use conc_build_sample_corpora\n\n\n\nFixed\n\nresolve error related to column not found when changing concordance context lengths",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#dependency-fix",
    "href": "development/releases.html#dependency-fix",
    "title": "Releases",
    "section": "[0.1.1] - 2025-06-10 - Dependency fix",
    "text": "[0.1.1] - 2025-06-10 - Dependency fix\n\nFixed\n\nmove memory_profiler from dev to main dependencies",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/releases.html#initial-release",
    "href": "development/releases.html#initial-release",
    "title": "Releases",
    "section": "[0.1.0] - 2025-06-09 - Initial release",
    "text": "[0.1.0] - 2025-06-09 - Initial release\nInitial release of Conc",
    "crumbs": [
      "Development",
      "Releases"
    ]
  },
  {
    "objectID": "development/index.html",
    "href": "development/index.html",
    "title": "Development",
    "section": "",
    "text": "This section provides information on Conc development.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\nReleases\n\n\nInformation on Conc releases.\n\n\n\n\n\n\nRoadmap\n\n\nAn incomplete roadmap for Conc.\n\n\n\n\n\n\nDeveloper Guide\n\n\nWant to contribute? Here is how to setup a development environment for Conc.\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Development"
    ]
  },
  {
    "objectID": "development/developer_guide.html",
    "href": "development/developer_guide.html",
    "title": "Developer Guide",
    "section": "",
    "text": "The instructions below are only relevant if you want to contribute to Conc. The nbdev library is being used for development. If you are new to using nbdevc, here are some useful pointers to get you started (or visit the nbdev website).\n\nInstall conc in Development mode\n# make sure conc package is installed in development mode\npip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to conc\nnbdev_prepare",
    "crumbs": [
      "Development",
      "Developer Guide"
    ]
  },
  {
    "objectID": "api/core.html",
    "href": "api/core.html",
    "title": "core",
    "section": "",
    "text": "source\n\n\n\n set_logger_state (state:str)\n\nSet the state of the conc logger to either ‘quiet’ or ‘verbose’\n\n\n\n\nType\nDetails\n\n\n\n\nstate\nstr\n‘quiet’ or ‘verbose’",
    "crumbs": [
      "API",
      "core"
    ]
  },
  {
    "objectID": "api/core.html#logging",
    "href": "api/core.html#logging",
    "title": "core",
    "section": "",
    "text": "source\n\n\n\n set_logger_state (state:str)\n\nSet the state of the conc logger to either ‘quiet’ or ‘verbose’\n\n\n\n\nType\nDetails\n\n\n\n\nstate\nstr\n‘quiet’ or ‘verbose’",
    "crumbs": [
      "API",
      "core"
    ]
  },
  {
    "objectID": "api/core.html#spacy",
    "href": "api/core.html#spacy",
    "title": "core",
    "section": "spaCy",
    "text": "spaCy\n\nsource\n\nspacy_attribute_name\n\n spacy_attribute_name (index)\n\nGet name of index from spacy.",
    "crumbs": [
      "API",
      "core"
    ]
  },
  {
    "objectID": "api/core.html#corpus-metadata-schema",
    "href": "api/core.html#corpus-metadata-schema",
    "title": "core",
    "section": "Corpus metadata schema",
    "text": "Corpus metadata schema\n\nsource\n\nCorpusMetadata\n\n CorpusMetadata (name:str, description:str, slug:str, conc_version:str,\n                 document_count:int, token_count:int,\n                 word_token_count:int, punct_token_count:int,\n                 space_token_count:int, unique_tokens:int,\n                 unique_word_tokens:int, date_created:str, EOF_TOKEN:int,\n                 SPACY_EOF_TOKEN:int, SPACY_MODEL:str,\n                 SPACY_MODEL_VERSION:str, punct_tokens:list[int],\n                 space_tokens:list[int])\n\nJSON validation schema for corpus metadata\n\nproperties = msgspec.json.schema(CorpusMetadata)['$defs']['CorpusMetadata']['properties']\ndisplay(properties)\n\n{'name': {'type': 'string'},\n 'description': {'type': 'string'},\n 'slug': {'type': 'string'},\n 'conc_version': {'type': 'string'},\n 'document_count': {'type': 'integer'},\n 'token_count': {'type': 'integer'},\n 'word_token_count': {'type': 'integer'},\n 'punct_token_count': {'type': 'integer'},\n 'space_token_count': {'type': 'integer'},\n 'unique_tokens': {'type': 'integer'},\n 'unique_word_tokens': {'type': 'integer'},\n 'date_created': {'type': 'string'},\n 'EOF_TOKEN': {'type': 'integer'},\n 'SPACY_EOF_TOKEN': {'type': 'integer'},\n 'SPACY_MODEL': {'type': 'string'},\n 'SPACY_MODEL_VERSION': {'type': 'string'},\n 'punct_tokens': {'type': 'array', 'items': {'type': 'integer'}},\n 'space_tokens': {'type': 'array', 'items': {'type': 'integer'}}}",
    "crumbs": [
      "API",
      "core"
    ]
  },
  {
    "objectID": "api/core.html#get-word-lists",
    "href": "api/core.html#get-word-lists",
    "title": "core",
    "section": "Get word lists",
    "text": "Get word lists\n\nsource\n\nget_stop_words\n\n get_stop_words (save_path:str, spacy_model:str='en_core_web_sm')\n\nGet stop words from spaCy and cache to disk\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsave_path\nstr\n\ndirectory to save stop words to, file name will be created based on spaCy model name\n\n\nspacy_model\nstr\nen_core_web_sm\nmodel to get stop words for\n\n\n\n\nprint(get_stop_words(save_path = save_path, spacy_model='en_core_web_sm'))\n\n[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'bottom', 'but', 'by', 'ca', 'call', 'can', 'cannot', 'could', 'did', 'do', 'does', 'doing', 'done', 'down', 'due', 'during', 'each', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'made', 'make', 'many', 'may', 'me', 'meanwhile', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', \"n't\", 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'n‘t', 'n’t', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'quite', 'rather', 're', 'really', 'regarding', 'same', 'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'under', 'unless', 'until', 'up', 'upon', 'us', 'used', 'using', 'various', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', '‘d', '‘ll', '‘m', '‘re', '‘s', '‘ve', '’d', '’ll', '’m', '’re', '’s', '’ve']",
    "crumbs": [
      "API",
      "core"
    ]
  },
  {
    "objectID": "api/core.html#access-these-functions-from-conc.corpora",
    "href": "api/core.html#access-these-functions-from-conc.corpora",
    "title": "core",
    "section": "Access these functions from conc.corpora",
    "text": "Access these functions from conc.corpora\nUp to version 0.1.1 conc.core included helper functions to list, download and build corpora. These have been moved to the conc.corpora module. Running these functions will trigger a warning with a note about deprecation and the new location of the functions. Access to these functions will only be via conc.corpora by Conc version 1.0.0.\n\nsource\n\nlist_corpora\n\n list_corpora (path:str)\n\n(Deprecated - call via conc.corpora) Scan a directory for available corpora\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\npath to load corpus\n\n\nReturns\nDataFrame\nDataframe with path, corpus, corpus name, document count, token count\n\n\n\n\nsource\n\n\ncreate_toy_corpus_sources\n\n create_toy_corpus_sources (source_path:str)\n\n(Deprecated - call via conc.corpora) Create txt files and csv to test build of toy corpus.\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\npath to location of sources for building corpora\n\n\n\n\nsource\n\n\nshow_toy_corpus\n\n show_toy_corpus (csv_path:str)\n\n(Deprecated - call via conc.corpora) Show toy corpus in a table.\n\n\n\n\nType\nDetails\n\n\n\n\ncsv_path\nstr\npath to location of csv for building corpora\n\n\nReturns\nGT\n\n\n\n\n\nsource\n\n\nget_nltk_corpus_sources\n\n get_nltk_corpus_sources (source_path:str)\n\n(Deprecated - call via conc.corpora) Get NLTK corpora as sources for development or testing Conc functionality.\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\npath to location of sources for building corpora\n\n\n\n\nsource\n\n\nget_garden_party\n\n get_garden_party (source_path:str)\n\n(Deprecated - call via conc.corpora) Get corpus of The Garden Party by Katherine Mansfield for development of Conc and testing Conc functionality.\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\npath to location of sources for building corpora\n\n\n\n\nsource\n\n\nget_large_dataset\n\n get_large_dataset (source_path:str)\n\n(Deprecated - call via conc.corpora) Get 1m rows of https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset for testing.\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\npath to location of sources for building corpora\n\n\n\n\nsource\n\n\ncreate_large_dataset_sizes\n\n create_large_dataset_sizes (source_path:str, sizes:list=[10000, 100000,\n                             200000, 500000])\n\n(Deprecated - call via conc.corpora) Create datasets of different sizes from data source retrieved by get_large_dataset for testing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_path\nstr\n\npath to location of sources for building corpora\n\n\nsizes\nlist\n[10000, 100000, 200000, 500000]\nlist of sizes for test data-sets",
    "crumbs": [
      "API",
      "core"
    ]
  },
  {
    "objectID": "api/text.html",
    "href": "api/text.html",
    "title": "text",
    "section": "",
    "text": "The Text class is not intended to be used directly. Functionality is accessible via the Corpus.text method, which provides the necessary inputs to instantiate the class. There are examples below illustrating how Text objects can be created and used for a Corpus.",
    "crumbs": [
      "API",
      "text"
    ]
  },
  {
    "objectID": "api/text.html#using-the-text-class",
    "href": "api/text.html#using-the-text-class",
    "title": "text",
    "section": "",
    "text": "The Text class is not intended to be used directly. Functionality is accessible via the Corpus.text method, which provides the necessary inputs to instantiate the class. There are examples below illustrating how Text objects can be created and used for a Corpus.",
    "crumbs": [
      "API",
      "text"
    ]
  },
  {
    "objectID": "api/text.html#text-class-api-reference",
    "href": "api/text.html#text-class-api-reference",
    "title": "text",
    "section": "Text class API reference",
    "text": "Text class API reference\n\nsource\n\nText\n\n Text (tokens:numpy.ndarray, has_spaces:numpy.ndarray, metadata:dict={},\n       doc_df:polars.dataframe.frame.DataFrame=None)\n\nClass to represent text documents\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokens\nndarray\n\nlist of token strs\n\n\nhas_spaces\nndarray\n\nwhether token strs followed by space\n\n\nmetadata\ndict\n{}\nmetadata for doc as a dict\n\n\ndoc_df\nDataFrame\nNone\nif provided can be used for enhanced display (e.g. keyword highlighting)\n\n\n\n\nsource\n\n\nText.as_string\n\n Text.as_string (max_tokens:int|None=None,\n                 highlighted_token_range:tuple|None=None)\n\nReturn the text as a string\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmax_tokens\nint | None\nNone\nmaximum length of text to display in tokens, if None, display all\n\n\nhighlighted_token_range\ntuple | None\nNone\nrange of tokens to highlight, note: these token ids are positions within the corpus, not the text itself\n\n\n\n\nsource\n\n\nText.as_tokens\n\n Text.as_tokens ()\n\nReturn the text as a tokens\n\nsource\n\n\nText.__str__\n\n Text.__str__ ()\n\nReturn str(self).\n\nsource\n\n\nText.tokens_count\n\n Text.tokens_count ()\n\n\nsource\n\n\nText.display_metadata\n\n Text.display_metadata ()\n\nOutput the metadata for a text\n\nsource\n\n\nText.get_metadata\n\n Text.get_metadata ()\n\nOutput the metadata for a text\n\nsource\n\n\nText.display\n\n Text.display (show_metadata:bool=True, max_tokens:int|None=None,\n               output_html:bool=True, textwrap_width:int|None=None,\n               textwrap_args:dict|None=None, reflow_paragraphs:bool=False,\n               paragraph_delimiter_regex:str='(\\\\s*\\\\n\\\\s*){1,}\\\\n')\n\nOutput a text\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nshow_metadata\nbool\nTrue\nwhether to display Metadata for the text\n\n\nmax_tokens\nint | None\nNone\nmaximum length of text to display in tokens, if None, display all\n\n\noutput_html\nbool\nTrue\nwhether to display text with HTML formatting\n\n\ntextwrap_width\nint | None\nNone\nmaximum length of text to display in characters, if None, no wrapping\n\n\ntextwrap_args\ndict | None\nNone\nadditional args to pass to textwrap.fill\n\n\nreflow_paragraphs\nbool\nFalse\nwhether to reflow paragraphs individually before text wrapping is applied\n\n\nparagraph_delimiter_regex\nstr\n(){1,}\nregex to split paragraphs for reflow_paragraphs (default looks for whitespace ending with a newline that contains at least one other newline)\n\n\n\n\nExamples\nSee the note above about accessing this functionality through the Corpus class.\n\ngardenparty.text(12).display(max_tokens = 200)\n\n\n    \n\n\n\n\n\nMetadata\n\n\n\n\n\nAttribute\nValue\n\n\n\n\ndocument_id\n12\n\n\nfile\nthe-singing-lesson.txt\n\n\n\n\n\n\n        With despair—cold, sharp despair—buried deep in her heart like a wicked\nknife, Miss Meadows, in cap and gown and carrying a little baton, trod\nthe cold corridors that led to the music hall. Girls of all ages, rosy\nfrom the air, and bubbling over with that gleeful excitement that comes\nfrom running to school on a fine autumn morning, hurried, skipped,\nfluttered by; from the hollow class-rooms came a quick drumming of\nvoices; a bell rang; a voice like a bird cried, “Muriel.” And then\nthere came from the staircase a tremendous knock-knock-knocking. Some\none had dropped her dumbbells.\n\nThe Science Mistress stopped Miss Meadows.\n\n“Good mor-ning,” she cried, in her sweet, affected drawl. “Isn’t it\ncold? It might be win-ter.”\n\nMiss Meadows, hugging the knife, stared in hatred at the Science\nMistress. Everything about her was sweet,…\n[200 of 2985 tokens]\n\n\n\ngardenparty.text(12).display(show_metadata = False, max_tokens = 200, textwrap_width = 100, reflow_paragraphs = True)\n\n\n    \n    With despair—cold, sharp despair—buried deep in her heart like a wicked  knife, Miss Meadows, in cap\nand gown and carrying a little baton, trod  the cold corridors that led to the music hall. Girls of\nall ages, rosy  from the air, and bubbling over with that gleeful excitement that comes  from\nrunning to school on a fine autumn morning, hurried, skipped,  fluttered by; from the hollow class-\nrooms came a quick drumming of  voices; a bell rang; a voice like a bird cried, “Muriel.” And then\nthere came from the staircase a tremendous knock-knock-knocking. Some  one had dropped her\ndumbbells.\n\nThe Science Mistress stopped Miss Meadows.\n\n“Good mor-ning,” she cried, in her sweet, affected drawl. “Isn’t it  cold? It might be win-ter.”\n\nMiss Meadows, hugging the knife, stared in hatred at the Science  Mistress. Everything about her was\nsweet,… [200 of 2985 tokens]\n\n\n\ngardenparty.text(12).as_string(max_tokens = 50)\n\n'With despair—cold, sharp despair—buried deep in her heart like a wicked\\r\\nknife, Miss Meadows, in cap and gown and carrying a little baton, trod\\r\\nthe cold corridors that led to the music hall. Girls of all ages, rosy'",
    "crumbs": [
      "API",
      "text"
    ]
  },
  {
    "objectID": "api/corpora.html",
    "href": "api/corpora.html",
    "title": "corpora",
    "section": "",
    "text": "source\n\n\n\n list_corpora (path:str)\n\nScan a directory for available corpora\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\npath to load corpus\n\n\nReturns\nDataFrame\nDataframe with path, corpus, format (Corpus or List Corpus), corpus name, document count, token count\n\n\n\n\nprint(list_corpora(save_path))\n\n┌──────────────────────────────────┬───────────────────────────┬─────────────┬─────────────────────┬────────────────┬─────────────┐\n│ corpus                           ┆ name                      ┆ format      ┆ date_created        ┆ document_count ┆ token_count │\n╞══════════════════════════════════╪═══════════════════════════╪═════════════╪═════════════════════╪════════════════╪═════════════╡\n│ baby-bnc.listcorpus              ┆ Baby BNC                  ┆ List Corpus ┆ 2025-07-09 08:44:05 ┆ 182            ┆ 4,674,632   │\n│ bnc.listcorpus                   ┆ BNC                       ┆ List Corpus ┆ 2025-07-09 08:54:00 ┆ 4,049          ┆ 113,536,056 │\n│ brown.listcorpus                 ┆ Brown Corpus              ┆ List Corpus ┆ 2025-07-10 13:30:09 ┆ 500            ┆ 1,138,566   │\n│ clinton-subcorpus.corpus         ┆ Clinton Subcorpus         ┆ Corpus      ┆ 2025-07-20 22:04:57 ┆ 36             ┆ 132,573     │\n│ garden-party.corpus              ┆ Garden Party Corpus       ┆ Corpus      ┆ 2025-08-12 09:17:34 ┆ 15             ┆ 74,664      │\n│ garden-party.listcorpus          ┆ Garden Party Corpus       ┆ List Corpus ┆ 2025-08-12 09:16:35 ┆ 15             ┆ 74,664      │\n│ gutenberg.corpus                 ┆ Gutenberg Corpus          ┆ Corpus      ┆ 2025-07-10 13:30:42 ┆ 18             ┆ 2,546,286   │\n│ introduce-yourself.corpus        ┆ Introduce Yourself        ┆ Corpus      ┆ 2025-07-11 17:00:14 ┆ 96             ┆ 35,229      │\n│ labour-nz-first-coalition.corpus ┆ Labour-NZ First Coalition ┆ Corpus      ┆ 2025-07-09 23:20:16 ┆ 325            ┆ 655,092     │\n│ national-led.corpus              ┆ National-led              ┆ Corpus      ┆ 2025-07-09 23:20:23 ┆ 391            ┆ 787,785     │\n│ quake-stories-v2.corpus          ┆ Quake Stories v2          ┆ Corpus      ┆ 2025-07-02 09:42:43 ┆ 487            ┆ 472,876     │\n│ reuters.corpus                   ┆ Reuters Corpus            ┆ Corpus      ┆ 2025-07-10 13:30:18 ┆ 10,788         ┆ 1,552,919   │\n│ rnz-climate-international.corpus ┆ RNZ Climate International ┆ Corpus      ┆ 2025-07-17 23:24:39 ┆ 4,948          ┆ 2,414,005   │\n│ rnz-climate-national.corpus      ┆ RNZ Climate National      ┆ Corpus      ┆ 2025-07-17 23:24:48 ┆ 6,104          ┆ 3,962,664   │\n│ rnz-climate.corpus               ┆ RNZ Climate Corpus        ┆ Corpus      ┆ 2025-07-17 23:22:57 ┆ 11,052         ┆ 6,376,669   │\n│ rnz.listcorpus                   ┆ RNZ Corpus                ┆ List Corpus ┆ 2025-07-10 19:22:40 ┆ 458,113        ┆ 143,331,518 │\n│ toy.corpus                       ┆ Toy Corpus                ┆ Corpus      ┆ 2025-08-12 09:25:29 ┆ 6              ┆ 38          │\n│ toy.listcorpus                   ┆ Toy Corpus                ┆ List Corpus ┆ 2025-07-10 13:30:02 ┆ 6              ┆ 38          │\n│ trump-subcorpus.corpus           ┆ Trump Subcorpus           ┆ Corpus      ┆ 2025-07-20 22:04:54 ┆ 82             ┆ 521,989     │\n└──────────────────────────────────┴───────────────────────────┴─────────────┴─────────────────────┴────────────────┴─────────────┘",
    "crumbs": [
      "API",
      "corpora"
    ]
  },
  {
    "objectID": "api/corpora.html#list-available-corpora",
    "href": "api/corpora.html#list-available-corpora",
    "title": "corpora",
    "section": "",
    "text": "source\n\n\n\n list_corpora (path:str)\n\nScan a directory for available corpora\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\npath to load corpus\n\n\nReturns\nDataFrame\nDataframe with path, corpus, format (Corpus or List Corpus), corpus name, document count, token count\n\n\n\n\nprint(list_corpora(save_path))\n\n┌──────────────────────────────────┬───────────────────────────┬─────────────┬─────────────────────┬────────────────┬─────────────┐\n│ corpus                           ┆ name                      ┆ format      ┆ date_created        ┆ document_count ┆ token_count │\n╞══════════════════════════════════╪═══════════════════════════╪═════════════╪═════════════════════╪════════════════╪═════════════╡\n│ baby-bnc.listcorpus              ┆ Baby BNC                  ┆ List Corpus ┆ 2025-07-09 08:44:05 ┆ 182            ┆ 4,674,632   │\n│ bnc.listcorpus                   ┆ BNC                       ┆ List Corpus ┆ 2025-07-09 08:54:00 ┆ 4,049          ┆ 113,536,056 │\n│ brown.listcorpus                 ┆ Brown Corpus              ┆ List Corpus ┆ 2025-07-10 13:30:09 ┆ 500            ┆ 1,138,566   │\n│ clinton-subcorpus.corpus         ┆ Clinton Subcorpus         ┆ Corpus      ┆ 2025-07-20 22:04:57 ┆ 36             ┆ 132,573     │\n│ garden-party.corpus              ┆ Garden Party Corpus       ┆ Corpus      ┆ 2025-08-12 09:17:34 ┆ 15             ┆ 74,664      │\n│ garden-party.listcorpus          ┆ Garden Party Corpus       ┆ List Corpus ┆ 2025-08-12 09:16:35 ┆ 15             ┆ 74,664      │\n│ gutenberg.corpus                 ┆ Gutenberg Corpus          ┆ Corpus      ┆ 2025-07-10 13:30:42 ┆ 18             ┆ 2,546,286   │\n│ introduce-yourself.corpus        ┆ Introduce Yourself        ┆ Corpus      ┆ 2025-07-11 17:00:14 ┆ 96             ┆ 35,229      │\n│ labour-nz-first-coalition.corpus ┆ Labour-NZ First Coalition ┆ Corpus      ┆ 2025-07-09 23:20:16 ┆ 325            ┆ 655,092     │\n│ national-led.corpus              ┆ National-led              ┆ Corpus      ┆ 2025-07-09 23:20:23 ┆ 391            ┆ 787,785     │\n│ quake-stories-v2.corpus          ┆ Quake Stories v2          ┆ Corpus      ┆ 2025-07-02 09:42:43 ┆ 487            ┆ 472,876     │\n│ reuters.corpus                   ┆ Reuters Corpus            ┆ Corpus      ┆ 2025-07-10 13:30:18 ┆ 10,788         ┆ 1,552,919   │\n│ rnz-climate-international.corpus ┆ RNZ Climate International ┆ Corpus      ┆ 2025-07-17 23:24:39 ┆ 4,948          ┆ 2,414,005   │\n│ rnz-climate-national.corpus      ┆ RNZ Climate National      ┆ Corpus      ┆ 2025-07-17 23:24:48 ┆ 6,104          ┆ 3,962,664   │\n│ rnz-climate.corpus               ┆ RNZ Climate Corpus        ┆ Corpus      ┆ 2025-07-17 23:22:57 ┆ 11,052         ┆ 6,376,669   │\n│ rnz.listcorpus                   ┆ RNZ Corpus                ┆ List Corpus ┆ 2025-07-10 19:22:40 ┆ 458,113        ┆ 143,331,518 │\n│ toy.corpus                       ┆ Toy Corpus                ┆ Corpus      ┆ 2025-08-12 09:25:29 ┆ 6              ┆ 38          │\n│ toy.listcorpus                   ┆ Toy Corpus                ┆ List Corpus ┆ 2025-07-10 13:30:02 ┆ 6              ┆ 38          │\n│ trump-subcorpus.corpus           ┆ Trump Subcorpus           ┆ Corpus      ┆ 2025-07-20 22:04:54 ┆ 82             ┆ 521,989     │\n└──────────────────────────────────┴───────────────────────────┴─────────────┴─────────────────────┴────────────────┴─────────────┘",
    "crumbs": [
      "API",
      "corpora"
    ]
  },
  {
    "objectID": "api/corpora.html#get-data-sources",
    "href": "api/corpora.html#get-data-sources",
    "title": "corpora",
    "section": "Get data sources",
    "text": "Get data sources\n\nsource\n\ncreate_toy_corpus_sources\n\n create_toy_corpus_sources (source_path:str)\n\nCreate txt files and csv to test build of toy corpus.\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\npath to location of sources for building corpora\n\n\n\n\nsource\n\n\nshow_toy_corpus\n\n show_toy_corpus (csv_path:str)\n\nShow toy corpus in a table.\n\n\n\n\nType\nDetails\n\n\n\n\ncsv_path\nstr\npath to location of csv for building corpora\n\n\nReturns\nGT\n\n\n\n\n\nshow_toy_corpus(os.path.join(source_path, 'toy.csv'))\n\n\n\n\n\n\n\nsource\ntext\ncategory\nspecies\n\n\n\n\n1.txt\nThe cat sat on the mat.\nfeline\ncat\n\n\n2.txt\nThe dog sat on the mat.\ncanine\ndog\n\n\n3.txt\nThe cat is meowing.\nfeline\ncat\n\n\n4.txt\nThe dog is barking.\ncanine\ndog\n\n\n5.txt\nThe cat is climbing a tree.\nfeline\ncat\n\n\n6.txt\nThe dog is digging a hole.\ncanine\ndog\n\n\n\n\n\n\n\n\n\nsource\n\n\nget_nltk_corpus_sources\n\n get_nltk_corpus_sources (source_path:str)\n\nGet NLTK corpora as sources for development or testing Conc functionality.\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\npath to location of sources for building corpora\n\n\n\nThe texts for the Brown corpus from nltk can be used to test Conc functionality. The Reuters and Gutenberg corpora are also prepared by get_nltk_corpus_sources. Running the function will download the texts and save the texts as a .csv.gz files with columns: source and text. The Brown Corpus is also saved as .txt files to test the Corpus.build_from_texts method.\n\nsource\n\n\nparse_bnc_to_csv\n\n parse_bnc_to_csv (source_path:str, save_path:str,\n                   output_filename:str='bnc.csv.gz')\n\nConverts BNC XML files, available via the British National Corpus, XML edition and the British National Corpus, Baby edition to a compressed CSV with title information retained.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_path\nstr\n\npath to location of sources for building corpora, this is like to be a path ending with ‘Texts’\n\n\nsave_path\nstr\n\npath to save the csv\n\n\noutput_filename\nstr\nbnc.csv.gz\nname of the output file e.g. bnc.csv.gz or bnc-baby.csv.gz\n\n\n\nThe 1994 version of the British National Corpus (BNC) is available from the Oxford Text Archive. The parse_bnc_to_csv function assumes you have downloaded and unziped the files. You need to specify the directory containing the XML files for the texts. This will be a path ending with Texts. Conc does not provide a way to download the BNC zip files directly. You need to go to the Oxford Text Archive and read the notes about restricted use. Note: this function was previously using NLTK’s BNC parser, but this has been rewritten for lxml, which is much (2x) faster.\n\nsource\n\n\nget_garden_party\n\n get_garden_party (source_path:str, create_archive_variations:bool=False)\n\nGet corpus of The Garden Party by Katherine Mansfield for development of Conc and testing Conc functionality.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_path\nstr\n\npath to location of sources for building corpora\n\n\ncreate_archive_variations\nbool\nFalse\ncreate .tar and .tar.gz files for dev/testing (leave False if you just want the zip)\n\n\n\nThe get_garden_party function downloads a zip file of an example corpus based on Katherine Mansfield short stories. This function creates a .tar and a .tar.gz version of the texts for testing Corpus build methods.\n\nget_garden_party(source_path, create_archive_variations = True)",
    "crumbs": [
      "API",
      "corpora"
    ]
  },
  {
    "objectID": "api/corpora.html#create-large-corpora-for-development-and-testing",
    "href": "api/corpora.html#create-large-corpora-for-development-and-testing",
    "title": "corpora",
    "section": "Create large corpora for development and testing",
    "text": "Create large corpora for development and testing\n\nsource\n\nget_large_dataset\n\n get_large_dataset (source_path:str)\n\nGet 1m rows of https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset for testing.\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\npath to location of sources for building corpora\n\n\n\n\nsource\n\n\ncreate_large_dataset_sizes\n\n create_large_dataset_sizes (source_path:str, sizes:list=[10000, 100000,\n                             200000, 500000])\n\nCreate datasets of different sizes from data source retrieved by get_large_dataset for testing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_path\nstr\n\npath to location of sources for building corpora\n\n\nsizes\nlist\n[10000, 100000, 200000, 500000]\nlist of sizes for test data-sets",
    "crumbs": [
      "API",
      "corpora"
    ]
  },
  {
    "objectID": "api/corpora.html#build-sample-corpora",
    "href": "api/corpora.html#build-sample-corpora",
    "title": "corpora",
    "section": "Build sample corpora",
    "text": "Build sample corpora\n\nsource\n\nbuild_sample_corpora\n\n build_sample_corpora (source_path:str, save_path:str,\n                       force_rebuild:bool=False)\n\nBuild all test corpora from source files.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsource_path\nstr\n\npath to folder with corpora\n\n\nsave_path\nstr\n\npath to save corpora\n\n\nforce_rebuild\nbool\nFalse\nforce rebuild of corpora, useful for development and testing\n\n\n\nThe build_sample_corpora function downloads sources and creates sample corpora for development and testing for releases. These datasets are a good way to get started working with Conc. Sample corpora available are:\n\nBrown Corpus (via NLTK)\n\nGutenberg Corpus (via NLTK)\n\nReuters Corpus (via NLTK)\n\nGarden Party Corpus (Katherine Mansfield short stories)\n\nToy Corpus (6 very short texts for testing only)\n\nBrown List Corpus (a lightweight ListCorpus version of the Brown Corpus for use with keywords functionality)\n\nAfter installing Conc you can invoke this function from the command line to download and build the sample corpora:\nconc_build_sample_corpora path/to/save/sources path/to/save/corpora\nAdd --force-rebuild when calling conc_build_sample_corpora to re-download the source data and recreate the corpora.\nNote: build_sample_corpora was accessible via conc.corpus as build_test_corpora up to version 0.1.1. This functionality is only accessible from conc.corpora now.\n\nbuild_sample_corpora(source_path, save_path, force_rebuild=False) # must be left as False after dev, otherwise could destroy corpora mid test in CI",
    "crumbs": [
      "API",
      "corpora"
    ]
  },
  {
    "objectID": "api/collocates.html",
    "href": "api/collocates.html",
    "title": "collocates",
    "section": "",
    "text": "Conc implements logDice as introduced in Rychlý’s (2008) paper “A Lexicographer-Friendly Association Score”. Conc’s implementation of Mutual Information is based on the formula in Rychlý’s paper.",
    "crumbs": [
      "API",
      "collocates"
    ]
  },
  {
    "objectID": "api/collocates.html#about-concs-collocates-functionality",
    "href": "api/collocates.html#about-concs-collocates-functionality",
    "title": "collocates",
    "section": "",
    "text": "Conc implements logDice as introduced in Rychlý’s (2008) paper “A Lexicographer-Friendly Association Score”. Conc’s implementation of Mutual Information is based on the formula in Rychlý’s paper.",
    "crumbs": [
      "API",
      "collocates"
    ]
  },
  {
    "objectID": "api/collocates.html#using-the-collocates-class",
    "href": "api/collocates.html#using-the-collocates-class",
    "title": "collocates",
    "section": "Using the Collocates class",
    "text": "Using the Collocates class\nThere are examples below showing how to use the Collocates class directly to output collocation tables. The recommended way to use this functionality is through the Conc class. This provides an interface to create frequency lists, concordances, collocation tables, keyword tables and more.",
    "crumbs": [
      "API",
      "collocates"
    ]
  },
  {
    "objectID": "api/collocates.html#collocates-class-api-reference",
    "href": "api/collocates.html#collocates-class-api-reference",
    "title": "collocates",
    "section": "Collocates class API reference",
    "text": "Collocates class API reference\n\nsource\n\nCollocates\n\n Collocates (corpus:conc.corpus.Corpus)\n\nClass for collocation analysis reporting.\n\n\n\n\nType\nDetails\n\n\n\n\ncorpus\nCorpus\nCorpus instance\n\n\n\n\nsource\n\n\nCollocates.collocates\n\n Collocates.collocates (token_str:str, effect_size_measure:str='logdice',\n                        statistical_significance_measure:str='log_likeliho\n                        od', order:str|None=None,\n                        order_descending:bool=True,\n                        statistical_significance_cut:float|None=None,\n                        apply_bonferroni:bool=False,\n                        context_length:int|tuple[int,int]=5,\n                        min_collocate_frequency:int=5, page_size:int=20,\n                        page_current:int=1, exclude_punctuation:bool=True)\n\nReport collocates for a given token string.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntoken_str\nstr\n\nToken to search for\n\n\neffect_size_measure\nstr\nlogdice\nstatistical measure to use for collocation calculation: logdice, mutual_information\n\n\nstatistical_significance_measure\nstr\nlog_likelihood\nstatistical significance measure to use, currently only ‘log_likelihood’ is supported\n\n\norder\nstr | None\nNone\ndefault of None orders by collocation measure, results can also be ordered by: collocate_frequency, frequency, log_likelihood\n\n\norder_descending\nbool\nTrue\norder is descending or ascending\n\n\nstatistical_significance_cut\nfloat | None\nNone\nstatistical significance p-value to filter results, e.g. 0.05 or 0.01 or 0.001 - ignored if None or 0\n\n\napply_bonferroni\nbool\nFalse\napply Bonferroni correction to the statistical significance cut-off\n\n\ncontext_length\nint | tuple[int, int]\n5\nWindow size per side in tokens - if an int (e.g. 5) context lengths on left and right will be the same, for independent control of left and right context length pass a tuple (context_length_left, context_left_right) (e.g. (0, 5))\n\n\nmin_collocate_frequency\nint\n5\nMinimum count of collocates\n\n\npage_size\nint\n20\nnumber of rows to return, if 0 returns all\n\n\npage_current\nint\n1\ncurrent page, ignored if page_size is 0\n\n\nexclude_punctuation\nbool\nTrue\nexclude punctuation tokens\n\n\nReturns\nResult\n\n\n\n\n\n\nExamples\nSee the note above about accessing this functionality through the Conc class.\n\ncollocates = Collocates(reuters)\n\n\nquery = 'economy'\ncollocates.collocates(query, order = None, order_descending = True, statistical_significance_cut = 0.0001, apply_bonferroni=True, effect_size_measure='logdice', context_length = 5, min_collocate_frequency = 5, page_current = 1).display()\n\n\n\n\n\n\n\nCollocates of \"economy\"\n\n\nReuters Corpus\n\n\nRank\nToken\nCollocate Frequency\nFrequency\nLogdice\nLog Likelihood\n\n\n\n\n1\nstimulate\n29\n85\n10.39\n206.37\n\n\n2\nboost\n20\n222\n9.60\n84.59\n\n\n3\njapanese\n35\n944\n9.52\n88.82\n\n\n4\ndomestic\n27\n700\n9.39\n70.45\n\n\n5\ngerman\n23\n537\n9.35\n64.41\n\n\n6\nworld\n35\n1,173\n9.32\n75.37\n\n\n7\ngrew\n12\n103\n9.09\n57.00\n\n\n8\nsluggish\n10\n44\n8.94\n61.75\n\n\n9\neconomy\n18\n621\n8.89\n195.51\n\n\n10\nmeasures\n13\n288\n8.87\n37.66\n\n\n11\nsectors\n10\n89\n8.85\n46.76\n\n\n12\nperformance\n11\n165\n8.84\n40.00\n\n\n13\nsigns\n10\n107\n8.81\n43.03\n\n\n14\neconomists\n12\n325\n8.70\n30.36\n\n\n15\nimpact\n11\n249\n8.69\n31.43\n\n\n16\nwest\n20\n964\n8.69\n30.92\n\n\n17\nstrength\n9\n95\n8.69\n38.97\n\n\n18\ngood\n12\n361\n8.65\n28.11\n\n\n19\nshows\n8\n65\n8.58\n38.90\n\n\n20\nu.s.\n70\n5,496\n8.55\n57.99\n\n\n\nReport based on word tokens\n\n\nContext tokens left: 5, context tokens right: 5\n\n\nFiltered tokens by minimum collocation frequency (5)\n\n\nKeywords filtered based on p-value 0.0001 with Bonferroni correction (based on 204 tests)\n\n\nUnique collocates: 34\n\n\nShowing 20 rows\n\n\nPage 1 of 2",
    "crumbs": [
      "API",
      "collocates"
    ]
  },
  {
    "objectID": "api/result.html",
    "href": "api/result.html",
    "title": "result",
    "section": "",
    "text": "Most Conc reporting functionality outputs a Result object. This gives flexibility about how you want to work with the results, including displaying them in a notebook, converting the result to a Polars dataframe, or returning the HTML for the result.",
    "crumbs": [
      "API",
      "result"
    ]
  },
  {
    "objectID": "api/result.html#using-the-result-class",
    "href": "api/result.html#using-the-result-class",
    "title": "result",
    "section": "",
    "text": "Most Conc reporting functionality outputs a Result object. This gives flexibility about how you want to work with the results, including displaying them in a notebook, converting the result to a Polars dataframe, or returning the HTML for the result.",
    "crumbs": [
      "API",
      "result"
    ]
  },
  {
    "objectID": "api/result.html#result-class-api-reference",
    "href": "api/result.html#result-class-api-reference",
    "title": "result",
    "section": "Result class API reference",
    "text": "Result class API reference\n\nsource\n\nResult\n\n Result (type:str,\n         df:polars.dataframe.frame.DataFrame|polars.lazyframe.frame.LazyFr\n         ame, title:str, description:str, summary_data:dict,\n         formatted_data:list[str])\n\nClass for results from Conc reports\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntype\nstr\nreport type\n\n\ndf\npolars.dataframe.frame.DataFrame | polars.lazyframe.frame.LazyFrame\nPolars dataframe or lazyframe with the results\n\n\ntitle\nstr\ntitle of the report\n\n\ndescription\nstr\ndescription\n\n\nsummary_data\ndict\nsummary data (ignored)\n\n\nformatted_data\nlist\nlist of formatted data about the table\n\n\n\n\nsource\n\n\nResult.to_frame\n\n Result.to_frame (collect_if_lazy:bool=True)\n\nReturn result output from conc as a dataframe\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncollect_if_lazy\nbool\nTrue\nif the df is a lazyframe, collect before returning\n\n\n\n\nsource\n\n\nResult.display\n\n Result.display ()\n\nPrint analysis result output from conc in a nice table format using the great_tables library\n\nsource\n\n\nResult.to_html\n\n Result.to_html (collect_if_lazy:bool=True)\n\nReturn result output from conc as a dataframe\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncollect_if_lazy\nbool\nTrue\nif the df is a lazyframe, collect before returning\n\n\n\n\nExamples\nThis example is just to illustrate the basic functionality. When you run a Conc report method, it will return a Result object. You can add .display() to the result to output the result in your notebook. You can use .to_frame() if you want to work with the data as a Polars dataframe. The Recipes page has helpful examples relevant to working with Conc results outside of Conc.\n\nresult = Result(type = 'example', \n       df = pl.DataFrame({'Token': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n                          'Frequency': [5, 4, 3, 2, 1]}),\n       title = 'Example Table',\n       description = 'This is an example result.',\n       summary_data = {},\n       formatted_data = ['Formatted data text example 1', 'Formatted data text example 2']\n      )\n\nresult.display()\n\n\n\n\n\n\n\nExample Table\n\n\nThis is an example result.\n\n\nToken\nFrequency\n\n\n\n\nMonday\n5\n\n\nTuesday\n4\n\n\nWednesday\n3\n\n\nThursday\n2\n\n\nFriday\n1\n\n\n\nFormatted data text example 1\n\n\nFormatted data text example 2\n\n\n\n\n\n\n\n\n\n\nresult.to_frame()\n\n\nshape: (5, 2)\n\n\n\nToken\nFrequency\n\n\nstr\ni64\n\n\n\n\n\"Monday\"\n5\n\n\n\"Tuesday\"\n4\n\n\n\"Wednesday\"\n3\n\n\n\"Thursday\"\n2\n\n\n\"Friday\"\n1",
    "crumbs": [
      "API",
      "result"
    ]
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API",
    "section": "",
    "text": "This section documents the Conc API. Conc’s submodules are listed below. There are specific modules for different kinds of analysis or reporting. This may be helpful for understand functionality, but if you want to use Conc you can load a corpus with the Corpus class and access the main reporting functionality using the Conc class.",
    "crumbs": [
      "API"
    ]
  },
  {
    "objectID": "api/index.html#overview-of-conc-functionality",
    "href": "api/index.html#overview-of-conc-functionality",
    "title": "API",
    "section": "Overview of Conc functionality",
    "text": "Overview of Conc functionality\nThe current Conc submodules and what they do are listed below with links to the relevant documentation page.\n\n\n\nClass / Function\nSub-module\nFunctionality\nNote\n\n\n\n\nCorpus\nconc.corpus\nBuild and load and get information on a corpus, methods to work with a corpus\nRequired\n\n\nConc\nconc.conc\nInferface to Conc reports for corpus analysis\nRecommended way to access reports for analysis, requires a corpus created by Corpus module\n\n\nCorpora\nconc.conc\nFunctions to work with multiple corpora and download and build sample corpora.\nOptional\n\n\nFrequency\nconc.frequency\nFrequency reporting\nAccess via Conc\n\n\nNgrams\nconc.ngrams\nReporting on ngram_frequencies across corpus and ngrams containing specific tokens\nAccess via Conc\n\n\nConcordance\nconc.concordance\nConcordancing\nAccess via Conc\n\n\nKeyness\nconc.keyness\nReporting for keyness analysis\nAccess via Conc\n\n\nCollocates\nconc.collocates\nReporting for collocation analysis\nAccess via Conc\n\n\nResult\nconc.result\nHandles report results, output result as table or get dataframe\nUsed by all reports\n\n\nText\nconc.text\nOutput text from the corpus\nAccess via Corpus\n\n\nConcLogger\nconc.core\nLogger\nLogging implemented in all modules\n\n\nCorpusMetadata\nconc.core\nClass to validate Corpus Metadata JSON\nUsed by Corpus class\n\n\n\nThe conc.core and conc.corpora modules implements some helpful functions …\n\n\n\nFunction\nSub-module\nFunctionality\n\n\n\n\nlist_corpora\nconc.corpora\nScan a directory for corpora and return a summary\n\n\nget_stop_words\nconc.core\nGet a spaCy stop word list list for a specific model\n\n\nVarious - see Get data sources\nconc.corpora\nFunctions to download source texts to create sample corpora. Primarily intended for development/testing, but can be used to try out Conc. To minimize requirements not all libraries are installed by default. Functions will raise errors with information on installing required libraries.",
    "crumbs": [
      "API"
    ]
  },
  {
    "objectID": "api/plot.html",
    "href": "api/plot.html",
    "title": "plot",
    "section": "",
    "text": "Conc plotting functionality outputs a Plot object. This gives flexibility to display the plot in a notebook, write it to file, or return the HTML for the result.",
    "crumbs": [
      "API",
      "plot"
    ]
  },
  {
    "objectID": "api/plot.html#using-the-plot-class",
    "href": "api/plot.html#using-the-plot-class",
    "title": "plot",
    "section": "",
    "text": "Conc plotting functionality outputs a Plot object. This gives flexibility to display the plot in a notebook, write it to file, or return the HTML for the result.",
    "crumbs": [
      "API",
      "plot"
    ]
  },
  {
    "objectID": "api/plot.html#plot-class-api-reference",
    "href": "api/plot.html#plot-class-api-reference",
    "title": "plot",
    "section": "Plot class API reference",
    "text": "Plot class API reference\n\nsource\n\nPlot\n\n Plot (type:str, html:str)\n\nClass for handling delivery of Conc plot results\n\n\n\n\nType\nDetails\n\n\n\n\ntype\nstr\nreport type\n\n\nhtml\nstr\nHTML representation of the plot, may include js/css\n\n\n\n\nsource\n\n\nPlot.to_html\n\n Plot.to_html ()\n\nReturn HTML string for plot\n\nsource\n\n\nPlot.to_file\n\n Plot.to_file (filename:str, encoding='utf-8')\n\nReturn HTML string for plot\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfilename\nstr\n\nfilename to write HTML to\n\n\nencoding\nstr\nutf-8\nencoding string for writing file\n\n\n\n\nsource\n\n\nPlot.display\n\n Plot.display ()\n\nShow the plot in a Jupyter notebook",
    "crumbs": [
      "API",
      "plot"
    ]
  },
  {
    "objectID": "tutorials/start.html",
    "href": "tutorials/start.html",
    "title": "Get Started with Conc",
    "section": "",
    "text": "This is a quick, no-frills introduction to using Conc. You can skip part 1 if you already have some data you want to work with.",
    "crumbs": [
      "Tutorials",
      "Get Started with Conc"
    ]
  },
  {
    "objectID": "tutorials/start.html#get-some-sample-texts",
    "href": "tutorials/start.html#get-some-sample-texts",
    "title": "Get Started with Conc",
    "section": "1. Get some sample texts",
    "text": "1. Get some sample texts\nFor this getting started guide I’m going to use the example of a collection of short stories from Katherine Mansfield’s The Garden Party as sample texts. This corpus is available as a zip file of text files and can be downloaded via the conc.corpora submodule. First, we will import the function from conc.corpora to get the sample data.\n\nimport os\nfrom conc.corpora import get_garden_party\n\nNow we define where we want the data to be stored (source_path) and where we want the corpus to be saved (save_path). When the corpus is built it will be saved in a new directory in save_path. Note: the os.environ.get in the paths below are not required. You can specify paths directly as strings (e.g. /some/path/).\n\nsource_path = f'{os.environ.get(\"HOME\")}/data/'  \nsave_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'\n\nNow we download the data. This will create the source_path directory defined above if it is not already there (and it is somewhere your user can write).\n\nget_garden_party(source_path=source_path)",
    "crumbs": [
      "Tutorials",
      "Get Started with Conc"
    ]
  },
  {
    "objectID": "tutorials/start.html#build-the-corpus",
    "href": "tutorials/start.html#build-the-corpus",
    "title": "Get Started with Conc",
    "section": "2. Build the corpus",
    "text": "2. Build the corpus\nYou can currently build a Conc corpus from:\n\na directory of text files or a .zip/.tar/.tar.gz containing text files (Corpus.build_from_files)\n\na .csv file (or .csv.gz file) with a column containing your text (Corpus.build_from_csv)\n\nMore source types will be added in the future, but lots of data can be wrangled into these formats.\nBoth methods support importing metadata. See the documentation links above for more details.\nFor information on the Conc corpus format, see the Anatomy of a Conc Corpus.\nThe following code imports the Corpus class from conc.corpus.\n\nfrom conc.corpus import Corpus\n\nThe following line creates a Corpus, gives it a name and description, and builds it from the Garden Party source files.\nRemember, a new directory for your corpus will be created in save_path. The name of that directory is a slugified version of the name you pass in. For the Garden Party Corpus, the directory garden-party.corpus will be created. The folder name can be changed later if you want. You can distribute your corpus by sharing the directory and its contents.\nThe build process time depends on the size of your corpus. The build process produces a corpus format that is quick to load and use. In this case, the corpus is small and it is done in a couple of seconds even on a old, slow computer.\n\nname = 'Garden Party Corpus'\ndescription = 'A corpus of short stories from The Garden Party: and Other Stories by Katherine Mansfield. Texts downloaded from Project Gutenberg https://gutenberg.org/ and are in the public domain. The text files contain the short story without the title. https://github.com/ucdh/scraping-garden-party'\nsource_file = 'garden-party-corpus.zip'\n\ncorpus = Corpus(name=name, description=description).build_from_files(source_path = f'{source_path}{source_file}', save_path = save_path)\n\nTo get information on the corpus, including various summary counts and information on the path of the corpus, you can use the Corpus.summary method.\n\ncorpus.summary()\n\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nName\nGarden Party Corpus\n\n\nDescription\nA corpus of short stories from The Garden Party: and Other Stories by Katherine Mansfield. Texts downloaded from Project Gutenberg https://gutenberg.org/ and are in the public domain. The text files contain the short story without the title. https://github.com/ucdh/scraping-garden-party\n\n\nDate Created\n2025-07-01 13:08:28\n\n\nConc Version\n0.1.5\n\n\nCorpus Path\n/home/geoff/data/conc-test-corpora/garden-party.corpus\n\n\nDocument Count\n15\n\n\nToken Count\n74,664\n\n\nWord Token Count\n59,514\n\n\nUnique Tokens\n5,410\n\n\nUnique Word Tokens\n5,392",
    "crumbs": [
      "Tutorials",
      "Get Started with Conc"
    ]
  },
  {
    "objectID": "tutorials/start.html#load-a-conc-corpus",
    "href": "tutorials/start.html#load-a-conc-corpus",
    "title": "Get Started with Conc",
    "section": "3. Load a Conc corpus",
    "text": "3. Load a Conc corpus\nHere is how we can load the corpus we just build. We don’t need to pass in a name and description, we just need the path to the corpus.\n\ncorpus = Corpus().load(corpus_path=f'{save_path}garden-party.corpus')\n\nLet’s check our corpus information again. We could use the summary method again here, but we can also access this information using the Corpus.info method. Here we include the include_disk_usage parameter to get additional information on how much disk space our corpus is using.\n\nprint(corpus.info(include_disk_usage=True))\n\n┌────────────────────────────┬───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│ Attribute                  ┆ Value                                                                                                                                                                                                                                             │\n╞════════════════════════════╪═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╡\n│ Name                       ┆ Garden Party Corpus                                                                                                                                                                                                                               │\n│ Description                ┆ A corpus of short stories from The Garden Party: and Other Stories by Katherine Mansfield. Texts downloaded from Project Gutenberg https://gutenberg.org/ and are in the public domain. The text files contain the short story without the title. │\n│                            ┆ https://github.com/ucdh/scraping-garden-party                                                                                                                                                                                                     │\n│ Date Created               ┆ 2025-07-01 13:08:28                                                                                                                                                                                                                               │\n│ Conc Version               ┆ 0.1.5                                                                                                                                                                                                                                             │\n│ Corpus Path                ┆ /home/geoff/data/conc-test-corpora/garden-party.corpus                                                                                                                                                                                            │\n│ Document Count             ┆ 15                                                                                                                                                                                                                                                │\n│ Token Count                ┆ 74,664                                                                                                                                                                                                                                            │\n│ Word Token Count           ┆ 59,514                                                                                                                                                                                                                                            │\n│ Unique Tokens              ┆ 5,410                                                                                                                                                                                                                                             │\n│ Unique Word Tokens         ┆ 5,392                                                                                                                                                                                                                                             │\n│ Corpus Metadata (Mb)       ┆ 0.001                                                                                                                                                                                                                                             │\n│ Document Metadata (Mb)     ┆ 0.001                                                                                                                                                                                                                                             │\n│ Tokens (Mb)                ┆ 0.259                                                                                                                                                                                                                                             │\n│ Vocab (Mb)                 ┆ 0.073                                                                                                                                                                                                                                             │\n│ Punctuation Positions (Mb) ┆ 0.038                                                                                                                                                                                                                                             │\n│ Space Positions (Mb)       ┆ 0.017                                                                                                                                                                                                                                             │\n└────────────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Tutorials",
      "Get Started with Conc"
    ]
  },
  {
    "objectID": "tutorials/start.html#using-conc",
    "href": "tutorials/start.html#using-conc",
    "title": "Get Started with Conc",
    "section": "4. Using Conc",
    "text": "4. Using Conc\nTo use the corpus we need to import the Conc class from conc.conc.\n\nfrom conc.conc import Conc\n\nThe Conc class is the main interface for working with your corpus. It provides methods for a range of corpus analysis, including analysis of frequency, ngrams, concordances, collocates, and keyness. There are classes for all these different analyses, but the Conc class provides the most straightforward way to do analysis.\nHere we instantiate a Conc object with the corpus just loaded.\n\nconc = Conc(corpus=corpus)\n\nThis getting started guide is a work-in-progress. Check out the Conc code recipes to see example code to generate Conc reports, as well as the tables or visualisations they create. More documentation on Conc reports will be available soon, but for now refer to recipes and API reference. There are API documentation pages for the main analysis types (e.g. concordancing, keyness analysis) with information on the various parameters available.",
    "crumbs": [
      "Tutorials",
      "Get Started with Conc"
    ]
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "This section provides tutorials to get you started using Conc.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\nGet Started with Conc\n\n\nInstalled Conc? The getting started guide steps you through building and loading corpora and introduces how to use Conc for analysis.\n\n\n\n\n\n\nQuick Conc Recipes\n\n\nCode snippets for common tasks in Conc.\n\n\n\n\n\n\nInstalling Conc\n\n\nConc install options\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "explanations/performance.html",
    "href": "explanations/performance.html",
    "title": "Performance",
    "section": "",
    "text": "This page reports timing results of corpus building/loading and Conc report methods with different size corpora using a machine with Intel Core i7-14700F, NVME SSD and 16GB usable RAM under WSL.\n\nfrom conc.corpus import Corpus\nfrom conc.conc import Conc\n\n\ntest_corpora = {\n                'us-congressional-speeches-subset-10k': 'US Congressional Speeches Subset 10k',\n                'us-congressional-speeches-subset-100k': 'US Congressional Speeches Subset 100k',\n                'us-congressional-speeches-subset-200k': 'US Congressional Speeches Subset 200k',\n                'us-congressional-speeches-subset-500k': 'US Congressional Speeches Subset 500k'\n                }\n\nCorpus build time varies from 4 seconds for 2m token data source (10k texts) to 150 seconds for 100m token data source (500k texts). Currently to build corpora larger than this requires large RAM. Work on memory management is ongoing, but this will improve when Polars new streaming engine matures. This is in the Roadmap for the library.\n\ncorpora = {}\nfor slug, name in test_corpora.items():\n    logger.info(f'Starting {name} build ...')\n    description = f'1 million speeches sampled from https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset to create corpora of varying sizes for development and testing. The dataset card at Huggingface is empty, so there is no further information available on the contents. The title indicates how many speeches are included in this corpus. '\n    try:\n\n    except Exception as e:\n        raise e\n\nCPU times: user 4.45 s, sys: 224 ms, total: 4.67 s\nWall time: 3.82 s\nCPU times: user 46.3 s, sys: 2.45 s, total: 48.7 s\nWall time: 30 s\nCPU times: user 1min 38s, sys: 10.8 s, total: 1min 49s\nWall time: 1min 2s\nCPU times: user 3min 55s, sys: 32 s, total: 4min 27s\nWall time: 2min 26s\n\n\nCorpora are loaded lazily - meaning large data tables are only accessed when required. Similar load times regardless of corpus size …\n\nfor slug, name in test_corpora.items():\n\n    corpus.summary()\n    del corpus\n\nCPU times: user 211 ms, sys: 15.7 ms, total: 227 ms\nWall time: 266 ms\n\n\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nName\nUS Congressional Speeches Subset 10k\n\n\nDescription\n1 million speeches sampled from https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset to create corpora of varying sizes for development and testing. The dataset card at Huggingface is empty, so there is no further information available on the contents. The title indicates how many speeches are included in this corpus.\n\n\nDate Created\n2025-06-09 15:03:14\n\n\nConc Version\n0.0.1\n\n\nCorpus Path\n/home/geoff/data/conc-test-corpora/us-congressional-speeches-subset-10k.corpus\n\n\nDocument Count\n10,000\n\n\nToken Count\n1,954,972\n\n\nWord Token Count\n1,767,904\n\n\nUnique Tokens\n50,640\n\n\nUnique Word Tokens\n50,520\n\n\n\n\n\n\n\n\nCPU times: user 182 ms, sys: 27.6 ms, total: 209 ms\nWall time: 220 ms\n\n\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nName\nUS Congressional Speeches Subset 100k\n\n\nDescription\n1 million speeches sampled from https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset to create corpora of varying sizes for development and testing. The dataset card at Huggingface is empty, so there is no further information available on the contents. The title indicates how many speeches are included in this corpus.\n\n\nDate Created\n2025-06-09 15:03:44\n\n\nConc Version\n0.0.1\n\n\nCorpus Path\n/home/geoff/data/conc-test-corpora/us-congressional-speeches-subset-100k.corpus\n\n\nDocument Count\n100,000\n\n\nToken Count\n19,927,241\n\n\nWord Token Count\n18,020,769\n\n\nUnique Tokens\n214,502\n\n\nUnique Word Tokens\n214,175\n\n\n\n\n\n\n\n\nCPU times: user 209 ms, sys: 0 ns, total: 209 ms\nWall time: 219 ms\n\n\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nName\nUS Congressional Speeches Subset 200k\n\n\nDescription\n1 million speeches sampled from https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset to create corpora of varying sizes for development and testing. The dataset card at Huggingface is empty, so there is no further information available on the contents. The title indicates how many speeches are included in this corpus.\n\n\nDate Created\n2025-06-09 15:04:47\n\n\nConc Version\n0.0.1\n\n\nCorpus Path\n/home/geoff/data/conc-test-corpora/us-congressional-speeches-subset-200k.corpus\n\n\nDocument Count\n200,000\n\n\nToken Count\n39,963,039\n\n\nWord Token Count\n36,136,744\n\n\nUnique Tokens\n345,631\n\n\nUnique Word Tokens\n345,310\n\n\n\n\n\n\n\n\nCPU times: user 207 ms, sys: 0 ns, total: 207 ms\nWall time: 217 ms\n\n\n\n\n\n\n\n\nCorpus Summary\n\n\n\n\n\nAttribute\nValue\n\n\n\n\nName\nUS Congressional Speeches Subset 500k\n\n\nDescription\n1 million speeches sampled from https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset to create corpora of varying sizes for development and testing. The dataset card at Huggingface is empty, so there is no further information available on the contents. The title indicates how many speeches are included in this corpus.\n\n\nDate Created\n2025-06-09 15:07:14\n\n\nConc Version\n0.0.1\n\n\nCorpus Path\n/home/geoff/data/conc-test-corpora/us-congressional-speeches-subset-500k.corpus\n\n\nDocument Count\n500,000\n\n\nToken Count\n99,902,593\n\n\nWord Token Count\n90,341,944\n\n\nUnique Tokens\n655,344\n\n\nUnique Word Tokens\n654,824\n\n\n\n\n\n\n\n\n\nfor slug, name in test_corpora.items():\n    corpus = Corpus().load(f'{save_path}{slug}.corpus')\n    conc = Conc(corpus)\n\n    del corpus\n\n\n\n\n\n\n\nFrequencies\n\n\nFrequencies of word tokens, US Congressional Speeches Subset 10k\n\n\nRank\nToken\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe\n135,984\n769.18\n\n\n2\nof\n67,597\n382.36\n\n\n3\nto\n60,132\n340.13\n\n\n4\nand\n44,832\n253.59\n\n\n5\nin\n36,959\n209.06\n\n\n6\nthat\n34,135\n193.08\n\n\n7\na\n29,557\n167.19\n\n\n8\ni\n29,329\n165.90\n\n\n9\nis\n25,175\n142.40\n\n\n10\nthis\n19,173\n108.45\n\n\n\nReport based on word tokens\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens: 1,767,904\n\n\nUnique word tokens: 50,520\n\n\nShowing 10 rows\n\n\nPage 1 of 5053\n\n\n\n\n\n\n\n\n\nCPU times: user 22.9 ms, sys: 10.1 ms, total: 33 ms\nWall time: 37.3 ms\n\n\n\n\n\n\n\n\nFrequencies\n\n\nFrequencies of word tokens, US Congressional Speeches Subset 100k\n\n\nRank\nToken\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe\n1,389,439\n771.02\n\n\n2\nof\n687,127\n381.30\n\n\n3\nto\n610,266\n338.65\n\n\n4\nand\n459,220\n254.83\n\n\n5\nin\n379,946\n210.84\n\n\n6\nthat\n346,216\n192.12\n\n\n7\na\n302,256\n167.73\n\n\n8\ni\n297,077\n164.85\n\n\n9\nis\n250,677\n139.10\n\n\n10\nthis\n192,933\n107.06\n\n\n\nReport based on word tokens\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens: 18,020,769\n\n\nUnique word tokens: 214,175\n\n\nShowing 10 rows\n\n\nPage 1 of 21418\n\n\n\n\n\n\n\n\n\nCPU times: user 61.5 ms, sys: 38 ms, total: 99.4 ms\nWall time: 46.1 ms\n\n\n\n\n\n\n\n\nFrequencies\n\n\nFrequencies of word tokens, US Congressional Speeches Subset 200k\n\n\nRank\nToken\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe\n2,781,475\n769.71\n\n\n2\nof\n1,377,003\n381.05\n\n\n3\nto\n1,225,404\n339.10\n\n\n4\nand\n922,720\n255.34\n\n\n5\nin\n760,867\n210.55\n\n\n6\nthat\n695,665\n192.51\n\n\n7\na\n606,747\n167.90\n\n\n8\ni\n593,766\n164.31\n\n\n9\nis\n504,385\n139.58\n\n\n10\nthis\n386,922\n107.07\n\n\n\nReport based on word tokens\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens: 36,136,744\n\n\nUnique word tokens: 345,310\n\n\nShowing 10 rows\n\n\nPage 1 of 34532\n\n\n\n\n\n\n\n\n\nCPU times: user 53.7 ms, sys: 78.1 ms, total: 132 ms\nWall time: 49.8 ms\n\n\n\n\n\n\n\n\nFrequencies\n\n\nFrequencies of word tokens, US Congressional Speeches Subset 500k\n\n\nRank\nToken\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe\n6,951,503\n769.47\n\n\n2\nof\n3,446,705\n381.52\n\n\n3\nto\n3,059,159\n338.62\n\n\n4\nand\n2,308,134\n255.49\n\n\n5\nin\n1,902,118\n210.55\n\n\n6\nthat\n1,737,689\n192.35\n\n\n7\na\n1,514,676\n167.66\n\n\n8\ni\n1,481,424\n163.98\n\n\n9\nis\n1,261,935\n139.68\n\n\n10\nthis\n966,165\n106.95\n\n\n\nReport based on word tokens\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens: 90,341,944\n\n\nUnique word tokens: 654,824\n\n\nShowing 10 rows\n\n\nPage 1 of 65483\n\n\n\n\n\n\n\n\n\nCPU times: user 104 ms, sys: 105 ms, total: 210 ms\nWall time: 53.2 ms\n\n\n\nfor slug, name in test_corpora.items():\n    corpus = Corpus().load(f'{save_path}{slug}.corpus')\n    conc = Conc(corpus)\n\n    del corpus\n\n\n\n\n\n\n\nNgrams for \"economy\"\n\n\nUS Congressional Speeches Subset 10k\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe economy\n94\n0.53\n\n\n2\nour economy\n59\n0.33\n\n\n3\nof economy\n23\n0.13\n\n\n4\namerican economy\n11\n0.06\n\n\n5\nfor economy\n8\n0.05\n\n\n\nReport based on word tokens\n\n\nNgram length: 2, Token position: right\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 106\n\n\nTotal ngrams: 355\n\n\nShowing 5 rows\n\n\nPage 1 of 22\n\n\n\n\n\n\n\n\n\nCPU times: user 49.3 ms, sys: 28 ms, total: 77.3 ms\nWall time: 48.8 ms\n\n\n\n\n\n\n\n\nNgrams for \"economy\"\n\n\nUS Congressional Speeches Subset 100k\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe economy\n930\n0.52\n\n\n2\nour economy\n643\n0.36\n\n\n3\nof economy\n203\n0.11\n\n\n4\namerican economy\n116\n0.06\n\n\n5\nnational economy\n84\n0.05\n\n\n\nReport based on word tokens\n\n\nNgram length: 2, Token position: right\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 464\n\n\nTotal ngrams: 3,725\n\n\nShowing 5 rows\n\n\nPage 1 of 93\n\n\n\n\n\n\n\n\n\nCPU times: user 338 ms, sys: 57 ms, total: 395 ms\nWall time: 198 ms\n\n\n\n\n\n\n\n\nNgrams for \"economy\"\n\n\nUS Congressional Speeches Subset 200k\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe economy\n1,924\n0.53\n\n\n2\nour economy\n1,312\n0.36\n\n\n3\nof economy\n401\n0.11\n\n\n4\namerican economy\n242\n0.07\n\n\n5\nnational economy\n172\n0.05\n\n\n\nReport based on word tokens\n\n\nNgram length: 2, Token position: right\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 682\n\n\nTotal ngrams: 7,668\n\n\nShowing 5 rows\n\n\nPage 1 of 137\n\n\n\n\n\n\n\n\n\nCPU times: user 578 ms, sys: 233 ms, total: 811 ms\nWall time: 435 ms\n\n\n\n\n\n\n\n\nNgrams for \"economy\"\n\n\nUS Congressional Speeches Subset 500k\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nthe economy\n4,818\n0.53\n\n\n2\nour economy\n3,258\n0.36\n\n\n3\nof economy\n1,039\n0.12\n\n\n4\namerican economy\n588\n0.07\n\n\n5\nnational economy\n448\n0.05\n\n\n\nReport based on word tokens\n\n\nNgram length: 2, Token position: right\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 1,193\n\n\nTotal ngrams: 19,211\n\n\nShowing 5 rows\n\n\nPage 1 of 239\n\n\n\n\n\n\n\n\n\nCPU times: user 1.66 s, sys: 552 ms, total: 2.21 s\nWall time: 1.02 s\n\n\n\n# still working on this!\nfor slug, name in test_corpora.items():\n    corpus = Corpus().load(f'{save_path}{slug}.corpus')\n    conc = Conc(corpus)\n\n    del corpus\n\n\n\n\n\n\n\nNgram Frequencies\n\n\nUS Congressional Speeches Subset 10k\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nof the\n22,312\n126.21\n\n\n2\nin the\n10,982\n62.12\n\n\n3\nto the\n9,119\n51.58\n\n\n4\nit is\n5,140\n29.07\n\n\n5\nthat the\n5,123\n28.98\n\n\n\nReport based on word tokens\n\n\nNgram length: 2\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 396,623\n\n\nTotal ngrams: 1,584,710\n\n\nShowing 5 rows\n\n\nPage 1 of 79325\n\n\n\n\n\n\n\n\n\nCPU times: user 1.5 s, sys: 147 ms, total: 1.65 s\nWall time: 209 ms\n\n\n\n\n\n\n\n\nNgram Frequencies\n\n\nUS Congressional Speeches Subset 100k\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nof the\n227,943\n126.49\n\n\n2\nin the\n114,241\n63.39\n\n\n3\nto the\n92,967\n51.59\n\n\n4\nit is\n51,659\n28.67\n\n\n5\nthat the\n51,620\n28.64\n\n\n\nReport based on word tokens\n\n\nNgram length: 2\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 2,046,190\n\n\nTotal ngrams: 16,153,485\n\n\nShowing 5 rows\n\n\nPage 1 of 409238\n\n\n\n\n\n\n\n\n\nCPU times: user 35.7 s, sys: 1.09 s, total: 36.8 s\nWall time: 831 ms\n\n\n\n\n\n\n\n\nNgram Frequencies\n\n\nUS Congressional Speeches Subset 200k\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nof the\n457,057\n126.48\n\n\n2\nin the\n228,891\n63.34\n\n\n3\nto the\n186,449\n51.60\n\n\n4\nit is\n103,619\n28.67\n\n\n5\nthat the\n103,418\n28.62\n\n\n\nReport based on word tokens\n\n\nNgram length: 2\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 3,304,755\n\n\nTotal ngrams: 32,389,849\n\n\nShowing 5 rows\n\n\nPage 1 of 660951\n\n\n\n\n\n\n\n\n\nCPU times: user 1min 9s, sys: 2.22 s, total: 1min 11s\nWall time: 4.33 s\n\n\n\n\n\n\n\n\nNgram Frequencies\n\n\nUS Congressional Speeches Subset 500k\n\n\nRank\nNgram\nFrequency\nNormalized Frequency\n\n\n\n\n1\nof the\n1,140,304\n126.22\n\n\n2\nin the\n570,295\n63.13\n\n\n3\nto the\n467,816\n51.78\n\n\n4\nit is\n259,770\n28.75\n\n\n5\nthat the\n258,068\n28.57\n\n\n\nReport based on word tokens\n\n\nNgram length: 2\n\n\nNgrams containing punctuation tokens excluded\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal unique ngrams: 6,158,427\n\n\nTotal ngrams: 80,976,586\n\n\nShowing 5 rows\n\n\nPage 1 of 1231686\n\n\n\n\n\n\n\n\n\nCPU times: user 3min 16s, sys: 10.2 s, total: 3min 26s\nWall time: 11.9 s\n\n\n\nfor slug, name in test_corpora.items():\n    corpus = Corpus().load(f'{save_path}{slug}.corpus')\n    conc = Conc(corpus)\n\n    del corpus\n\n\n\n\n\n\n\nConcordance for \"economy\"\n\n\nUS Congressional Speeches Subset 10k, Context tokens: 5, Order: 1R2R3R\n\n\nDocument Id\nLeft\nNode\nRight\n\n\n\n\n5,878\nruled by a government of\neconomy\n.\n\n\n1,163\n. help strengthen our Nations\neconomy\n.\n\n\n316\notherwise generally strong and prosperous\neconomy\n.\n\n\n6,910\nthis critical sector in our\neconomy\n.\n\n\n9,517\nhealth care pressures in this\neconomy\n.\n\n\n\nTotal Concordance Lines: 358\n\n\nTotal Documents: 251\n\n\nShowing 5 lines\n\n\nPage 1 of 72\n\n\n\n\n\n\n\n\n\nCPU times: user 89.7 ms, sys: 1.14 ms, total: 90.8 ms\nWall time: 61.2 ms\n\n\n\n\n\n\n\n\nConcordance for \"economy\"\n\n\nUS Congressional Speeches Subset 100k, Context tokens: 5, Order: 1R2R3R\n\n\nDocument Id\nLeft\nNode\nRight\n\n\n\n\n82,659\namounts that it throws our\neconomy\n\n\n\n75,018\nHoney . I shrun the\neconomy\n\" ? It is honest\n\n\n19,176\ngetting away from \" Coolidge\neconomy\n\" already . and making\n\n\n6,729\n. We are talking \"\neconomy\n\" and at the same\n\n\n83,170\nfurther into an \" innovating\neconomy\n\" based on a highly\n\n\n\nTotal Concordance Lines: 3758\n\n\nTotal Documents: 2684\n\n\nShowing 5 lines\n\n\nPage 1 of 752\n\n\n\n\n\n\n\n\n\nCPU times: user 414 ms, sys: 340 ms, total: 755 ms\nWall time: 437 ms\n\n\n\n\n\n\n\n\nConcordance for \"economy\"\n\n\nUS Congressional Speeches Subset 200k, Context tokens: 5, Order: 1R2R3R\n\n\nDocument Id\nLeft\nNode\nRight\n\n\n\n\n77,084\nthe way it is .\nECONOMY\n\n\n\n6,026\nits central office . Political\nEconomy\n\n\n\n130,531\nthe maintenance of her national\neconomy\n\n\n\n20,685\non something else . Coolidge\neconomy\n! I am for it\n\n\n132,603\nrailroads of this country .\nEconomy\n! What about this pitpible\n\n\n\nTotal Concordance Lines: 7753\n\n\nTotal Documents: 5480\n\n\nShowing 5 lines\n\n\nPage 1 of 1551\n\n\n\n\n\n\n\n\n\nCPU times: user 871 ms, sys: 596 ms, total: 1.47 s\nWall time: 831 ms\n\n\n\n\n\n\n\n\nConcordance for \"economy\"\n\n\nUS Congressional Speeches Subset 500k, Context tokens: 5, Order: 1R2R3R\n\n\nDocument Id\nLeft\nNode\nRight\n\n\n\n\n140,837\nprayers are with them .\nECONOMY\n\n\n\n162,997\nits central office . Political\nEconomy\n\n\n\n325,086\nWHAT ARE CoNDrrONS IN THE\nECONOMY\n\n\n\n64,711\ncountry . Condition of Nations\nEconomy\n\n\n\n360,787\ncountry ! This spasm of\neconomy\n!\n\n\n\nTotal Concordance Lines: 19399\n\n\nTotal Documents: 13564\n\n\nShowing 5 lines\n\n\nPage 1 of 3880\n\n\n\n\n\n\n\n\n\nCPU times: user 2.83 s, sys: 1.42 s, total: 4.24 s\nWall time: 1.86 s\n\n\n\nreference = Corpus().load(f'{save_path}brown.corpus')\nfor slug, name in test_corpora.items():\n    corpus = Corpus().load(f'{save_path}{slug}.corpus')\n    conc = Conc(corpus)\n    conc.set_reference_corpus(reference)\n\n    del corpus\n\n\n\n\n\n\n\nKeywords\n\n\nTarget corpus: US Congressional Speeches Subset 10k, Reference corpus: Brown Corpus\n\n\nRank\nToken\nFrequency\nFrequency Reference\nNormalized Frequency\nNormalized Frequency Reference\nRelative Risk\nLog Ratio\nLog Likelihood\n\n\n\n\n1\nunanimous\n907\n5\n5.13\n0.05\n100.57\n6.65\n748.42\n\n\n2\namendment\n4,039\n24\n22.85\n0.24\n93.30\n6.54\n3,318.48\n\n\n3\nappropriation\n716\n5\n4.05\n0.05\n79.39\n6.31\n582.28\n\n\n4\nsenator\n5,488\n39\n31.04\n0.40\n78.02\n6.29\n4,457.76\n\n\n5\nsubcommittee\n585\n5\n3.31\n0.05\n64.87\n6.02\n468.73\n\n\n\nReport based on word tokens\n\n\nFiltered tokens by minimum frequency in target corpus (5), minimum frequency in reference corpus (5)\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens in target corpus: 1,767,904\n\n\nTotal word tokens in reference corpus: 980,144\n\n\nKeywords: 8,291\n\n\nShowing 5 rows\n\n\nPage 1 of 1659\n\n\n\n\n\n\n\n\n\nCPU times: user 369 ms, sys: 220 ms, total: 589 ms\nWall time: 94.3 ms\n\n\n\n\n\n\n\n\nKeywords\n\n\nTarget corpus: US Congressional Speeches Subset 100k, Reference corpus: Brown Corpus\n\n\nRank\nToken\nFrequency\nFrequency Reference\nNormalized Frequency\nNormalized Frequency Reference\nRelative Risk\nLog Ratio\nLog Likelihood\n\n\n\n\n1\nunanimous\n8,978\n5\n4.98\n0.05\n97.66\n6.61\n895.70\n\n\n2\namendment\n39,940\n24\n22.16\n0.24\n90.51\n6.50\n3,968.88\n\n\n3\nappropriation\n6,847\n5\n3.80\n0.05\n74.48\n6.22\n672.68\n\n\n4\nsenator\n52,772\n39\n29.28\n0.40\n73.60\n6.20\n5,180.64\n\n\n5\ngentleman\n32,178\n28\n17.86\n0.29\n62.51\n5.97\n3,123.80\n\n\n\nReport based on word tokens\n\n\nFiltered tokens by minimum frequency in target corpus (5), minimum frequency in reference corpus (5)\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens in target corpus: 18,020,769\n\n\nTotal word tokens in reference corpus: 980,144\n\n\nKeywords: 12,136\n\n\nShowing 5 rows\n\n\nPage 1 of 2428\n\n\n\n\n\n\n\n\n\nCPU times: user 2.78 s, sys: 417 ms, total: 3.19 s\nWall time: 274 ms\n\n\n\n\n\n\n\n\nKeywords\n\n\nTarget corpus: US Congressional Speeches Subset 200k, Reference corpus: Brown Corpus\n\n\nRank\nToken\nFrequency\nFrequency Reference\nNormalized Frequency\nNormalized Frequency Reference\nRelative Risk\nLog Ratio\nLog Likelihood\n\n\n\n\n1\nunanimous\n17,813\n5\n4.93\n0.05\n96.63\n6.59\n897.98\n\n\n2\namendment\n80,078\n24\n22.16\n0.24\n90.50\n6.50\n4,023.10\n\n\n3\nappropriation\n13,896\n5\n3.85\n0.05\n75.38\n6.24\n690.81\n\n\n4\nsenator\n105,824\n39\n29.28\n0.40\n73.60\n6.20\n5,252.88\n\n\n5\ngentleman\n63,852\n28\n17.67\n0.29\n61.85\n5.95\n3,132.10\n\n\n\nReport based on word tokens\n\n\nFiltered tokens by minimum frequency in target corpus (5), minimum frequency in reference corpus (5)\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens in target corpus: 36,136,744\n\n\nTotal word tokens in reference corpus: 980,144\n\n\nKeywords: 12,704\n\n\nShowing 5 rows\n\n\nPage 1 of 2541\n\n\n\n\n\n\n\n\n\nCPU times: user 6.71 s, sys: 451 ms, total: 7.16 s\nWall time: 516 ms\n\n\n\n\n\n\n\n\nKeywords\n\n\nTarget corpus: US Congressional Speeches Subset 500k, Reference corpus: Brown Corpus\n\n\nRank\nToken\nFrequency\nFrequency Reference\nNormalized Frequency\nNormalized Frequency Reference\nRelative Risk\nLog Ratio\nLog Likelihood\n\n\n\n\n1\nunanimous\n44,193\n5\n4.89\n0.05\n95.89\n6.58\n898.23\n\n\n2\namendment\n198,132\n24\n21.93\n0.24\n89.57\n6.48\n4,012.78\n\n\n3\nappropriation\n34,215\n5\n3.79\n0.05\n74.24\n6.21\n685.45\n\n\n4\nsenator\n264,478\n39\n29.28\n0.40\n73.57\n6.20\n5,295.45\n\n\n5\ngentleman\n159,877\n28\n17.70\n0.29\n61.95\n5.95\n3,163.94\n\n\n\nReport based on word tokens\n\n\nFiltered tokens by minimum frequency in target corpus (5), minimum frequency in reference corpus (5)\n\n\nNormalized Frequency is per 10,000 tokens\n\n\nTotal word tokens in target corpus: 90,341,944\n\n\nTotal word tokens in reference corpus: 980,144\n\n\nKeywords: 13,118\n\n\nShowing 5 rows\n\n\nPage 1 of 2624\n\n\n\n\n\n\n\n\n\nCPU times: user 20 s, sys: 802 ms, total: 20.8 s\nWall time: 1.17 s\n\n\n\nfor slug, name in test_corpora.items():\n    corpus = Corpus().load(f'{save_path}{slug}.corpus')\n    conc = Conc(corpus)\n\n    del corpus\n\n\n\n\n\n\n\nCollocates of \"economy\"\n\n\nUS Congressional Speeches Subset 10k\n\n\nRank\nToken\nCollocate Frequency\nFrequency\nLogdice\nLog Likelihood\n\n\n\n\n1\neconomy\n20\n358\n9.84\n248.59\n\n\n2\nhealthy\n10\n50\n9.65\n74.41\n\n\n3\nsegment\n9\n24\n9.59\n80.17\n\n\n4\nour\n93\n5,938\n8.92\n221.67\n\n\n5\nfalse\n6\n55\n8.89\n36.86\n\n\n\nReport based on word tokens\n\n\nContext tokens left: 5, context tokens right: 5\n\n\nFiltered tokens by minimum collocation frequency (5)\n\n\nUnique collocates: 115\n\n\nShowing 5 rows\n\n\nPage 1 of 24\n\n\n\n\n\n\n\n\n\nCPU times: user 123 ms, sys: 21.9 ms, total: 145 ms\nWall time: 54.8 ms\n\n\n\n\n\n\n\n\nCollocates of \"economy\"\n\n\nUS Congressional Speeches Subset 100k\n\n\nRank\nToken\nCollocate Frequency\nFrequency\nLogdice\nLog Likelihood\n\n\n\n\n1\nour\n1,084\n60,051\n9.12\n2,801.70\n\n\n2\nefficiency\n60\n732\n8.77\n329.93\n\n\n3\nstimulate\n51\n299\n8.69\n358.80\n\n\n4\nglobal\n55\n618\n8.69\n311.68\n\n\n5\njobs\n83\n3,100\n8.63\n274.52\n\n\n\nReport based on word tokens\n\n\nContext tokens left: 5, context tokens right: 5\n\n\nFiltered tokens by minimum collocation frequency (5)\n\n\nUnique collocates: 864\n\n\nShowing 5 rows\n\n\nPage 1 of 173\n\n\n\n\n\n\n\n\n\nCPU times: user 413 ms, sys: 244 ms, total: 657 ms\nWall time: 328 ms\n\n\n\n\n\n\n\n\nCollocates of \"economy\"\n\n\nUS Congressional Speeches Subset 200k\n\n\nRank\nToken\nCollocate Frequency\nFrequency\nLogdice\nLog Likelihood\n\n\n\n\n1\nour\n2,219\n121,489\n9.14\n5,670.76\n\n\n2\nglobal\n119\n1,221\n8.76\n689.99\n\n\n3\nsector\n119\n1,741\n8.68\n604.09\n\n\n4\nstimulate\n101\n611\n8.63\n698.06\n\n\n5\njobs\n166\n6,312\n8.60\n534.83\n\n\n\nReport based on word tokens\n\n\nContext tokens left: 5, context tokens right: 5\n\n\nFiltered tokens by minimum collocation frequency (5)\n\n\nUnique collocates: 1,524\n\n\nShowing 5 rows\n\n\nPage 1 of 305\n\n\n\n\n\n\n\n\n\nCPU times: user 710 ms, sys: 212 ms, total: 922 ms\nWall time: 523 ms\n\n\n\n\n\n\n\n\nCollocates of \"economy\"\n\n\nUS Congressional Speeches Subset 500k\n\n\nRank\nToken\nCollocate Frequency\nFrequency\nLogdice\nLog Likelihood\n\n\n\n\n1\nour\n5,656\n304,919\n9.16\n14,596.00\n\n\n2\nstimulate\n267\n1,472\n8.71\n1,898.46\n\n\n3\nglobal\n283\n2,924\n8.70\n1,636.06\n\n\n4\njobs\n418\n15,339\n8.62\n1,373.41\n\n\n5\neconomy\n446\n19,399\n8.56\n5,491.13\n\n\n\nReport based on word tokens\n\n\nContext tokens left: 5, context tokens right: 5\n\n\nFiltered tokens by minimum collocation frequency (5)\n\n\nUnique collocates: 2,786\n\n\nShowing 5 rows\n\n\nPage 1 of 558\n\n\n\n\n\n\n\n\n\nCPU times: user 2.14 s, sys: 589 ms, total: 2.73 s\nWall time: 1.34 s",
    "crumbs": [
      "Explanations",
      "Performance"
    ]
  },
  {
    "objectID": "explanations/index.html",
    "href": "explanations/index.html",
    "title": "Explanations",
    "section": "",
    "text": "This section provides information on Conc, how it works, and how to access Conc results or data structures for use with other Python libraries.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\nWhy Conc?\n\n\nWhy I’ve built Conc and the principles guiding its development.\n\n\n\n\n\n\nAnatomy of a corpus\n\n\nInformation on Conc corpus format if you want to access the data directly.\n\n\n\n\n\n\nPerformance\n\n\nInformation on Conc performance across different corpus sizes.\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Explanations"
    ]
  }
]