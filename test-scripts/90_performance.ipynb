{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "This page reports timing results of conc methods with different size corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# %load_ext memray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conc.core import logger, set_logger_state\n",
    "from conc.corpus import Corpus\n",
    "from conc.conc import Conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_logger_state('quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = f'{os.environ.get(\"HOME\")}/data/'\n",
    "save_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 14:52:00 - INFO - <module> - Starting US Congressional Speeches Subset 10k build ...\n",
      "2025-06-04 14:52:00 - INFO - memory_usage - init, memory usage: 234.1484375 MB\n",
      "2025-06-04 14:52:02 - INFO - memory_usage - processed 5000 documents, memory usage: 553.34765625 MB, difference: 319.19921875 MB\n",
      "2025-06-04 14:52:03 - INFO - memory_usage - processed 10000 documents, memory usage: 568.4765625 MB, difference: 15.12890625 MB\n",
      "2025-06-04 14:52:03 - INFO - memory_usage - Completing build process, memory usage: 568.4765625 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:03 - INFO - memory_usage - init, memory usage: 568.4765625 MB\n",
      "2025-06-04 14:52:03 - INFO - memory_usage - got input length 1975172, memory usage: 574.7578125 MB, difference: 6.28125 MB\n",
      "2025-06-04 14:52:03 - INFO - memory_usage - collected vocab, memory usage: 574.7578125 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:03 - INFO - memory_usage - freed up combined_df and input_df, memory usage: 574.7578125 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - got vocab strings, memory usage: 653.25390625 MB, difference: 78.49609375 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - added vocab strings, memory usage: 653.25390625 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - got punct tokens, memory usage: 699.16796875 MB, difference: 45.9140625 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - got space tokens, memory usage: 699.16796875 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - saved punct positions, memory usage: 776.24609375 MB, difference: 77.078125 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - saved space positions, memory usage: 788.1953125 MB, difference: 11.94921875 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - added frequency to vocab, memory usage: 788.1953125 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - got unique tokens None, memory usage: 804.34765625 MB, difference: 16.15234375 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - added is_punct is_space to vocab, memory usage: 804.34765625 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - wrote vocab to disk, memory usage: 1648.1328125 MB, difference: 843.78515625 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - wrote tokens to disk, memory usage: 1646.39453125 MB, difference: -1.73828125 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - got doc count 10000, memory usage: 1645.77734375 MB, difference: -0.6171875 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - got token count, memory usage: 1645.77734375 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - got punct token count, memory usage: 1648.3671875 MB, difference: 2.58984375 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - got space token count, memory usage: 1651.03515625 MB, difference: 2.66796875 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - done, memory usage: 1651.03515625 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - Completed build process, memory usage: 1651.03515625 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:04 - INFO - save_corpus_metadata - Saved corpus metadata time: 0.001 seconds\n",
      "2025-06-04 14:52:04 - INFO - build - Build time: 4.310 seconds\n",
      "2025-06-04 14:52:04 - INFO - build_from_csv - Build from csv time: 4.569 seconds\n",
      "2025-06-04 14:52:04 - INFO - <module> - Starting US Congressional Speeches Subset 100k build ...\n",
      "2025-06-04 14:52:04 - INFO - memory_usage - init, memory usage: 1673.09375 MB\n",
      "2025-06-04 14:52:06 - INFO - memory_usage - processed 5000 documents, memory usage: 1697.65234375 MB, difference: 24.55859375 MB\n",
      "2025-06-04 14:52:08 - INFO - memory_usage - processed 10000 documents, memory usage: 1710.2109375 MB, difference: 12.55859375 MB\n",
      "2025-06-04 14:52:09 - INFO - memory_usage - processed 15000 documents, memory usage: 1713.1328125 MB, difference: 2.921875 MB\n",
      "2025-06-04 14:52:11 - INFO - memory_usage - processed 20000 documents, memory usage: 1728.6015625 MB, difference: 15.46875 MB\n",
      "2025-06-04 14:52:12 - INFO - memory_usage - processed 25000 documents, memory usage: 1730.9140625 MB, difference: 2.3125 MB\n",
      "2025-06-04 14:52:13 - INFO - memory_usage - processed 30000 documents, memory usage: 1735.09765625 MB, difference: 4.18359375 MB\n",
      "2025-06-04 14:52:15 - INFO - memory_usage - processed 35000 documents, memory usage: 1731.390625 MB, difference: -3.70703125 MB\n",
      "2025-06-04 14:52:16 - INFO - memory_usage - processed 40000 documents, memory usage: 1733.33984375 MB, difference: 1.94921875 MB\n",
      "2025-06-04 14:52:17 - INFO - memory_usage - processed 45000 documents, memory usage: 1735.390625 MB, difference: 2.05078125 MB\n",
      "2025-06-04 14:52:19 - INFO - memory_usage - processed 50000 documents, memory usage: 1747.35546875 MB, difference: 11.96484375 MB\n",
      "2025-06-04 14:52:20 - INFO - memory_usage - processed 55000 documents, memory usage: 1772.46484375 MB, difference: 25.109375 MB\n",
      "2025-06-04 14:52:22 - INFO - memory_usage - processed 60000 documents, memory usage: 1759.0 MB, difference: -13.46484375 MB\n",
      "2025-06-04 14:52:23 - INFO - memory_usage - processed 65000 documents, memory usage: 1760.84375 MB, difference: 1.84375 MB\n",
      "2025-06-04 14:52:22 - INFO - memory_usage - processed 70000 documents, memory usage: 1762.5859375 MB, difference: 1.7421875 MB\n",
      "2025-06-04 14:52:24 - INFO - memory_usage - processed 75000 documents, memory usage: 1764.359375 MB, difference: 1.7734375 MB\n",
      "2025-06-04 14:52:25 - INFO - memory_usage - processed 80000 documents, memory usage: 1766.03515625 MB, difference: 1.67578125 MB\n",
      "2025-06-04 14:52:26 - INFO - memory_usage - processed 85000 documents, memory usage: 1775.71484375 MB, difference: 9.6796875 MB\n",
      "2025-06-04 14:52:28 - INFO - memory_usage - processed 90000 documents, memory usage: 1777.453125 MB, difference: 1.73828125 MB\n",
      "2025-06-04 14:52:29 - INFO - memory_usage - processed 95000 documents, memory usage: 1778.95703125 MB, difference: 1.50390625 MB\n",
      "2025-06-04 14:52:31 - INFO - memory_usage - processed 100000 documents, memory usage: 1780.953125 MB, difference: 1.99609375 MB\n",
      "2025-06-04 14:52:31 - INFO - memory_usage - Completing build process, memory usage: 1780.953125 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:31 - INFO - memory_usage - init, memory usage: 1780.953125 MB\n",
      "2025-06-04 14:52:31 - INFO - memory_usage - got input length 20127441, memory usage: 1781.43359375 MB, difference: 0.48046875 MB\n",
      "2025-06-04 14:52:31 - INFO - memory_usage - collected vocab, memory usage: 1781.43359375 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:31 - INFO - memory_usage - freed up combined_df and input_df, memory usage: 1781.43359375 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:32 - INFO - memory_usage - got vocab strings, memory usage: 1973.7421875 MB, difference: 192.30859375 MB\n",
      "2025-06-04 14:52:32 - INFO - memory_usage - added vocab strings, memory usage: 1980.2578125 MB, difference: 6.515625 MB\n",
      "2025-06-04 14:52:32 - INFO - memory_usage - got punct tokens, memory usage: 2034.47265625 MB, difference: 54.21484375 MB\n",
      "2025-06-04 14:52:32 - INFO - memory_usage - got space tokens, memory usage: 2034.47265625 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:32 - INFO - memory_usage - saved punct positions, memory usage: 2152.3046875 MB, difference: 117.83203125 MB\n",
      "2025-06-04 14:52:32 - INFO - memory_usage - saved space positions, memory usage: 2195.37109375 MB, difference: 43.06640625 MB\n",
      "2025-06-04 14:52:32 - INFO - memory_usage - added frequency to vocab, memory usage: 2195.37109375 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:33 - INFO - memory_usage - got unique tokens None, memory usage: 2236.48828125 MB, difference: 41.1171875 MB\n",
      "2025-06-04 14:52:33 - INFO - memory_usage - added is_punct is_space to vocab, memory usage: 2236.48828125 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:36 - INFO - memory_usage - wrote vocab to disk, memory usage: 8371.68359375 MB, difference: 6135.1953125 MB\n",
      "2025-06-04 14:52:37 - INFO - memory_usage - wrote tokens to disk, memory usage: 8304.92578125 MB, difference: -66.7578125 MB\n",
      "2025-06-04 14:52:37 - INFO - memory_usage - got doc count 100000, memory usage: 8297.36328125 MB, difference: -7.5625 MB\n",
      "2025-06-04 14:52:37 - INFO - memory_usage - got token count, memory usage: 8297.36328125 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:37 - INFO - memory_usage - got punct token count, memory usage: 8297.23828125 MB, difference: -0.125 MB\n",
      "2025-06-04 14:52:37 - INFO - memory_usage - got space token count, memory usage: 8297.953125 MB, difference: 0.71484375 MB\n",
      "2025-06-04 14:52:37 - INFO - memory_usage - done, memory usage: 8297.953125 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:37 - INFO - memory_usage - Completed build process, memory usage: 8297.953125 MB, difference: 0.0 MB\n",
      "2025-06-04 14:52:37 - INFO - save_corpus_metadata - Saved corpus metadata time: 0.000 seconds\n",
      "2025-06-04 14:52:37 - INFO - build - Build time: 32.926 seconds\n",
      "2025-06-04 14:52:37 - INFO - build_from_csv - Build from csv time: 33.158 seconds\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "test_corpora = {\n",
    "\t\t\t\t'us-congressional-speeches-subset-10k': 'US Congressional Speeches Subset 10k',\n",
    "                'us-congressional-speeches-subset-100k': 'US Congressional Speeches Subset 100k',\n",
    "\t\t\t\t#'us-congressional-speeches-subset-500k': 'US Congressional Speeches Subset 500k'\n",
    "\t\t\t\t}\n",
    "\n",
    "\n",
    "corpora = {}\n",
    "for slug, name in test_corpora.items():\n",
    "\tset_logger_state('verbose')\n",
    "\tlogger.info(f'Starting {name} build ...')\n",
    "\tdescription = f'1 million speeches sampled from https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset to create corpora of varying sizes for development and testing. The dataset card at Huggingface is empty, so there is no further information available on the contents. The title indicates how many speeches are included in this corpus. '\n",
    "\ttry:\n",
    "\t\tcorpora[slug] = Corpus(name = name, description = description).build_from_csv(f'{source_path}{slug}.csv.gz', save_path = save_path, text_column='text', metadata_columns = ['speech_id', 'date', 'speaker', 'chamber', 'state'], build_process_cleanup = False)\n",
    "\t\tdel corpora[slug]\n",
    "\texcept Exception as e:\n",
    "\t\traise e\n",
    "\tset_logger_state('quiet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56 ms ± 376 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "13.9 ms ± 169 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "19.4 ms ± 183 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "for name in test_corpora_names:\n",
    "    %timeit report[name].frequencies(normalize_by=10000, page_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conc.frequency import Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {}\n",
    "for slug, name in test_corpora.items():\n",
    "    report[slug] = Frequency(corpora[slug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_logger_state('quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.95 ms ± 155 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "22.1 ms ± 394 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "34.9 ms ± 340 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "for slug, name in test_corpora.items():\n",
    "    %timeit report[slug].frequencies(normalize_by=10000, page_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714 μs ± 32.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "11.2 ms ± 348 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "23.6 ms ± 1.21 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "token_str = 'dog'\n",
    "use_cache = False\n",
    "for name in test_corpora_names:\n",
    "    %timeit report[name].ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', use_cache = use_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.4 μs ± 544 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "96.8 μs ± 405 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "97.1 μs ± 383 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "token_str = 'dog'\n",
    "use_cache = True\n",
    "for name in test_corpora_names:\n",
    "    report[name].ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', use_cache = use_cache) # warm up\n",
    "    %timeit report[name].ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', use_cache = use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.44 ms ± 121 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "84.5 ms ± 1.15 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "176 ms ± 902 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "token_str = 'the'\n",
    "use_cache = False\n",
    "for name in test_corpora_names:\n",
    "    %timeit report[name].ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', use_cache = use_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.1 μs ± 545 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "89.4 μs ± 1.49 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "87.3 μs ± 214 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "token_str = 'the'\n",
    "use_cache = True\n",
    "for name in test_corpora_names:\n",
    "    report[name].ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', use_cache = use_cache) # warm up\n",
    "    %timeit report[name].ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', use_cache = use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99 ms ± 241 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "14 ms ± 401 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "25.9 ms ± 697 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "use_cache = False\n",
    "token_str = 'dog'\n",
    "for name in test_corpora_names:\n",
    "    %timeit report[name].concordance(token_str, context_words = 5, order='1L2L3L', use_cache = use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.98 ms ± 32 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "2.26 ms ± 158 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "2.5 ms ± 19 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "use_cache = True\n",
    "token_str = 'dog'\n",
    "for name in test_corpora_names:\n",
    "    report[name].concordance(token_str, context_words = 5, order='1L2L3L', use_cache = use_cache) # warm up\n",
    "    %timeit report[name].concordance(token_str, context_words = 5, order='1L2L3L', use_cache = use_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.4 ms ± 251 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "102 ms ± 2.65 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "214 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "use_cache = False\n",
    "token_str = 'the'\n",
    "for name in test_corpora_names:\n",
    "    %timeit report[name].concordance(token_str, context_words = 5, order='1L2L3L', use_cache = use_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.42 ms ± 59.5 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "3.35 ms ± 148 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "4.23 ms ± 157 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "use_cache = True\n",
    "token_str = 'the'\n",
    "for name in test_corpora_names:\n",
    "    report[name].concordance(token_str, context_words = 5, order='1L2L3L', use_cache = use_cache) # warm up\n",
    "    %timeit report[name].concordance(token_str, context_words = 5, order='1L2L3L', use_cache = use_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reclaim space!\n",
    "for name in test_corpora_names:\n",
    "\tif os.path.exists(f'{save_path}{name}.corpus'):\n",
    "\t\tos.remove(f'{save_path}{name}.corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
