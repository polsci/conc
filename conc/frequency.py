"""Functionality for frequency analysis."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/70_frequency.ipynb.

# %% auto 0
__all__ = ['Frequency']

# %% ../nbs/70_frequency.ipynb 3
import time
import polars as pl
from fastcore.basics import patch

# %% ../nbs/70_frequency.ipynb 4
from .corpus import Corpus
from .result import Result
from .core import logger, PAGE_SIZE

# %% ../nbs/70_frequency.ipynb 5
class Frequency:
	""" Class for frequency analysis reporting """
	def __init__(self,
			  corpus:Corpus # Corpus instance
			  ): 
		self.corpus = corpus


# %% ../nbs/70_frequency.ipynb 6
@patch
def frequencies(self: Frequency,
				normalize_by:int=1000000, # normalize frequencies by a number (e.g. 10000)
				page_size:int=PAGE_SIZE, # number of rows to return
				page_current:int=1, # current page
				show_token_id:bool=False, # show token_id in output
				exclude_punctuation:bool=True, # exclude punctuation tokens
				exclude_spaces:bool=True # exclude space tokens
				) -> Result: # return a Result object with the frequency table
	""" Report frequent tokens. """
	# TODO - allow choice of index ORTH, LOWER
	# TODO - add in restrict_to and exclude options - latter is for stopword removal
	
	start_time = time.time()
	self.corpus._init_frequency_table()

	columns = ['rank', 'token', 'frequency']
	if show_token_id == True:
		columns = ['rank', 'token_id', 'token', 'frequency']

	count_tokens = self.corpus.token_count
	tokens_descriptor = 'all tokens'
	if exclude_punctuation and exclude_spaces:
		count_tokens = self.corpus.word_token_count
		tokens_descriptor = 'word tokens'
	elif exclude_punctuation:
		count_tokens = self.corpus.word_token_count + len(self.corpus.space_tokens)
		tokens_descriptor = 'word and space tokens'
	elif exclude_spaces:
		count_tokens = self.corpus.word_token_count + len(self.corpus.punct_tokens)
		tokens_descriptor = 'word and punctuation tokens'

	formatted_data = []
	formatted_data.append(f'Report based on {tokens_descriptor}')

	# if a number is passed then normalize by that number
	if type(normalize_by) != int:
		raise ValueError('normalize_by must be an integer, e.g. 1000000 or 10000')
	self.corpus.frequency_table = self.corpus.frequency_table.with_columns((pl.col('frequency') * normalize_by / count_tokens).alias('normalized_frequency'))
	columns.append('normalized_frequency')

	if normalize_by is not None:
		formatted_data.append(f'Normalized Frequency is per {normalize_by:,.0f} tokens')
	formatted_data.append(f'Total tokens: {count_tokens:,.0f}')

	logger.info(f'Frequencies report time: {(time.time() - start_time):.5f} seconds')

	df = self.corpus.frequency_table.sort('frequency', descending=True)
	if exclude_punctuation:
		df = df.filter(pl.col('is_punct') == False)
	if exclude_spaces:
		df = df.filter(pl.col('is_space') == False)
	unique_tokens = len(df)

	df = df.slice((page_current-1)*page_size, page_current*page_size)[columns]
	df = df.drop('rank').with_row_index(name='rank', offset=(page_current-1)*page_size+1)

	formatted_data.append(f'Unique tokens: {unique_tokens:,.0f}')
	formatted_data.append(f'Showing {page_size} rows')
	formatted_data.append(f'Page {page_current} of {count_tokens // page_size + 1}')

	return Result(type = 'frequencies', df=df, title='Frequencies', description='Frequencies of tokens in the corpus', summary_data={}, formatted_data=formatted_data)

