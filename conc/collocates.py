"""Functionality for collocation analysis."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/74_collocates.ipynb.

# %% ../nbs/74_collocates.ipynb 3
from __future__ import annotations
import numpy as np
import time
import polars as pl
from fastcore.basics import patch
import math

# %% auto 0
__all__ = ['Collocates']

# %% ../nbs/74_collocates.ipynb 4
from .corpus import Corpus
from .result import Result
from .core import logger, PAGE_SIZE, set_logger_state

# %% ../nbs/74_collocates.ipynb 9
class Collocates:
	""" Class for collocation analysis reporting. """
	def __init__(self,
			  corpus:Corpus # Corpus instance
			  ): 
		self.corpus = corpus


# %% ../nbs/74_collocates.ipynb 10
@patch
def _shift_zeroes_to_end(self:Collocates,
						arr:np.ndarray # Numpy array of collocate frequencies to process
						):
	""" Move 0 value positions for punctuation and space removal """
	result = np.empty_like(arr)
	for col in range(arr.shape[1]):
		col_data = arr[:, col]
		mask = col_data != 0
		result[:mask.sum(), col] = col_data[mask]
		result[mask.sum():, col] = 0
	return result

# %% ../nbs/74_collocates.ipynb 11
@patch
def _zero_after_value(self:Collocates,
					  arr:np.ndarray, # Numpy array of collocate frequencies to process
					  target: int # Target value to find in the array (e.g., an end-of-file token or a specific collocate frequency)
					  ):
	""" Set values from first occurence of target value to 0 in each column (for processing tokens outside text using eof token) """
	arr = arr.copy()  
	for col in range(arr.shape[1]):
		col_data = arr[:, col]
		idx = np.where(col_data == target)[0]
		if idx.size > 0:
			first_idx = idx[0]
			arr[first_idx:, col] = 0
	return arr

# %% ../nbs/74_collocates.ipynb 13
@patch
def _get_collocates_in_context(self:Collocates,
							   token_positions:np.ndarray, # Numpy array of token positions in the corpus
							   index:str, # Index to use - lower_index, orth_index
							   context_length:int = 5, # Number of context words to consider on each side of the token
							   position_offset:int = 1 # offset to start retrieving context words - -1 for left, positive for right (may be adjusted by sequence_len)
							   ) -> Result:
	""" Get collocates in context for a given token index, operates one side at a time. """

	start_time = time.time()

	if context_length < 1:
		# return empty result
		return np.zeros((0, 0), dtype=np.int32)

	if position_offset < 0:
		position_offset_step = -1
	else:
		position_offset_step = 1
	
	collected = False
	context_tokens_arr = []
	while collected == False:
		new_positions = np.array(token_positions[0] + position_offset, dtype = token_positions[0].dtype)
		context_tokens_arr.append(self.corpus.get_tokens_by_index(index)[new_positions])
		position_offset += position_offset_step
		if len(context_tokens_arr) >= context_length: # cleaning spaces and punctuation and check if need more iterations
			context_tokens = np.array(context_tokens_arr, dtype = token_positions[0].dtype)
			context_tokens = np.where(np.isin(context_tokens, self.corpus.punct_tokens + self.corpus.space_tokens), 0, context_tokens)
			counts = np.count_nonzero(context_tokens, axis=0)
			if np.min(counts) < context_length:
				pass
			else:
				collected = True

	context_tokens = self._shift_zeroes_to_end(context_tokens)
	context_tokens = context_tokens[:context_length, :]

	if self.corpus.EOF_TOKEN in context_tokens:
		context_tokens = self._zero_after_value(context_tokens, self.corpus.EOF_TOKEN)

	logger.info(f"Collocates retrieved in {time.time() - start_time:.2f} seconds.")

	return context_tokens

# %% ../nbs/74_collocates.ipynb 14
@patch
def collocates(self:Collocates, 
				token_str:str, # Token to search for
				collocation_measure:str = 'logdice', # statistical measure to use for collocation calculation: logdice, mutual_information
				context_length:int|None=5, # Window size per side in tokens - use this for setting context lengths on left and right to same value
				context_left:int|None=None, # If context_left or context_right > 0 sets context lengths independently
				context_right:int|None=None, # see context_left
				min_collocate_frequency:int=5, # Minimum count of collocates
				page_size:int=PAGE_SIZE, # number of rows to return, if 0 returns all
				page_current:int=1, # current page, ignored if page_size is 0
				exclude_punctuation:bool=True, # exclude punctuation tokens
				exclude_spaces:bool=True # exclude space tokens
				) -> Result:
	""" Report collocates for a given token string. """

	token_sequence, index_id = self.corpus.tokenize(token_str, simple_indexing=True)

	index_column = 'lower_index'
	frequency_column = 'frequency_lower'
	columns = ['rank', 'token', 'collocate_frequency', 'frequency']

	if collocation_measure not in ['logdice', 'mutual_information']:
		raise ValueError(f'Collocation measure must be one of "logdice" or "mutual_information"')

	start_time = time.time()
	debug = False

	sequence_len = len(token_sequence[0])
	token_positions = self.corpus.get_token_positions(token_sequence, index_id)

	count_tokens = self.corpus.token_count
	tokens_descriptor = 'all tokens'
	total_descriptor = 'Total tokens'
	if exclude_punctuation and exclude_spaces:
		count_tokens = self.corpus.word_token_count
		tokens_descriptor = 'word tokens'
		total_descriptor = 'Total word tokens'
	elif exclude_punctuation:
		space_tokens_count = self.corpus.spaces.select(pl.len()).collect(engine='streaming').item()
		count_tokens = self.corpus.word_token_count + space_tokens_count
		tokens_descriptor = 'word and space tokens'
		total_descriptor = 'Total word and space tokens'
	elif exclude_spaces:
		punct_tokens_count = self.corpus.puncts.select(pl.len()).collect(engine='streaming').item()
		count_tokens = self.corpus.word_token_count + punct_tokens_count
		tokens_descriptor = 'word and punctuation tokens'
		total_descriptor = 'Total word and punctuation tokens'

	formatted_data = []
	formatted_data.append(f'Report based on {tokens_descriptor}')

	# if any of context_length, context_left, context_right are None - set them to 0
	if context_length is None:
		context_length = 0
	if context_left is None:
		context_left = 0
	if context_right is None:
		context_right = 0

	if context_left == 0 and context_right == 0:
		context_left = context_length
		context_right = context_length
	elif (context_left > 0 or context_right > 0) and context_length > 0:
		logger.warning('Context length is ignored if either context_left or context_right is set to a value greater than 0. To remove this warning, set context_length to None or 0.')

	formatted_data.append(f'Context tokens left: {context_left}, context tokens right: {context_right}')

	# getting context tokens
	left_tokens = self._get_collocates_in_context(token_positions=token_positions, index=index_column, context_length=context_left, position_offset=-1)
	right_tokens = self._get_collocates_in_context(token_positions=token_positions, index=index_column, context_length=context_right, position_offset=sequence_len)
	combined_tokens = np.concatenate([left_tokens.flatten(), right_tokens.flatten()])
	combined_tokens = combined_tokens[combined_tokens != 0] # removes punctuation and space placeholder
	token_count_in_context_window = combined_tokens.shape[0]

	# getting frequencies of collocates
	unique_token_ids, counts = np.unique(combined_tokens, return_counts=True)
	#unique_token_ids = unique_token_ids.astype(np.int32)

	df = pl.DataFrame({
		'token_id': unique_token_ids,
		'collocate_frequency': counts
	})

	# adding frequency of collocates in corpus
	df = df.join(self.corpus.vocab.collect().select(['token_id', 'token', frequency_column]), on='token_id', how='left', maintain_order='left')
	df = df.rename({frequency_column: 'frequency'})

	filtering_descriptors = []
	if min_collocate_frequency > 1: # applying min_frequency filter
		df = df.filter(pl.col('collocate_frequency') >= min_collocate_frequency)
		filtering_descriptors.append(f'minimum collocation frequency ({min_collocate_frequency})')
	if len(filtering_descriptors) > 0:
		formatted_data.append(f'Filtered tokens by {(", ".join(filtering_descriptors))}')

	if collocation_measure == 'logdice':
		# calculating collocation measure
		# from old code: logdice = 14 + math.log2((2 * collocate_count) / (node_frequency + loaded_corpora[corpus_name]['frequency_lookup'][collocate]))
		df = df.with_columns(
			(pl.lit(14) + ((2 * pl.col('collocate_frequency')) / (pl.lit(token_positions[0].shape[0]) + pl.col('frequency'))).log(2))
			.alias('logdice')
		)
		columns.append('logdice')

	if collocation_measure == 'mutual_information':
		# from old code: mi = math.log2((loaded_corpora[corpus_name]['token_count'] * collocate_count) / (node_frequency * loaded_corpora[corpus_name]['frequency_lookup'][collocate]))
		df = df.with_columns(
			(pl.lit(count_tokens) * pl.col('collocate_frequency') / (pl.lit(token_positions[0].shape[0]) * pl.col('frequency'))).log(2)
			.alias('mutual_information')
		)
		columns.append('mutual_information')


	# based on calculation for keyness: https://ucrel.lancs.ac.uk/llwizard.html
	# a = collocate frequency in context, is collocate_frequency
	# b = collocate frequency outside context ...
	# TODO - add when syntax to handle case for individual tokens constituting the node, adjust collocate_frequency_outside_context by token_positions[0].shape[0]
	df = df.with_columns(
		(pl.col('frequency') - pl.col('collocate_frequency')).alias('collocate_frequency_outside_context')
	)

	# c = total tokens in context windows, is token_count_in_context_window calculated above
	# d = total tokens outside context windows
	total_tokens_outside_context_window = count_tokens - token_count_in_context_window - (token_positions[0].shape[0] * sequence_len)

	# E1 = c*(a+b) / (c+d) 
	# E2 = d*(a+b) / (c+d)
	# E1 and E2
	df = df.with_columns(
		((pl.lit(token_count_in_context_window) * (pl.col('collocate_frequency') + pl.col('collocate_frequency_outside_context'))) / (pl.lit(token_count_in_context_window) + pl.lit(total_tokens_outside_context_window))).alias('expected_frequency1'),
		((pl.lit(total_tokens_outside_context_window) * (pl.col('collocate_frequency') + pl.col('collocate_frequency_outside_context'))) / (pl.lit(token_count_in_context_window) + pl.lit(total_tokens_outside_context_window))).alias('expected_frequency2'), 
	)

	# G2 = 2*((a*ln (a/E1)) + (b*ln (b/E2))) 
	# components of G2 as term1 and term 2 - (a*ln (a/E1)) (b*ln (b/E2))
	df = df.with_columns([
		pl.when(pl.col('collocate_frequency') > 0)
		.then(pl.col('collocate_frequency') * (pl.col('collocate_frequency') / pl.col('expected_frequency1')).log())
		.otherwise(0)
		.alias('term1'),
		pl.when(pl.col('collocate_frequency_outside_context') > 0)
		.then(pl.col('collocate_frequency_outside_context') * (pl.col('collocate_frequency_outside_context') / pl.col('expected_frequency2')).log())
		.otherwise(0)
		.alias('term2') # 0 if no reference frequency
	])

	# G2
	df = df.with_columns(
		(2 * (pl.col('term1') + pl.col('term2'))).alias('log_likelihood')
	)
	columns.append('log_likelihood')

	# prepare report information and paging ...
	df = df.sort(collocation_measure, descending = True).with_row_index('rank', offset=1)
	unique_collocates = df.select(pl.len()).item()
	formatted_data.append(f'Unique collocates: {unique_collocates:,.0f}')

	if page_size > 0:
		start = (page_current - 1) * page_size
		df = df.slice(start, page_size)

	if page_size != 0 and unique_collocates > page_size:
		formatted_data.append(f'Showing {page_size} rows')
		formatted_data.append(f'Page {page_current} of {unique_collocates // page_size + 1}')

	#formatted_data.append(f'{total_descriptor}: {count_tokens:,.0f}')

	logger.info(f"Collocates calculated in {time.time() - start_time:.2f} seconds.")

	return Result(type = 'collocates', df = df.select(columns), title=f'Collocates of "{token_str}"', description=f'{self.corpus.name}', summary_data={}, formatted_data=formatted_data)
	
