{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# requirements - numpy pandas spacy nltk\n",
    "# dev requirements - nbdev, jupyterlab, memory_profiler\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import polars as pl # alternative to pandas\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "#import dill as pickle\n",
    "import math\n",
    "import json\n",
    "from collections import Counter\n",
    "from distutils.dir_util import copy_tree # for combine_corpora\n",
    "import spacy\n",
    "from spacy.attrs import ORTH, LOWER # remove? - add ENT_TYPE, ENT_IOB\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pickle\n",
    "import gzip\n",
    "import csv\n",
    "import pyarrow.csv\n",
    "from fastcore.basics import patch\n",
    "import logging\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.Config.set_tbl_hide_column_data_types(True)\n",
    "pl.Config.set_tbl_hide_dataframe_shape(True)\n",
    "pl.Config.set_tbl_rows(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "_RE_PUNCT = re.compile(r\"^[^\\s^\\w^\\d]$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# check if can load model - if exception then probably not there - improve message and / or setup\n",
    "try:\n",
    "\tnlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "\tprint('Error loading model en_core_web_sm. You probably need to run python -m spacy download en_core_web_sm to download the model.')\t\n",
    "\traise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "PAGE_SIZE = 20\n",
    "EOF_TOKEN = nlp.vocab[' context-end-of-file-token'].orth # starts with space so eof_token can't match anything from corpus\n",
    "NOT_DOC_TOKEN = -1\n",
    "INDEX_HEADER_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of toy csv:\n",
      "source,text,category,species\n",
      "1.txt,The cat sat on the mat.,feline,cat\n",
      "2.txt,The dog sat on the mat.,canine,dog\n",
      "3.txt,The cat is meowing.,feline,cat\n",
      "4.txt,The dog is barking.,canine,dog\n",
      "5.txt,The cat is climbing a tree.,feline,cat\n",
      "6.txt,The dog is digging a hole.,canine,dog\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_sample_data = True\n",
    "if get_sample_data == True:\n",
    "\tdata = []\n",
    "\tdata.append(['1.txt', 'The cat sat on the mat.', 'feline', 'cat'])\n",
    "\tdata.append(['2.txt', 'The dog sat on the mat.', 'canine', 'dog'])\n",
    "\tdata.append(['3.txt', 'The cat is meowing.', 'feline', 'cat'])\n",
    "\tdata.append(['4.txt', 'The dog is barking.', 'canine', 'dog'])\n",
    "\tdata.append(['5.txt', 'The cat is climbing a tree.', 'feline', 'cat'])\n",
    "\tdata.append(['6.txt', 'The dog is digging a hole.', 'canine', 'dog'])\n",
    "\n",
    "\tos.makedirs('../test-corpora/source/toy', exist_ok=True)\n",
    "\tfor row in data:\n",
    "\t\twith open(f'../test-corpora/source/toy/{row[0]}', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(row[1])\n",
    "\n",
    "\tdf = pl.DataFrame(data, orient='row', schema=(('source', str), ('text', str), ('category', str), ('species', str)))\n",
    "\tdf.write_csv('../test-corpora/source/toy.csv')\n",
    "\tdf.write_csv('../test-corpora/source/toy.csv.gz')\n",
    "\n",
    "\t# print contents of toy corpus\n",
    "\tprint('Contents of toy csv:')\n",
    "\twith open('../test-corpora/source/toy.csv', 'r') as f:\n",
    "\t\tprint(f.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample_data = False\n",
    "if get_sample_data == True:\n",
    "\t# get brown corpus from nltk\n",
    "\timport nltk\n",
    "\t#nltk.download('gutenberg')\n",
    "\t#nltk.download('brown')\n",
    "\t#nltk.download('reuters')\n",
    "\tfrom nltk.corpus import gutenberg\n",
    "\tfrom nltk.corpus import reuters\n",
    "\tfrom nltk.corpus import brown\n",
    "\tlen(reuters.fileids())\n",
    "\n",
    "\t# words just look like this ...\n",
    "\t#Under/in committee/nn rules/nns ,/, it/pps went/vbd automatically/rb to/in a/at subcommittee/nn for/in one/cd week/nn ./.\n",
    "\n",
    "\t# function to clean the annotation /etc \n",
    "\tdef clean_text(text):\n",
    "\t\t# to match words/punc that followed by /tags\n",
    "\t\tpattern = re.compile(r\"(\\S+)(/[^ ]+)\") # match non-space followed by / and non-space\n",
    "\t\treturn pattern.sub(r\"\\1\", text)\n",
    "\n",
    "\tos.makedirs('../test-corpora/source/brown', exist_ok=True)\n",
    "\tfor fileid in brown.fileids():\n",
    "\t\twith open(f'../test-corpora/source/brown/{fileid}.txt', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(clean_text(brown.raw(fileid)))\n",
    "\n",
    "\tos.makedirs('../test-corpora/source/gutenberg', exist_ok=True)\n",
    "\tfor fileid in gutenberg.fileids():\n",
    "\t\twith open(f'../test-corpora/source/gutenberg/{fileid}.txt', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(clean_text(gutenberg.raw(fileid)))\n",
    "\n",
    "\t# save files to ../test-corpora/brown\n",
    "\tos.makedirs('../test-corpora/source/reuters', exist_ok=True)\n",
    "\tfor fileid in reuters.fileids():\n",
    "\t\tfileid_name = fileid.split('/')[1]\n",
    "\t\twith open(f'../test-corpora/source/reuters/{fileid_name}.txt', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(clean_text(reuters.raw(fileid)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sample_data = False\n",
    "if create_sample_data == True:\n",
    "\t# load standard csv library\n",
    "\timport csv\n",
    "\timport gzip\n",
    "\n",
    "\tfor max_i in [10000, 100000, 200000, 500000]:\n",
    "\t\tmax_i_label = int(max_i / 1000)\n",
    "\t\t# create version with just first 100000 rows\n",
    "\t\twith gzip.open('../test-corpora/source/rnz.csv.gz', 'rt') as f:\n",
    "\t\t\treader = csv.DictReader(f)\n",
    "\t\t\twith gzip.open(f'../test-corpora/source/rnz-{max_i_label}k.csv.gz', 'wt') as f_out:\n",
    "\t\t\t\twriter = csv.DictWriter(f_out, fieldnames=reader.fieldnames)\n",
    "\t\t\t\twriter.writeheader()\n",
    "\t\t\t\tfor i, row in enumerate(reader):\n",
    "\t\t\t\t\tif i > max_i - 1:\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\twriter.writerow(row)\n",
    "\t\t\tprint(f'Created file rnz-{max_i_label}k.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for a corpus with methods to load_from_files, load_from_csv\n",
    "# the loaders get a path and return a generator\n",
    "# class is initiated with a corpus name\n",
    "\n",
    "#| export\n",
    "class Corpus:\n",
    "\t\"\"\"Class for a corpus with methods to load texts from files, csv, metadata from csv, json. Class is initiated with a corpus name.\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, name):\n",
    "\t\t# information about corpus\n",
    "\t\tself.name = name\n",
    "\t\tself.corpus_path = None\n",
    "\t\tself.source_path = None\n",
    "\n",
    "\t\t# settings\n",
    "\t\tself.EOF_TOKEN = None\n",
    "\n",
    "\t\t# metadata for corpus\n",
    "\t\tself.document_count = None\n",
    "\t\tself.token_count = None\n",
    "\t\tself.unique_tokens = None\n",
    "\n",
    "\t\t# token data\n",
    "\t\tself.orth_index = None\n",
    "\t\tself.lower_index = None\n",
    "\n",
    "\t\t# lookup mapping doc_id to every token in doc\n",
    "\t\tself.token2doc_index = None\n",
    "\n",
    "\t\t# lookups to get token string or frequency \n",
    "\t\tself.vocab = None\n",
    "\t\tself.frequency_lookup = None\n",
    "\n",
    "\t\t# offsets for each document in token data\n",
    "\t\tself.offsets = None\n",
    "\n",
    "\t\t# metadata for each document\n",
    "\t\tself.metadata = []\n",
    "\n",
    "\t\t# lookups to get spacy tokenizer or internal ids\n",
    "\t\tself.original_to_new = None\n",
    "\t\tself.new_to_original = None\n",
    "\t\t\n",
    "\t\t# temporary data used when processing text, not saved to disk permanently on save\n",
    "\t\tself.frequency_table = None\n",
    "\t\tself.ngram_index = {}\n",
    "\t\tself.results_cache = {}\n",
    "\n",
    "\tdef load_from_files(self, source_path, file_mask='*.txt', metadata_file=None, metadata_file_column = 'file', metadata_columns=[]):\n",
    "\t\t\"\"\"Load texts from files in path with file_mask.\"\"\"\n",
    "\t\t# TOOD - allow input from a compressed folder\n",
    "\n",
    "\t\tif not os.path.isdir(source_path):\n",
    "\t\t\traise FileNotFoundError(f\"Path '{source_path}' is not a directory\")\n",
    "\t\tfiles = glob.glob(os.path.join(source_path, file_mask))\n",
    "\t\tif not files:\n",
    "\t\t\traise FileNotFoundError(f\"No files found in '{source_path}'\")\n",
    "\n",
    "\t\torder = pl.DataFrame({metadata_file_column: [os.path.basename(p) for p in files]})\n",
    "\n",
    "\t\tif metadata_file:\n",
    "\t\t\tif not os.path.isfile(metadata_file):\n",
    "\t\t\t\traise FileNotFoundError(f\"Metadata file '{metadata_file}' not found\")\n",
    "\t\t\ttry:\n",
    "\t\t\t\tmetadata_columns = set([metadata_file_column] + metadata_columns)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# ordering metadata based on order of files so token data and metadata aligned\n",
    "\t\t\t\tmetadata = pl.read_csv(metadata_file).select(metadata_columns)\n",
    "\t\t\t\tself.metadata = order.join(metadata, on=metadata_file_column, how='left')\n",
    "\t\t\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\t\t\traise\n",
    "\t\telse:\n",
    "\t\t\tself.metadata = order\n",
    "\n",
    "\t\tself.source_path = source_path\n",
    "\n",
    "\t\tfor p in files:\n",
    "\t\t\tyield open(p, \"rb\").read().decode(\"utf-8\")\n",
    "\t\n",
    "\tdef load_from_csv(self, source_path, text_column='text', metadata_columns=[]):\n",
    "\t\t\"\"\"Load texts from csv in path with text_column.\"\"\"\n",
    "\n",
    "\t\t# TODO - used polars instead of pandas, but loads full csv into memory - consider streaming this another way or loading in chunks\n",
    "\n",
    "\t\tif not os.path.isfile(source_path):\n",
    "\t\t\traise FileNotFoundError(f\"Path '{source_path}' is not a file\")\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tdf = pl.read_csv(source_path).select([text_column] + metadata_columns)\n",
    "\t\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\t\traise\n",
    "\n",
    "\t\tself.source_path = source_path\n",
    "\t\tself.metadata = df.select(metadata_columns)\n",
    "\n",
    "\t\tfor row in df.iter_rows():\n",
    "\t\t\tyield row[0]\n",
    "\t\t\n",
    "\n",
    "\tdef build(self, iterator, batch_size=1000):\n",
    "\t\tstart_time = time.time()\n",
    "\t\teof_arr = np.array([EOF_TOKEN], dtype=np.uint64)\n",
    "\t\tnot_doc_arr = np.array([NOT_DOC_TOKEN], dtype=np.int16)\n",
    "\t\t# this is added to start and end of index to prevent out of bound issues on searches\n",
    "\t\tindex_header_arr = np.array([EOF_TOKEN] * INDEX_HEADER_LENGTH, dtype=np.uint64)\n",
    "\n",
    "\t\t#corpus['texts'] = texts\n",
    "\t\t#corpus['docs'] = []\n",
    "\t\torth_index = [index_header_arr]\n",
    "\t\tlower_index = [index_header_arr]\n",
    "\t\ttoken2doc_index = [np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32)]\n",
    "\n",
    "\t\toffset = INDEX_HEADER_LENGTH\n",
    "\t\tself.offsets = [] # TODO - check that this is being used  - consider removing\n",
    "\n",
    "\t\tdoc_order = 0\n",
    "\t\tfor doc in nlp.tokenizer.pipe(iterator, batch_size=batch_size): # test varying this TODO\n",
    "\t\t\t#TODO  - as corpus size increases memory requirements will increase - consider buffering orth_index, lower_index, token2doc_index and writing to disk periodically\n",
    "\t\t\torth_index.append(doc.to_array(ORTH))\n",
    "\t\t\torth_index.append(eof_arr)\n",
    "\n",
    "\t\t\tlower_index_tmp = doc.to_array(LOWER)\n",
    "\t\t\tlower_index.append(lower_index_tmp)\n",
    "\t\t\tlower_index.append(eof_arr)\n",
    "\n",
    "\t\t\t#token2doc_index.append(np.full(np.shape(lower_index_tmp),doc_order, dtype=np.int32))\n",
    "\t\t\t# speed up using list\n",
    "\t\t\ttoken2doc_index.append(np.array([doc_order] * len(lower_index_tmp), dtype=np.int32))\n",
    "\t\t\ttoken2doc_index.append(not_doc_arr)\n",
    "\n",
    "\t\t\tself.offsets.append(offset) \n",
    "\t\t\toffset = offset + len(lower_index_tmp) + 1\n",
    "\t\t\tdoc_order += 1\n",
    "\n",
    "\t\torth_index.append(index_header_arr)\n",
    "\t\tlower_index.append(index_header_arr)\n",
    "\t\ttoken2doc_index.append(np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32))\n",
    "\n",
    "\t\tself.create_indices(orth_index, lower_index, token2doc_index)\n",
    "\n",
    "\t\tself.document_count = len(self.offsets)\n",
    "\t\tself.token_count = self.lower_index.shape[0] - self.document_count - len(index_header_arr) - len(index_header_arr) # adjust for text breaks and start and end of index\n",
    "\t\tself.unique_tokens = len(self.frequency_lookup)\n",
    "\t\t# cleanup all tmp arrays\n",
    "\t\tdel orth_index\n",
    "\t\tdel lower_index\n",
    "\t\tdel token2doc_index\n",
    "\t\tlogging.info(f'Build time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\tdef create_indices(self, orth_index, lower_index, token2doc_index):\n",
    "\t\t\"\"\" Takes as input a list of token ids (np.uint64) and reindexes that outputting a lookup and the reindexed token_index. \"\"\"\n",
    "\t\tunique_values, inverse = np.unique(np.concatenate(orth_index + lower_index), return_inverse=True)\n",
    "\t\tnew_values = np.arange(len(unique_values), dtype=np.uint32)\n",
    "\t\tself.original_to_new = dict(zip(unique_values, new_values))\n",
    "\t\tself.new_to_original = dict(zip(new_values, unique_values))\n",
    "\n",
    "\t\t# the order_index and lower_index are first and second half of inverse - so split it\n",
    "\t\tself.orth_index = np.array(np.split(inverse, 2)[0], dtype=np.uint32)\n",
    "\t\tself.lower_index = np.array(np.split(inverse, 2)[1], dtype=np.uint32)\n",
    "\t\tdel inverse\n",
    "\n",
    "\t\tvocab = {k:nlp.vocab.strings[k] for k in unique_values}\n",
    "\t\t#reindexed_vocab = {**{original_to_new[k]:vocab[k] for k in vocab}, **{vocab[k]:original_to_new[k] for k in vocab}}\n",
    "\t\t# speed up reindexed_vocab by using the new_to_original dict\n",
    "\t\t#\n",
    "\t\t# make self.vocab a two-way lookup - one line to join both - so can look up token or token_id\n",
    "\t\tself.vocab = {**{k:vocab[self.new_to_original[k]] for k in new_values}}\n",
    "\t\n",
    "\t\t# del self.vocab[self.original_to_new[EOF_TOKEN]]\n",
    "\t\tself.EOF_TOKEN = self.original_to_new[EOF_TOKEN]\n",
    "\n",
    "\t\tself.frequency_lookup = dict(zip(*np.unique(self.lower_index, return_counts=True)))\n",
    "\t\tdel self.frequency_lookup[self.EOF_TOKEN]\n",
    "\n",
    "\t\tself.token2doc_index = np.concatenate(token2doc_index)\n",
    "\t\n",
    "\tdef info(self, include_memory_usage = False):\n",
    "\t\tresult = []\n",
    "\t\tattributes = ['name', 'corpus_path', 'source_path', 'token_count', 'unique_tokens', 'document_count']\n",
    "\t\tfor attr in attributes:\n",
    "\t\t\tvalue = getattr(self, attr)\n",
    "\t\t\t# if int\n",
    "\t\t\tif isinstance(value, int):\n",
    "\t\t\t\tresult.append(f'{value:,}')\n",
    "\t\t\telse:\n",
    "\t\t\t\tresult.append(str(value))\n",
    "\t\tif include_memory_usage:\n",
    "\t\t\tsize_attributes = ['orth_index', 'lower_index', 'token2doc_index', 'vocab', 'frequency_lookup', 'offsets', 'metadata', 'original_to_new', 'new_to_original', 'results_cache', 'ngram_index', 'frequency_table']\n",
    "\t\t\tfor attr in size_attributes:\n",
    "\t\t\t\tsize = sys.getsizeof(getattr(self, attr))\n",
    "\t\t\t\tattributes.append(attr + ' (MB)')\n",
    "\t\t\t\tresult.append(f'{size/1024/1024:.3f}')\n",
    "\t\treturn str(pl.DataFrame({'Attribute': attributes, 'Value': result}))\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\treturn self.info()\n",
    "\n",
    "\tdef index_name(self, index):\n",
    "\t\t\"\"\"Return name of index.\"\"\"\n",
    "\t\treturn list(spacy.attrs.IDS.keys())[list(spacy.attrs.IDS.values()).index(index)]\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def save(self: Corpus, corpus_path):\n",
    "\tstart_time = time.time()\n",
    "\tif not os.path.isdir(corpus_path):\n",
    "\t\tos.makedirs(corpus_path)\n",
    "\tself.corpus_path = corpus_path\n",
    "\tnp.savez_compressed(os.path.join(corpus_path, 'arrays.npz'), orth_index=self.orth_index, lower_index=self.lower_index, token2doc_index=self.token2doc_index, offsets=self.offsets)\n",
    "\t# save vocab, frequency_lookup, original_to_new, new_to_original, document_count, token_count, unique_tokens, source_path, name, EOF_TOKEN, \n",
    "\twith gzip.open(os.path.join(corpus_path, 'corpus.pkl.gz'), 'wb') as f:\n",
    "\t\t# save only the necessary data\n",
    "\t\tpickle.dump({k: getattr(self, k) for k in ['metadata', 'vocab', 'frequency_lookup', 'original_to_new', 'new_to_original', 'document_count', 'token_count', 'unique_tokens', 'source_path', 'name', 'EOF_TOKEN']}, f)\n",
    "\tlogging.info(f'Save time: {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def compare_saved_to_source(self: Corpus):\n",
    "\t# get size of directory or file at source_path and compare to size of directory at corpus_path\n",
    "\tprint('Source text data:')\n",
    "\tsource_size = 0\n",
    "\tif os.path.isdir(self.source_path):\n",
    "\t\t# loop through files in source_path and get size\n",
    "\t\ti = 0\n",
    "\t\tfor root, dirs, files in os.walk(self.source_path):\n",
    "\t\t\tfor file in files:\n",
    "\t\t\t\ti += 1\n",
    "\t\t\t\tfile_path = os.path.join(root, file)\n",
    "\t\t\t\tsource_size += os.path.getsize(file_path)\n",
    "\t\t\t\tif i < 5:\n",
    "\t\t\t\t\tprint(f'\\t{os.path.basename(file_path)}: {os.path.getsize(file_path)/1024/1024:.3f} MB')\n",
    "\t\t\t\telif i == 5:\n",
    "\t\t\t\t\tprint('\\t...')\n",
    "\telse:\n",
    "\t\t# show file\n",
    "\t\tprint(f'\\t{os.path.basename(self.source_path)}: {os.path.getsize(self.source_path)/1024/1024:.3f} MB')\n",
    "\t\tsource_size = os.path.getsize(self.source_path)\n",
    "\tprint(f'Source total size: {source_size/1024/1024:.3f} MB')\n",
    "\tprint()\n",
    "\n",
    "\t# print files in corpus_path and size of each - nice formatting\n",
    "\tprint('Saved corpus:')\n",
    "\tcorpus_size = 0\n",
    "\tfor root, dirs, files in os.walk(self.corpus_path):\n",
    "\t\tfor file in files:\n",
    "\t\t\tfile_path = os.path.join(root, file)\n",
    "\t\t\tcorpus_size += os.path.getsize(file_path)\n",
    "\t\t\tprint(f'\\t{os.path.basename(file_path)}: {os.path.getsize(file_path)/1024/1024:.3f} MB')\n",
    "\tprint(f'Corpus total size: {corpus_size/1024/1024:.3f} MB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def load(self: Corpus, corpus_path):\n",
    "\tstart_time = time.time()\n",
    "\tif not os.path.isdir(corpus_path):\n",
    "\t\traise FileNotFoundError(f\"Path '{corpus_path}' is not a directory\")\n",
    "\tself.corpus_path = corpus_path\n",
    "\twith gzip.open(os.path.join(corpus_path, 'corpus.pkl.gz'), 'rb') as f:\n",
    "\t\tdata = pickle.load(f)\n",
    "\t\tfor k, v in data.items():\n",
    "\t\t\tsetattr(self, k, v)\n",
    "\tnpz = np.load(os.path.join(corpus_path, 'arrays.npz'))\n",
    "\tself.orth_index = npz['orth_index']\n",
    "\tself.lower_index = npz['lower_index']\n",
    "\tself.token2doc_index = npz['token2doc_index']\n",
    "\tself.offsets = npz['offsets']\n",
    "\tlogging.info(f'Load time: {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:03:38 - INFO - Load time: 0.003 seconds\n",
      "2025-02-13 11:03:38 - INFO - Load time: 0.087 seconds\n",
      "2025-02-13 11:03:38 - INFO - Load time: 0.322 seconds\n",
      "2025-02-13 11:03:39 - INFO - Load time: 0.583 seconds\n",
      "2025-02-13 11:03:40 - INFO - Load time: 1.554 seconds\n"
     ]
    }
   ],
   "source": [
    "rebuild = False\n",
    "\n",
    "toy = Corpus('toy')\n",
    "rnz = Corpus('rnz-10k')\n",
    "rnz100 = Corpus('rnz-100k')\n",
    "rnz200 = Corpus('rnz-200k')\n",
    "rnz500 = Corpus('rnz-500k')\n",
    "\n",
    "if rebuild == True:\n",
    "\ttoy.build(toy.load_from_files('../test-corpora/source/toy'))\n",
    "\trnz.build(rnz.load_from_csv('../test-corpora/source/rnz-10k.csv.gz', text_column='description'))\n",
    "\trnz100.build(rnz100.load_from_csv('../test-corpora/source/rnz-100k.csv.gz', text_column='description'))\n",
    "\trnz200.build(rnz200.load_from_csv('../test-corpora/source/rnz-200k.csv.gz', text_column='description'))\n",
    "\trnz500.build(rnz500.load_from_csv('../test-corpora/source/rnz-500k.csv.gz', text_column='description'))\n",
    "\n",
    "\ttoy.save('../test-corpora/saved/toy')\n",
    "\trnz.save('../test-corpora/saved/rnz-10k')\n",
    "\trnz100.save('../test-corpora/saved/rnz-100k')\n",
    "\trnz200.save('../test-corpora/saved/rnz-200k')\n",
    "\trnz500.save('../test-corpora/saved/rnz-500k')\n",
    "else:\n",
    "\ttoy.load('../test-corpora/saved/toy')\n",
    "\trnz.load('../test-corpora/saved/rnz-10k')\n",
    "\trnz100.load('../test-corpora/saved/rnz-100k')\n",
    "\trnz200.load('../test-corpora/saved/rnz-200k')\n",
    "\trnz500.load('../test-corpora/saved/rnz-500k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────────┬────────────────────────────┐\n",
      "│ Attribute             ┆ Value                      │\n",
      "╞═══════════════════════╪════════════════════════════╡\n",
      "│ name                  ┆ toy                        │\n",
      "│ corpus_path           ┆ ../test-corpora/saved/toy  │\n",
      "│ source_path           ┆ ../test-corpora/source/toy │\n",
      "│ token_count           ┆ 38                         │\n",
      "│ unique_tokens         ┆ 15                         │\n",
      "│ document_count        ┆ 6                          │\n",
      "│ orth_index (MB)       ┆ 0.001                      │\n",
      "│ lower_index (MB)      ┆ 0.001                      │\n",
      "│ token2doc_index (MB)  ┆ 0.001                      │\n",
      "│ vocab (MB)            ┆ 0.001                      │\n",
      "│ frequency_lookup (MB) ┆ 0.001                      │\n",
      "│ offsets (MB)          ┆ 0.000                      │\n",
      "│ metadata (MB)         ┆ 0.000                      │\n",
      "│ original_to_new (MB)  ┆ 0.001                      │\n",
      "│ new_to_original (MB)  ┆ 0.001                      │\n",
      "│ results_cache (MB)    ┆ 0.000                      │\n",
      "│ ngram_index (MB)      ┆ 0.000                      │\n",
      "│ frequency_table (MB)  ┆ 0.000                      │\n",
      "└───────────────────────┴────────────────────────────┘\n",
      "┌───────────────────────┬─────────────────────────────────┐\n",
      "│ Attribute             ┆ Value                           │\n",
      "╞═══════════════════════╪═════════════════════════════════╡\n",
      "│ name                  ┆ rnz-10k                         │\n",
      "│ corpus_path           ┆ ../test-corpora/saved/rnz-10k   │\n",
      "│ source_path           ┆ ../test-corpora/source/rnz-10k… │\n",
      "│ token_count           ┆ 341,671                         │\n",
      "│ unique_tokens         ┆ 21,357                          │\n",
      "│ document_count        ┆ 10,000                          │\n",
      "│ orth_index (MB)       ┆ 1.342                           │\n",
      "│ lower_index (MB)      ┆ 1.342                           │\n",
      "│ token2doc_index (MB)  ┆ 1.342                           │\n",
      "│ vocab (MB)            ┆ 1.250                           │\n",
      "│ frequency_lookup (MB) ┆ 0.563                           │\n",
      "│ offsets (MB)          ┆ 0.076                           │\n",
      "│ metadata (MB)         ┆ 0.000                           │\n",
      "│ original_to_new (MB)  ┆ 1.250                           │\n",
      "│ new_to_original (MB)  ┆ 1.250                           │\n",
      "│ results_cache (MB)    ┆ 0.000                           │\n",
      "│ ngram_index (MB)      ┆ 0.000                           │\n",
      "│ frequency_table (MB)  ┆ 0.000                           │\n",
      "└───────────────────────┴─────────────────────────────────┘\n",
      "┌───────────────────────┬─────────────────────────────────┐\n",
      "│ Attribute             ┆ Value                           │\n",
      "╞═══════════════════════╪═════════════════════════════════╡\n",
      "│ name                  ┆ rnz-100k                        │\n",
      "│ corpus_path           ┆ ../test-corpora/saved/rnz-100k  │\n",
      "│ source_path           ┆ ../test-corpora/source/rnz-100… │\n",
      "│ token_count           ┆ 3,133,652                       │\n",
      "│ unique_tokens         ┆ 53,541                          │\n",
      "│ document_count        ┆ 100,000                         │\n",
      "│ orth_index (MB)       ┆ 12.336                          │\n",
      "│ lower_index (MB)      ┆ 12.336                          │\n",
      "│ token2doc_index (MB)  ┆ 12.336                          │\n",
      "│ vocab (MB)            ┆ 2.500                           │\n",
      "│ frequency_lookup (MB) ┆ 2.500                           │\n",
      "│ offsets (MB)          ┆ 0.763                           │\n",
      "│ metadata (MB)         ┆ 0.000                           │\n",
      "│ original_to_new (MB)  ┆ 2.500                           │\n",
      "│ new_to_original (MB)  ┆ 2.500                           │\n",
      "│ results_cache (MB)    ┆ 0.000                           │\n",
      "│ ngram_index (MB)      ┆ 0.000                           │\n",
      "│ frequency_table (MB)  ┆ 0.000                           │\n",
      "└───────────────────────┴─────────────────────────────────┘\n",
      "┌───────────────────────┬─────────────────────────────────┐\n",
      "│ Attribute             ┆ Value                           │\n",
      "╞═══════════════════════╪═════════════════════════════════╡\n",
      "│ name                  ┆ rnz-200k                        │\n",
      "│ corpus_path           ┆ ../test-corpora/saved/rnz-200k  │\n",
      "│ source_path           ┆ ../test-corpora/source/rnz-200… │\n",
      "│ token_count           ┆ 5,934,801                       │\n",
      "│ unique_tokens         ┆ 76,930                          │\n",
      "│ document_count        ┆ 200,000                         │\n",
      "│ orth_index (MB)       ┆ 23.403                          │\n",
      "│ lower_index (MB)      ┆ 23.403                          │\n",
      "│ token2doc_index (MB)  ┆ 23.403                          │\n",
      "│ vocab (MB)            ┆ 5.000                           │\n",
      "│ frequency_lookup (MB) ┆ 2.500                           │\n",
      "│ offsets (MB)          ┆ 1.526                           │\n",
      "│ metadata (MB)         ┆ 0.000                           │\n",
      "│ original_to_new (MB)  ┆ 5.000                           │\n",
      "│ new_to_original (MB)  ┆ 5.000                           │\n",
      "│ results_cache (MB)    ┆ 0.000                           │\n",
      "│ ngram_index (MB)      ┆ 0.000                           │\n",
      "│ frequency_table (MB)  ┆ 0.000                           │\n",
      "└───────────────────────┴─────────────────────────────────┘\n",
      "┌───────────────────────┬─────────────────────────────────┐\n",
      "│ Attribute             ┆ Value                           │\n",
      "╞═══════════════════════╪═════════════════════════════════╡\n",
      "│ name                  ┆ rnz-500k                        │\n",
      "│ corpus_path           ┆ ../test-corpora/saved/rnz-500k  │\n",
      "│ source_path           ┆ ../test-corpora/source/rnz-500… │\n",
      "│ token_count           ┆ 15,326,015                      │\n",
      "│ unique_tokens         ┆ 120,311                         │\n",
      "│ document_count        ┆ 500,000                         │\n",
      "│ orth_index (MB)       ┆ 60.372                          │\n",
      "│ lower_index (MB)      ┆ 60.372                          │\n",
      "│ token2doc_index (MB)  ┆ 60.372                          │\n",
      "│ vocab (MB)            ┆ 10.000                          │\n",
      "│ frequency_lookup (MB) ┆ 5.000                           │\n",
      "│ offsets (MB)          ┆ 3.815                           │\n",
      "│ metadata (MB)         ┆ 0.000                           │\n",
      "│ original_to_new (MB)  ┆ 10.000                          │\n",
      "│ new_to_original (MB)  ┆ 10.000                          │\n",
      "│ results_cache (MB)    ┆ 0.000                           │\n",
      "│ ngram_index (MB)      ┆ 0.000                           │\n",
      "│ frequency_table (MB)  ┆ 0.000                           │\n",
      "└───────────────────────┴─────────────────────────────────┘\n",
      "Source text data:\n",
      "\trnz-500k.csv.gz: 178.977 MB\n",
      "Source total size: 178.977 MB\n",
      "\n",
      "Saved corpus:\n",
      "\tarrays.npz: 53.184 MB\n",
      "\tcorpus.pkl.gz: 6.053 MB\n",
      "Corpus total size: 59.238 MB\n"
     ]
    }
   ],
   "source": [
    "print(toy.info(include_memory_usage=True))\n",
    "print(rnz.info(include_memory_usage=True))\n",
    "print(rnz100.info(include_memory_usage=True))\n",
    "print(rnz200.info(include_memory_usage=True))\n",
    "print(rnz500.info(include_memory_usage=True))\n",
    "\n",
    "rnz500.compare_saved_to_source()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def create_frequency_table(self: Corpus):\n",
    "\t# note: don't sort this - leave in order of token_id - sorts can be done when required\n",
    "\tstart_time = time.time()\n",
    "\tself.frequency_table = pl.DataFrame({'token_id': list(self.frequency_lookup.keys()), 'frequency': list(self.frequency_lookup.values())}) #'token': list([''] * self.unique_tokens), \n",
    "\tself.frequency_table = self.frequency_table.join(pl.DataFrame({'token_id': list(self.vocab.keys()), 'token': list(self.vocab.values())}), on='token_id', how='left')\n",
    "\tself.frequency_table = self.frequency_table.with_row_index(name='rank', offset=1)\n",
    "\tlogging.info(f'Frequency table created in {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def frequencies(self: Corpus, n=None, show_token_id=False, normalize_by=False, sort_by='frequency'):\n",
    "\tstart_time = time.time()\n",
    "\tif self.frequency_table is None:\n",
    "\t\tself.create_frequency_table()\n",
    "\n",
    "\tcolumns = ['rank', 'token', 'frequency']\n",
    "\tif show_token_id == True:\n",
    "\t\tcolumns = ['rank', 'token_id', 'token', 'frequency']\n",
    "\n",
    "\tif normalize_by != False:\n",
    "\t\t# if a number is passed then normalize by that number\n",
    "\t\tif type(normalize_by) != int:\n",
    "\t\t\traise ValueError('normalize_by must be an integer, e.g. 1000000 or 10000')\n",
    "\t\tself.frequency_table = self.frequency_table.with_columns((pl.col('frequency') * normalize_by / self.token_count).alias('normalized_frequency'))\n",
    "\t\tcolumns.append('normalized_frequency')\n",
    "\n",
    "\t# TODO - add back if needed\n",
    "\t# if sort_by in ['frequency', 'normalized_frequency']:\n",
    "\t# \tself.frequency_table = self.frequency_table.sort(sort_by, descending=True)\n",
    "\t# \tself.frequency_table = self.frequency_table.drop('rank').with_row_index(name='rank', offset=1)\n",
    "\n",
    "\tlogging.info(f'Frequencies report time: {(time.time() - start_time):.5f} seconds')\n",
    "\tif n:\n",
    "\t\treturn self.frequency_table.sort('frequency', descending=True)[columns].head(n).to_pandas().set_index('rank')\n",
    "\telse:\n",
    "\t\treturn self.frequency_table.sort('frequency', descending=True)[columns].to_pandas().set_index('rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:03:51 - INFO - Frequency table created in 0.002 seconds\n",
      "2025-02-13 11:03:51 - INFO - Frequencies report time: 0.00325 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>frequency</th>\n",
       "      <th>normalized_frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>8</td>\n",
       "      <td>210.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>6</td>\n",
       "      <td>157.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>4</td>\n",
       "      <td>105.263158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token  frequency  normalized_frequency\n",
       "rank                                       \n",
       "9      the          8            210.526316\n",
       "14       .          6            157.894737\n",
       "2       is          4            105.263158"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:03:51 - INFO - Frequency table created in 0.022 seconds\n",
      "2025-02-13 11:03:51 - INFO - Frequencies report time: 0.02356 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>frequency</th>\n",
       "      <th>normalized_frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>the</td>\n",
       "      <td>19545</td>\n",
       "      <td>57.204153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14546</th>\n",
       "      <td>.</td>\n",
       "      <td>13966</td>\n",
       "      <td>40.875579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>,</td>\n",
       "      <td>9178</td>\n",
       "      <td>26.862098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  frequency  normalized_frequency\n",
       "rank                                        \n",
       "8543    the      19545             57.204153\n",
       "14546     .      13966             40.875579\n",
       "3047      ,       9178             26.862098"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:03:51 - INFO - Frequency table created in 0.057 seconds\n",
      "2025-02-13 11:03:51 - INFO - Frequencies report time: 0.05805 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>frequency</th>\n",
       "      <th>normalized_frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21551</th>\n",
       "      <td>the</td>\n",
       "      <td>186137</td>\n",
       "      <td>59.399384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36623</th>\n",
       "      <td>.</td>\n",
       "      <td>115595</td>\n",
       "      <td>36.888270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34506</th>\n",
       "      <td>a</td>\n",
       "      <td>76002</td>\n",
       "      <td>24.253491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  frequency  normalized_frequency\n",
       "rank                                        \n",
       "21551   the     186137             59.399384\n",
       "36623     .     115595             36.888270\n",
       "34506     a      76002             24.253491"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:03:51 - INFO - Frequency table created in 0.063 seconds\n",
      "2025-02-13 11:03:51 - INFO - Frequencies report time: 0.06519 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>frequency</th>\n",
       "      <th>normalized_frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30913</th>\n",
       "      <td>the</td>\n",
       "      <td>360131</td>\n",
       "      <td>60.681226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52610</th>\n",
       "      <td>.</td>\n",
       "      <td>227333</td>\n",
       "      <td>38.305075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>of</td>\n",
       "      <td>142599</td>\n",
       "      <td>24.027596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  frequency  normalized_frequency\n",
       "rank                                        \n",
       "30913   the     360131             60.681226\n",
       "52610     .     227333             38.305075\n",
       "3730     of     142599             24.027596"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:03:51 - INFO - Frequency table created in 0.093 seconds\n",
      "2025-02-13 11:03:51 - INFO - Frequencies report time: 0.09483 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>frequency</th>\n",
       "      <th>normalized_frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48480</th>\n",
       "      <td>the</td>\n",
       "      <td>939251</td>\n",
       "      <td>61.284750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82289</th>\n",
       "      <td>.</td>\n",
       "      <td>577156</td>\n",
       "      <td>37.658582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5848</th>\n",
       "      <td>of</td>\n",
       "      <td>378584</td>\n",
       "      <td>24.702051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  frequency  normalized_frequency\n",
       "rank                                        \n",
       "48480   the     939251             61.284750\n",
       "82289     .     577156             37.658582\n",
       "5848     of     378584             24.702051"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(toy.frequencies(n=3, normalize_by=1000))\n",
    "display(rnz.frequencies(n=3, normalize_by=1000))\n",
    "display(rnz100.frequencies(n=3, normalize_by=1000))\n",
    "display(rnz200.frequencies(n=3, normalize_by=1000))\n",
    "display(rnz500.frequencies(n=3, normalize_by=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def token_ids_to_tokens(self: Corpus, token_ids):\n",
    "\tif 'tokens_array' not in self.results_cache:\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.results_cache['tokens_array'] = np.array(list(self.vocab.values()))\n",
    "\t\tlogging.info(f'Create tokens_array in {(time.time() - start_time):.3f} seconds')\n",
    "\treturn self.results_cache['tokens_array'][token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:14:37 - INFO - Create tokens_array in 0.000 seconds\n",
      "2025-02-13 11:14:37 - INFO - Create tokens_array in 0.011 seconds\n",
      "2025-02-13 11:14:37 - INFO - Create tokens_array in 0.023 seconds\n",
      "2025-02-13 11:14:37 - INFO - Create tokens_array in 0.042 seconds\n",
      "2025-02-13 11:14:37 - INFO - Create tokens_array in 0.053 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325 ns ± 19.3 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "623 ns ± 16.6 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "632 ns ± 20.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "622 ns ± 14.1 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "633 ns ± 29.1 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "405 ns ± 11.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "637 ns ± 13 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "643 ns ± 32.3 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "819 ns ± 170 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "662 ns ± 26.3 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "408 ns ± 14.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "710 ns ± 68.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "710 ns ± 50.7 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "674 ns ± 58 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# destroy token_ids_to_tokens cache\n",
    "if 'tokens_array' in toy.results_cache:\n",
    "\tdel toy.results_cache['tokens_array']\n",
    "if 'tokens_array' in rnz.results_cache:\n",
    "\tdel rnz.results_cache['tokens_array']\n",
    "if 'tokens_array' in rnz100.results_cache:\n",
    "\tdel rnz100.results_cache['tokens_array']\n",
    "if 'tokens_array' in rnz200.results_cache:\n",
    "\tdel rnz200.results_cache['tokens_array']\n",
    "if 'tokens_array' in rnz500.results_cache:\n",
    "\tdel rnz500.results_cache['tokens_array']\n",
    "\n",
    "# warm up tokens_array\n",
    "warm_up_array = np.array(range(0,5))\n",
    "toy.token_ids_to_tokens(warm_up_array)\n",
    "rnz.token_ids_to_tokens(warm_up_array)\n",
    "rnz100.token_ids_to_tokens(warm_up_array)\n",
    "rnz200.token_ids_to_tokens(warm_up_array)\n",
    "rnz500.token_ids_to_tokens(warm_up_array)\n",
    "\n",
    "# testing token range at start of array using warm_up_array\n",
    "%timeit toy.token_ids_to_tokens(warm_up_array)\n",
    "%timeit rnz.token_ids_to_tokens(warm_up_array)\n",
    "%timeit rnz100.token_ids_to_tokens(warm_up_array)\n",
    "%timeit rnz200.token_ids_to_tokens(warm_up_array)\n",
    "%timeit rnz500.token_ids_to_tokens(warm_up_array)\n",
    "\n",
    "# testing token range at end of array\n",
    "toy_ids = np.array(range(toy.unique_tokens-1, toy.unique_tokens-6, -1))\n",
    "%timeit toy.token_ids_to_tokens(toy_ids)\n",
    "rnz_ids = np.array(range(rnz.unique_tokens-1, rnz.unique_tokens-6, -1))\n",
    "%timeit rnz.token_ids_to_tokens(rnz_ids)\n",
    "rnz100_ids = np.array(range(rnz100.unique_tokens-1, rnz100.unique_tokens-6, -1))\n",
    "%timeit rnz100.token_ids_to_tokens(rnz100_ids)\n",
    "rnz200_ids = np.array(range(rnz200.unique_tokens-1, rnz200.unique_tokens-6, -1))\n",
    "%timeit rnz200.token_ids_to_tokens(rnz200_ids)\n",
    "rnz500_ids = np.array(range(rnz500.unique_tokens-1, rnz500.unique_tokens-6, -1))\n",
    "%timeit rnz500.token_ids_to_tokens(rnz500_ids)\n",
    "\n",
    "# testing random token ids\n",
    "toy_ids = np.random.randint(0, toy.unique_tokens, 5)\n",
    "%timeit toy.token_ids_to_tokens(toy_ids)\n",
    "rnz_ids = np.random.randint(0, rnz.unique_tokens, 5)\n",
    "%timeit rnz.token_ids_to_tokens(rnz_ids)\n",
    "rnz100_ids = np.random.randint(0, rnz100.unique_tokens, 5)\n",
    "%timeit rnz100.token_ids_to_tokens(rnz100_ids)\n",
    "rnz200_ids = np.random.randint(0, rnz200.unique_tokens, 5)\n",
    "%timeit rnz200.token_ids_to_tokens(rnz200_ids)\n",
    "rnz500_ids = np.random.randint(0, rnz500.unique_tokens, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "# maybe convert to using tokens_array rather than frequency_table\n",
    "def token_to_id(self: Corpus, token):\n",
    "\tif self.frequency_table is None:\n",
    "\t\tself.create_frequency_table()\n",
    "\ttoken = self.frequency_table.filter(pl.col('token') == token)['token_id']\n",
    "\tif token.shape[0] == 0:\n",
    "\t\treturn False\n",
    "\telse:\n",
    "\t\ttoken = token[0]\n",
    "\treturn token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83723"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnz500.token_to_id('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def frequency_of(self: Corpus, token):\n",
    "\tstart_time = time.time()\n",
    "\tif self.frequency_table is None:\n",
    "\t\tself.create_frequency_table()\n",
    "\t\n",
    "\tif type(token) == str:\n",
    "\t\ttoken = self.token_to_id(token)\n",
    "\t\tif token == False:\n",
    "\t\t\treturn 0\n",
    "\n",
    "\tlogging.info(f'Token frequency retrieval time: {(time.time() - start_time):.5f} seconds')\n",
    "\n",
    "\tif token in self.frequency_lookup:\n",
    "\t\treturn int(self.frequency_lookup[token])\n",
    "\telse:\n",
    "\t\treturn 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:16:55 - INFO - Token frequency retrieval time: 0.00116 seconds\n",
      "2025-02-13 11:16:55 - INFO - Token frequency retrieval time: 0.00107 seconds\n",
      "2025-02-13 11:16:55 - INFO - Token frequency retrieval time: 0.00154 seconds\n",
      "2025-02-13 11:16:55 - INFO - Token frequency retrieval time: 0.00126 seconds\n",
      "2025-02-13 11:16:55 - INFO - Token frequency retrieval time: 0.00108 seconds\n",
      "2025-02-13 11:16:55 - INFO - Token frequency retrieval time: 0.00000 seconds\n",
      "2025-02-13 11:16:55 - INFO - Token frequency retrieval time: 0.00000 seconds\n",
      "2025-02-13 11:16:55 - INFO - Token frequency retrieval time: 0.00000 seconds\n",
      "2025-02-13 11:16:55 - INFO - Token frequency retrieval time: 0.00000 seconds\n",
      "2025-02-13 11:16:55 - INFO - Token frequency retrieval time: 0.00000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog 3\n",
      "dog 31\n",
      "dog 202\n",
      "dog 363\n",
      "dog 894\n",
      "dog 10 3\n",
      "dog 13162 31\n",
      "dog 34833 202\n",
      "dog 52073 363\n",
      "dog 83723 894\n"
     ]
    }
   ],
   "source": [
    "token = 'dog'\n",
    "print(token, toy.frequency_of(token))\n",
    "print(token, rnz.frequency_of(token))\n",
    "print(token, rnz100.frequency_of(token))\n",
    "print(token, rnz200.frequency_of(token))\n",
    "print(token, rnz500.frequency_of(token))\n",
    "\n",
    "token_id = toy.token_to_id(token)\n",
    "print(token, token_id, toy.frequency_of(token_id))\n",
    "token_id = rnz.token_to_id(token)\n",
    "print(token, token_id, rnz.frequency_of(token_id))\n",
    "token_id = rnz100.token_to_id(token)\n",
    "print(token, token_id, rnz100.frequency_of(token_id))\n",
    "token_id = rnz200.token_to_id(token)\n",
    "print(token, token_id, rnz200.frequency_of(token_id))\n",
    "token_id = rnz500.token_to_id(token)\n",
    "print(token, token_id, rnz500.frequency_of(token_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def tokenize(self: Corpus, string, return_doc = False, simple_indexing = False): #TODO test speed on this using pipe for one doc\n",
    "\tstart_time = time.time()\n",
    "\tplaceholder_string = 'zzxxzzplaceholderzzxxzz' # so doesn't split tokens\n",
    "\tis_wildcard_search = False\n",
    "\tif simple_indexing == True:\n",
    "\t\tindex_id = LOWER\n",
    "\t\tstrings_to_tokenize = [string.strip()]\n",
    "\telse:\n",
    "\t\traise('only simple_indexing implemented')\n",
    "\t\t# TODO rework\n",
    "\t\t# if '*' in string:\n",
    "\t\t# \tis_wildcard_search = True\n",
    "\t\t# \tstring = string.replace('*',placeholder_string)\n",
    "\t\t# if string.islower() == True:\n",
    "\t\t# \tindex_id = LOWER\n",
    "\t\t# else:\n",
    "\t\t# \tindex_id = ORTH\n",
    "\t\t# if '|' in string:\n",
    "\t\t# \tstrings_to_tokenize = string.split('|')\n",
    "\t\t# else:\n",
    "\t\t# \tstrings_to_tokenize = [string.strip()]\n",
    "\ttoken_sequences = []\n",
    "\tfor doc in nlp.tokenizer.pipe(strings_to_tokenize):\n",
    "\t\ttoken_sequences.append(tuple(doc.to_array(index_id)))\n",
    "\t# if is_wildcard_search == True:\n",
    "\t# \ttmp_token_sequence = []\n",
    "\t# \tsequence_count = 1\n",
    "\t# \tfor token in doc:\n",
    "\t# \t\ttmp_token_sequence.append([])\n",
    "\t# \t\tif placeholder_string in token.text:\n",
    "\t# \t\t\tchunked_string = token.text.split(placeholder_string)\n",
    "\t# \t\t\tif len(chunked_string) > 2 or (len(chunked_string) == 2 and chunked_string[0] != '' and chunked_string[1] != ''):\n",
    "\t# \t\t\t\t# use regex\n",
    "\t# \t\t\t\tapproach = 'regex'\n",
    "\t# \t\t\t\tregex = re.compile('.*'.join(chunked_string))\n",
    "\t# \t\t\telif chunked_string[0] == '':\n",
    "\t# \t\t\t\tapproach = 'endswith'\n",
    "\t# \t\t\telse:\n",
    "\t# \t\t\t\tapproach = 'startswith'\n",
    "\t# \t\t\tfor token_id in loaded_corpora[corpus_name]['frequency_lookup']:\n",
    "\t# \t\t\t\tpossible_word = False\n",
    "\t# \t\t\t\tword = loaded_corpora[corpus_name]['vocab'][token_id]\n",
    "\t# \t\t\t\tif approach == 'regex':\n",
    "\t# \t\t\t\t\tif regex.match(word):\n",
    "\t# \t\t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\telif getattr(word,approach)(''.join(chunked_string)):\n",
    "\t# \t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\tif possible_word != False:\n",
    "\t# \t\t\t\t\ttmp_token_sequence[token.i].append(loaded_corpora[corpus_name]['vocab'][possible_word])\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttmp_token_sequence[token.i].append(token.orth)\n",
    "\t# \t\tsequence_count *= len(tmp_token_sequence[token.i])\n",
    "\t# \trotated_token_sequence = []\n",
    "\t# \ttoken_repeat = sequence_count\n",
    "\t# \tfor pos in range(len(tmp_token_sequence)):\n",
    "\t# \t\trotated_token_sequence.append([])\n",
    "\t# \t\tif len(tmp_token_sequence[pos]) == 1:\n",
    "\t# \t\t\trotated_token_sequence[pos] += sequence_count * [tmp_token_sequence[pos][0]]\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttoken_repeat = token_repeat // len(tmp_token_sequence[pos])\n",
    "\t# \t\t\twhile len(rotated_token_sequence[pos]) < sequence_count:\n",
    "\t# \t\t\t\tfor token in tmp_token_sequence[pos]:\n",
    "\t# \t\t\t\t\trotated_token_sequence[pos] += token_repeat * [token]\n",
    "\t# \ttoken_sequences = list(zip(*rotated_token_sequence))\n",
    "\t# \t#for tokens in tmp_token_sequence:\n",
    "\t# \t#    for token in tokens:\n",
    "\t# covert token_sequences to reindexed tokens using original_to_new\n",
    "\ttoken_sequences = [tuple([self.original_to_new[token] for token in sequence]) for sequence in token_sequences]\n",
    "\tlogging.info(f'Tokenization time: {(time.time() - start_time):.5f} seconds')\n",
    "\tif return_doc == True:\n",
    "\t\treturn token_sequences, index_id, doc\n",
    "\telse:\n",
    "\t\treturn token_sequences, index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:16:57 - INFO - Tokenization time: 0.00026 seconds\n",
      "2025-02-13 11:16:57 - INFO - Tokenization time: 0.00009 seconds\n",
      "2025-02-13 11:16:57 - INFO - Tokenization time: 0.00011 seconds\n",
      "2025-02-13 11:16:57 - INFO - Tokenization time: 0.00009 seconds\n",
      "2025-02-13 11:16:57 - INFO - Tokenization time: 0.00008 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.uint32(10),)] LOWER\n"
     ]
    }
   ],
   "source": [
    "token_str = 'dog'\n",
    "toy_token_sequence, toy_index_id = toy.tokenize(token_str, simple_indexing=True)\n",
    "rnz_token_sequence, rnz_index_id = rnz.tokenize(token_str, simple_indexing=True)\n",
    "rnz100_token_sequence, rnz100_index_id = rnz100.tokenize(token_str, simple_indexing=True)\n",
    "rnz200_token_sequence, rnz200_index_id = rnz200.tokenize(token_str, simple_indexing=True)\n",
    "rnz500_token_sequence, rnz500_index_id = rnz500.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "print(toy_token_sequence, toy.index_name(toy_index_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def get_token_index(self: Corpus, token_sequence, index_id): #TEST - refactor token_sequence\n",
    "\tstart_time = time.time()\n",
    "\tresults = []\n",
    "\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\tvariants_len = len(token_sequence)\n",
    "\n",
    "\tif index_id == ORTH:\n",
    "\t\tindex = 'orth_index'\n",
    "\telse:\n",
    "\t\tindex = 'lower_index'\n",
    "\n",
    "\tif (index, sequence_len) not in self.ngram_index:\n",
    "\t\tslices = [] # TODO adjust so not just lower below - so need a var to pass to this function with whether islower\n",
    "\t\t[slices.append(np.roll(getattr(self, index), shift)) for shift in -np.arange(sequence_len)]\n",
    "\t\tseq = np.vstack(slices).T\n",
    "\t\tself.ngram_index[(index, sequence_len)] = seq\n",
    "\n",
    "\tif variants_len == 1:\n",
    "\t\tresults.append(np.where(np.all(self.ngram_index[(index, sequence_len)] == token_sequence[0], axis=1))[0])\n",
    "\telse:\n",
    "\t\tcondition_list = []\n",
    "\t\tchoice_list = variants_len * [True]\n",
    "\t\tfor seq in token_sequence:\n",
    "\t\t\tcondition_list.append(self.ngram_index[(index, sequence_len)] == seq)\n",
    "\t\tresults.append(np.where(np.all(np.select(condition_list, choice_list),axis=1))[0])\n",
    "\n",
    "\tlogging.info(f'Token indexing ({len(results[0])}) time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:16:59 - INFO - Token indexing (3) time: 0.00048 seconds\n",
      "2025-02-13 11:16:59 - INFO - Token indexing (31) time: 0.00094 seconds\n",
      "2025-02-13 11:16:59 - INFO - Token indexing (202) time: 0.01015 seconds\n",
      "2025-02-13 11:16:59 - INFO - Token indexing (363) time: 0.01740 seconds\n",
      "2025-02-13 11:16:59 - INFO - Token indexing (894) time: 0.15002 seconds\n"
     ]
    }
   ],
   "source": [
    "toy_token_index = toy.get_token_index(toy_token_sequence, toy_index_id)\n",
    "rnz_token_index = rnz.get_token_index(rnz_token_sequence, rnz_index_id)\n",
    "rnz100_token_index = rnz100.get_token_index(rnz100_token_sequence, rnz100_index_id)\n",
    "rnz200_token_index = rnz200.get_token_index(rnz200_token_sequence, rnz200_index_id)\n",
    "rnz500_token_index = rnz500.get_token_index(rnz500_token_sequence, rnz500_index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "\n",
    "# speed up by:\n",
    "# 1. keep as columns\n",
    "# 2. optimize removal to only search relevant columns\n",
    "# 3. return numpy array\n",
    "\n",
    "def get_ngrams(self: Corpus, token_sequence, index_id, token_index, ngram_length = 2, ngram_word_position = 'LEFT'): #TEST refactor token_sequence\n",
    "\tstart_time = time.time()\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\tvariants_len = len(token_sequence)\n",
    "\ttoken_index_len = len(token_index[0])\n",
    "\n",
    "\tif index_id == ORTH:\n",
    "\t\tindex = 'orth_index'\n",
    "\telse:\n",
    "\t\tindex = 'lower_index'\n",
    "\n",
    "\tif ngram_word_position == 'RIGHT':\n",
    "\t\tngram_range = range(-1 * ngram_length + sequence_len, sequence_len)\n",
    "\telif ngram_word_position == 'MIDDLE':\n",
    "\t\tngram_range = range(-1 * ngram_length + sequence_len + 1, sequence_len + 1)\n",
    "\telse:\n",
    "\t\tngram_range = range(0, ngram_length)\n",
    "\n",
    "\tngrams = []\n",
    "\tfor pos in ngram_range:\n",
    "\t\tif variants_len == 1 and pos > -1 and pos < sequence_len:\n",
    "\t\t\t# create numpy array with the same token (token_sequence[pos]) for length of token_index[0]\n",
    "\t\t\tngrams.append(np.full(token_index_len, token_sequence[0][pos]))\n",
    "\t\telse:\n",
    "\t\t\tseq = token_index[0] + pos\n",
    "\t\t\tngrams.append(getattr(self, index)[seq])\n",
    "\n",
    "\tngrams = np.stack(ngrams)\n",
    "\n",
    "\t# get column positions to search for EOF_TOKEN\n",
    "\tcolumns = (np.array(ngram_range)[:, None] != np.arange(sequence_len)).all(axis=1)\n",
    "\n",
    "\tngrams = np.delete(ngrams, np.where(ngrams[columns] == self.EOF_TOKEN)[1], axis=1)\n",
    "\tlogging.info(f'Ngrams ({ngrams.shape[1]}) retrieval time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:17:05 - INFO - Ngrams (3) retrieval time: 0.00025 seconds\n",
      "2025-02-13 11:17:05 - INFO - Ngrams (31) retrieval time: 0.00017 seconds\n",
      "2025-02-13 11:17:05 - INFO - Ngrams (202) retrieval time: 0.00024 seconds\n",
      "2025-02-13 11:17:05 - INFO - Ngrams (363) retrieval time: 0.00031 seconds\n",
      "2025-02-13 11:17:05 - INFO - Ngrams (894) retrieval time: 0.00257 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 10 10]\n",
      " [11  1  1]]\n"
     ]
    }
   ],
   "source": [
    "result = toy.get_ngrams(toy_token_sequence, toy_index_id, toy_token_index, ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "print(result)\n",
    "result = rnz.get_ngrams(rnz_token_sequence, rnz_index_id, rnz_token_index, ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "result = rnz100.get_ngrams(rnz100_token_sequence, rnz100_index_id, rnz100_token_index, ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "result = rnz200.get_ngrams(rnz200_token_sequence, rnz200_index_id, rnz200_token_index, ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "result = rnz500.get_ngrams(rnz500_token_sequence, rnz500_index_id, rnz500_token_index, ngram_length = 2, ngram_word_position = 'LEFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def ngrams(self: Corpus, token_str, ngram_length = 2, ngram_word_position = 'LEFT', page_size = PAGE_SIZE, page_current = 0, pretty = True):\n",
    "\ttoken_sequence, index_id = self.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tcache_id = tuple(['ngram'] + list(token_sequence) + [ngram_length, ngram_word_position])\n",
    "\n",
    "\tif cache_id in self.results_cache:\n",
    "\t\tlogging.info('Using cached ngrams results')\n",
    "\t\tngrams_report = self.results_cache[cache_id]\n",
    "\telse:\n",
    "\t\ttoken_index = self.get_token_index(token_sequence, index_id)\n",
    "\t\t\n",
    "\t\tif len(token_index[0]) == 0:\n",
    "\t\t\tlogging.info('No tokens found')\n",
    "\t\t\treturn None, 0\n",
    "\n",
    "\t\tlogging.info('Generating ngrams results')\n",
    "\t\tngrams = self.get_ngrams(token_sequence, index_id, token_index, ngram_length = ngram_length, ngram_word_position = ngram_word_position)\n",
    "\t\tschema = [f'token_{i+1}' for i in range(ngram_length)]\n",
    "\t\tngrams_report = pl.DataFrame(ngrams.T, schema=schema).to_struct(name = 'ngram_token_ids').value_counts(sort=True).rename({\"count\": \"frequency\"})\n",
    "\t\tngrams_report = ngrams_report.with_row_index(name='rank', offset=1)\n",
    "\t\tself.results_cache[cache_id] = ngrams_report\n",
    "\n",
    "\ttotal_count = len(ngrams_report)\n",
    "\n",
    "\tresultset_start = page_size*page_current\n",
    "\tresultset_end = page_size*(page_current+1)\n",
    "\n",
    "\n",
    "\t# get specific chunk of report into pandas based on resultset_start:\n",
    "\tngrams_report_page = ngrams_report.slice(resultset_start, page_size).unnest('ngram_token_ids')\n",
    "\ttoken_strs = []\n",
    "\tfor i in range(ngram_length):\n",
    "\t\ttoken_strs.append(self.token_ids_to_tokens(ngrams_report_page[f'token_{i+1}'].to_numpy()))\n",
    "\ttoken_strs = np.array(token_strs)\n",
    "\tngram_text = [' '.join(column) for column in token_strs.T]\n",
    "\tngrams_report_page = ngrams_report_page.with_columns(pl.Series(name=\"ngram\", values=ngram_text))\n",
    "\tngrams_report_page = ngrams_report_page.to_pandas().set_index('rank')\n",
    "\n",
    "\t# sort column display - add normalized_frequency and rank, optional ngram_token_ids\n",
    "\tlogging.info(f'Ngrams report time: {(time.time() - start_time):.5f} seconds')\n",
    "\tif pretty == True:\n",
    "\t\treturn ngrams_report_page[['ngram', 'frequency']], total_count\n",
    "\telse:\n",
    "\t\treturn ngrams_report_page, total_count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:17:10 - INFO - Tokenization time: 0.00014 seconds\n",
      "2025-02-13 11:17:10 - INFO - Token indexing (3) time: 0.00008 seconds\n",
      "2025-02-13 11:17:10 - INFO - Generating ngrams results\n",
      "2025-02-13 11:17:10 - INFO - Ngrams (3) retrieval time: 0.00062 seconds\n",
      "2025-02-13 11:17:10 - INFO - Create tokens_array in 0.000 seconds\n",
      "2025-02-13 11:17:10 - INFO - Ngrams report time: 0.01070 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog sat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ngram  frequency\n",
       "rank                    \n",
       "1      dog is          2\n",
       "2     dog sat          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:17:10 - INFO - Tokenization time: 0.00013 seconds\n",
      "2025-02-13 11:17:10 - INFO - Token indexing (31) time: 0.00098 seconds\n",
      "2025-02-13 11:17:10 - INFO - Generating ngrams results\n",
      "2025-02-13 11:17:10 - INFO - Ngrams (31) retrieval time: 0.00025 seconds\n",
      "2025-02-13 11:17:10 - INFO - Create tokens_array in 0.015 seconds\n",
      "2025-02-13 11:17:10 - INFO - Ngrams report time: 0.02486 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog to</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog attack</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dog died</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dog \"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dog while</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dog had</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dog as</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dog owners</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dog can</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dog gets</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dog walking</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dog -</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dog and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dog rescue</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dog when</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dog '</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dog that</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dog registers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dog who</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ngram  frequency\n",
       "rank                          \n",
       "1            dog to          4\n",
       "2             dog .          2\n",
       "3        dog attack          2\n",
       "4          dog died          2\n",
       "5             dog \"          2\n",
       "6         dog while          1\n",
       "7           dog had          1\n",
       "8            dog as          1\n",
       "9        dog owners          1\n",
       "10          dog can          1\n",
       "11         dog gets          1\n",
       "12      dog walking          1\n",
       "13            dog -          1\n",
       "14          dog and          1\n",
       "15       dog rescue          1\n",
       "16         dog when          1\n",
       "17            dog '          1\n",
       "18         dog that          1\n",
       "19    dog registers          1\n",
       "20          dog who          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:17:10 - INFO - Tokenization time: 0.00013 seconds\n",
      "2025-02-13 11:17:10 - INFO - Token indexing (202) time: 0.00730 seconds\n",
      "2025-02-13 11:17:10 - INFO - Generating ngrams results\n",
      "2025-02-13 11:17:10 - INFO - Ngrams (202) retrieval time: 0.00042 seconds\n",
      "2025-02-13 11:17:10 - INFO - Create tokens_array in 0.037 seconds\n",
      "2025-02-13 11:17:10 - INFO - Ngrams report time: 0.05472 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog .</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog owners</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog in</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dog attack</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dog trial</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dog to</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dog died</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dog -</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dog that</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dog control</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dog has</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dog was</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dog walking</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dog \"</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dog 's</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dog fighting</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dog river</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dog population</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dog ,</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dog registration</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ngram  frequency\n",
       "rank                             \n",
       "1                dog .         19\n",
       "2           dog owners         14\n",
       "3               dog in         10\n",
       "4           dog attack          8\n",
       "5            dog trial          7\n",
       "6               dog to          4\n",
       "7             dog died          4\n",
       "8                dog -          4\n",
       "9             dog that          4\n",
       "10         dog control          4\n",
       "11             dog has          4\n",
       "12             dog was          4\n",
       "13         dog walking          3\n",
       "14               dog \"          3\n",
       "15              dog 's          3\n",
       "16        dog fighting          3\n",
       "17           dog river          3\n",
       "18      dog population          3\n",
       "19               dog ,          3\n",
       "20    dog registration          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:17:10 - INFO - Tokenization time: 0.00010 seconds\n",
      "2025-02-13 11:17:10 - INFO - Token indexing (363) time: 0.00511 seconds\n",
      "2025-02-13 11:17:10 - INFO - Generating ngrams results\n",
      "2025-02-13 11:17:10 - INFO - Ngrams (363) retrieval time: 0.00023 seconds\n",
      "2025-02-13 11:17:10 - INFO - Create tokens_array in 0.044 seconds\n",
      "2025-02-13 11:17:10 - INFO - Ngrams report time: 0.05525 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog .</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog owners</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog in</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dog control</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dog attack</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dog trial</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dog ,</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dog and</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dog has</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dog to</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dog -</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dog that</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dog was</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dog 's</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dog fighting</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dog from</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dog had</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dog died</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dog \"</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dog walking</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ngram  frequency\n",
       "rank                         \n",
       "1            dog .         38\n",
       "2       dog owners         16\n",
       "3           dog in         14\n",
       "4      dog control         14\n",
       "5       dog attack         11\n",
       "6        dog trial          9\n",
       "7            dog ,          9\n",
       "8          dog and          8\n",
       "9          dog has          8\n",
       "10          dog to          7\n",
       "11           dog -          6\n",
       "12        dog that          6\n",
       "13         dog was          6\n",
       "14          dog 's          5\n",
       "15    dog fighting          5\n",
       "16        dog from          5\n",
       "17         dog had          4\n",
       "18        dog died          4\n",
       "19           dog \"          4\n",
       "20     dog walking          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:17:10 - INFO - Tokenization time: 0.00008 seconds\n",
      "2025-02-13 11:17:10 - INFO - Token indexing (894) time: 0.01575 seconds\n",
      "2025-02-13 11:17:10 - INFO - Generating ngrams results\n",
      "2025-02-13 11:17:10 - INFO - Ngrams (894) retrieval time: 0.00098 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:17:10 - INFO - Create tokens_array in 0.103 seconds\n",
      "2025-02-13 11:17:10 - INFO - Ngrams report time: 0.12732 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog .</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog ,</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog owners</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dog in</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dog attack</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dog has</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dog -</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dog that</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dog control</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dog and</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dog was</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dog to</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dog 's</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dog attacks</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dog trial</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dog on</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dog population</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dog group</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dog from</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dog handler</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ngram  frequency\n",
       "rank                           \n",
       "1              dog .         63\n",
       "2              dog ,         30\n",
       "3         dog owners         29\n",
       "4             dog in         29\n",
       "5         dog attack         27\n",
       "6            dog has         23\n",
       "7              dog -         22\n",
       "8           dog that         22\n",
       "9        dog control         22\n",
       "10           dog and         19\n",
       "11           dog was         18\n",
       "12            dog to         17\n",
       "13            dog 's         17\n",
       "14       dog attacks         15\n",
       "15         dog trial         12\n",
       "16            dog on         12\n",
       "17    dog population         12\n",
       "18         dog group         11\n",
       "19          dog from         10\n",
       "20       dog handler         10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "pretty = True\n",
    "\n",
    "toy.results_cache = {}\n",
    "toy_ngrams, toy_total_count = toy.ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', pretty = pretty)\n",
    "display(toy_ngrams)\n",
    "print(toy_total_count)\n",
    "\n",
    "rnz.results_cache = {}\n",
    "rnz_ngrams, rnz_total_count = rnz.ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', pretty = pretty)\n",
    "display(rnz_ngrams)\n",
    "print(rnz_total_count)\n",
    "\n",
    "rnz100.results_cache = {}\n",
    "rnz100_ngrams, rnz100_total_count = rnz100.ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', pretty = pretty)\n",
    "display(rnz100_ngrams)\n",
    "print(rnz100_total_count)\n",
    "\n",
    "rnz200.results_cache = {}\n",
    "rnz200_ngrams, rnz200_total_count = rnz200.ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', pretty = pretty)\n",
    "display(rnz200_ngrams)\n",
    "print(rnz200_total_count)\n",
    "\n",
    "rnz500.results_cache = {}\n",
    "rnz500_ngrams, rnz500_total_count = rnz500.ngrams(token_str, ngram_length = 2, ngram_word_position = 'LEFT', pretty = pretty)\n",
    "display(rnz500_ngrams)\n",
    "print(rnz500_total_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the cache using a query with lots of results. Note: this has been optimised to remove reliance on np.unique with_counts for 10x faster retrieval of large result set. Caching speeds this up further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:19:04 - INFO - Create tokens_array in 0.054 seconds\n",
      "2025-02-13 11:19:04 - INFO - Tokenization time: 0.00009 seconds\n",
      "2025-02-13 11:19:04 - INFO - Token indexing (939251) time: 0.03140 seconds\n",
      "2025-02-13 11:19:04 - INFO - Generating ngrams results\n",
      "2025-02-13 11:19:04 - INFO - Ngrams (939244) retrieval time: 0.02559 seconds\n",
      "2025-02-13 11:19:04 - INFO - Ngrams report time: 0.12569 seconds\n",
      "2025-02-13 11:19:04 - INFO - Tokenization time: 0.00009 seconds\n",
      "2025-02-13 11:19:04 - INFO - Using cached ngrams results\n",
      "2025-02-13 11:19:04 - INFO - Ngrams report time: 0.00436 seconds\n",
      "2025-02-13 11:19:04 - INFO - Tokenization time: 0.00007 seconds\n",
      "2025-02-13 11:19:04 - INFO - Using cached ngrams results\n",
      "2025-02-13 11:19:04 - INFO - Ngrams report time: 0.00259 seconds\n"
     ]
    }
   ],
   "source": [
    "rnz500.results_cache = {}\n",
    "# warm up tokens_array cache\n",
    "result = rnz500.token_ids_to_tokens([1, 2, 3])\n",
    "\n",
    "rnz500_ngrams, rnz500_total_count = rnz500.ngrams('the', ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "rnz500_ngrams, rnz500_total_count = rnz500.ngrams('the', ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "rnz500_ngrams, rnz500_total_count = rnz500.ngrams('the', ngram_length = 2, ngram_word_position = 'LEFT', page_current=10)\n",
    "\n",
    "#rnz500.results_cache = {}\n",
    "# # warm up tokens_array cache\n",
    "# result = rnz500.token_ids_to_tokens([1, 2, 3])\n",
    "# %lprun -f Corpus.ngrams rnz500.ngrams('the', ngram_length = 2, ngram_word_position = 'LEFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def get_concordance(self: Corpus, token_sequence, token_index, context_words = 5, index_id = None): #TEST refactor token_sequence\n",
    "\tstart_time = time.time()\n",
    "\t# TODO could build the concordance_range part when actually doing the display - i.e. all we really need is the concordance + sort columns\n",
    "\n",
    "\tif index_id == LOWER:\n",
    "\t\tindex = 'lower_index'\n",
    "\telse:\n",
    "\t\tindex = 'orth_index'\n",
    "\n",
    "\tconcordance = []\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\tconcordance_range = range(-1 * context_words, context_words + sequence_len)\n",
    "\n",
    "\t#concordance.append(token_index[0])\n",
    "\n",
    "\tfor pos in concordance_range:\n",
    "\t\tseq = list(token_index[0]+pos)\n",
    "\t\tconcordance.append(getattr(self, index)[seq])\n",
    "\n",
    "\tconcordance = np.vstack(concordance).T\n",
    "\t#concordance = pd.DataFrame(data=concordance, index=token_index[0], columns=concordance_range)\n",
    "\tlogging.info(f'Concordance results ({concordance.shape[0]}) retrieval time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn list(concordance_range), concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:19:08 - INFO - Concordance results (3) retrieval time: 0.00020 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_columns [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
      "concordance (token ids)\n",
      " [[16 16 16 16  3 10 11  7  9 12 14]\n",
      " [ 9 12 14 16  3 10  1  8 13 15 14]\n",
      " [13  4 14 16  3 10  1  2 14 16  3]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "positional_columns, concordance = toy.get_concordance(toy_token_sequence, toy_token_index, context_words = 5)\n",
    "print('positional_columns', positional_columns)\n",
    "print('concordance (token ids)\\n', concordance)\n",
    "print(len(concordance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:19:11 - INFO - Concordance results (3) retrieval time: 0.00017 seconds\n",
      "2025-02-13 11:19:11 - INFO - Concordance results (31) retrieval time: 0.00014 seconds\n",
      "2025-02-13 11:19:11 - INFO - Concordance results (202) retrieval time: 0.00033 seconds\n",
      "2025-02-13 11:19:11 - INFO - Concordance results (363) retrieval time: 0.00051 seconds\n",
      "2025-02-13 11:19:11 - INFO - Concordance results (894) retrieval time: 0.00268 seconds\n"
     ]
    }
   ],
   "source": [
    "result = toy.get_concordance(toy_token_sequence, toy_token_index, context_words = 5)\n",
    "result = rnz.get_concordance(rnz_token_sequence, rnz_token_index, context_words = 5)\n",
    "result = rnz100.get_concordance(rnz100_token_sequence, rnz100_token_index, context_words = 5)\n",
    "result = rnz200.get_concordance(rnz200_token_sequence, rnz200_token_index, context_words = 5)\n",
    "result = rnz500.get_concordance(rnz500_token_sequence, rnz500_token_index, context_words = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def get_concordance_revised(self: Corpus, token_sequence, token_index, context_words = 5, index_id = None): #TEST refactor token_sequence\n",
    "\tstart_time = time.time()\n",
    "\t# TODO could build the concordance_range part when actually doing the display - i.e. all we really need is the concordance + sort columns\n",
    "\n",
    "\tif index_id == LOWER:\n",
    "\t\tindex = 'lower_index'\n",
    "\telse:\n",
    "\t\tindex = 'orth_index'\n",
    "\n",
    "\tconcordance = []\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\tconcordance_range = range(-1 * context_words, context_words + sequence_len)\n",
    "\n",
    "\t#concordance.append(token_index[0])\n",
    "\n",
    "\tfor pos in concordance_range:\n",
    "\t\tseq = list(token_index[0]+pos)\n",
    "\t\tconcordance.append(getattr(self, index)[seq])\n",
    "\n",
    "\t#concordance = np.vstack(concordance).T\n",
    "\tconcordance = np.stack(concordance)\n",
    "\n",
    "\tlogging.info(f'Concordance results ({concordance.shape[0]}) retrieval time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn list(concordance_range), concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:19:14 - INFO - Concordance results (11) retrieval time: 0.00020 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_columns [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
      "concordance (token ids)\n",
      " [[16  9 13]\n",
      " [16 12  4]\n",
      " [16 14 14]\n",
      " [16 16 16]\n",
      " [ 3  3  3]\n",
      " [10 10 10]\n",
      " [11  1  1]\n",
      " [ 7  8  2]\n",
      " [ 9 13 14]\n",
      " [12 15 16]\n",
      " [14 14  3]]\n",
      "11\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Corpus' object has no attribute 'get_concordance_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[712], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcordance (token ids)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, concordance)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(concordance))\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrnz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_concordance_columns\u001b[49m(rnz_token_sequence, rnz_token_index, context_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m rnz100\u001b[38;5;241m.\u001b[39mget_concordance_columns(rnz100_token_sequence, rnz100_token_index, context_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m rnz200\u001b[38;5;241m.\u001b[39mget_concordance_columns(rnz200_token_sequence, rnz200_token_index, context_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Corpus' object has no attribute 'get_concordance_columns'"
     ]
    }
   ],
   "source": [
    "positional_columns, concordance = toy.get_concordance_revised(toy_token_sequence, toy_token_index, context_words = 5)\n",
    "print('positional_columns', positional_columns)\n",
    "print('concordance (token ids)\\n', concordance)\n",
    "print(len(concordance))\n",
    "result = rnz.get_concordance_columns(rnz_token_sequence, rnz_token_index, context_words = 5)\n",
    "result = rnz100.get_concordance_columns(rnz100_token_sequence, rnz100_token_index, context_words = 5)\n",
    "result = rnz200.get_concordance_columns(rnz200_token_sequence, rnz200_token_index, context_words = 5)\n",
    "result = rnz500.get_concordance_columns(rnz500_token_sequence, rnz500_token_index, context_words = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def concordance(self: Corpus, token_str, context_words = 5, order='1R2R3R', page_size=PAGE_SIZE, page_current=0): #TEST refactor token_sequence\n",
    "\t#  TODO: shift to polars dataframes\n",
    "\ttoken_sequence, index_id = self.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\n",
    "\tcache_id = tuple(['concordance'] + list(token_sequence) + [context_words, order])\n",
    "\tif cache_id in self.results_cache:\n",
    "\t\tlogging.info('Using cached concordance results')\n",
    "\t\tpositional_columns = self.results_cache[cache_id][0]\n",
    "\t\tconcordance = self.results_cache[cache_id][1]\n",
    "\t\ttotal_count = self.results_cache[cache_id][2]\n",
    "\t\ttotal_docs = self.results_cache[cache_id][3]\n",
    "\telse:\n",
    "\t\ttoken_index = self.get_token_index(token_sequence, index_id)\n",
    "\n",
    "\t\tif len(token_index[0]) == 0:\n",
    "\t\t\tlogging.info('No tokens found')\n",
    "\t\t\treturn None, 0, False, False\n",
    "\n",
    "\t\tlogging.info('Generating concordance results')\n",
    "\t\tpositional_columns, concordance = self.get_concordance(token_sequence, token_index, context_words = context_words)\n",
    "\t\t# get doc from token_index positions in token2doc_index\n",
    "\n",
    "\t\tif order == '1L2L3L':\n",
    "\t\t\tsort_columns = [-1,-2,-3]\n",
    "\t\telif order == '3L2L1L':\n",
    "\t\t\tsort_columns = [-3,-2,-1]\n",
    "\t\telif order == '2L1L1R':\n",
    "\t\t\tsort_columns = [-2,-1,sequence_len + 1 - 1]\n",
    "\t\telif order == '1L1R2R':\n",
    "\t\t\tsort_columns = [-1,sequence_len + 1 - 1,sequence_len + 2 - 1]\n",
    "\t\telse:\n",
    "\t\t\tsort_columns = [sequence_len + 1 - 1,sequence_len + 2 - 1,sequence_len + 3 - 1]\n",
    "\n",
    "\t\tsort_columns_data = {}\n",
    "\t\tfor x in range(len(sort_columns)):\n",
    "\t\t\tsort_columns_data[x] = concordance[:,[positional_columns.index(sort_columns[x])]].astype(np.str_)\n",
    "\t\t\tfor y in range(len(sort_columns_data[x])):\n",
    "\t\t\t\t# TODO - this was updated to use .item() for numpy compatibility, but the x*y iteration is very slow - apply a method that avoids the loop\n",
    "\t\t\t\tsort_columns_data[x][y] = self.vocab[int(sort_columns_data[x][y].item())] \n",
    "\n",
    "\t\tconcordance = pd.DataFrame(data=concordance, index=token_index[0], columns=positional_columns)\n",
    "\t\tconcordance['sort0'] = sort_columns_data[0]\n",
    "\t\tconcordance['sort1'] = sort_columns_data[1]\n",
    "\t\tconcordance['sort2'] = sort_columns_data[2]\n",
    "\n",
    "\t\tconcordance['document_id'] = ''\n",
    "\t\tconcordance['left'] = ''\n",
    "\t\tconcordance['keyword'] = ''\n",
    "\t\tconcordance['right'] = ''\n",
    "\n",
    "\t\tconcordance = concordance.sort_values(['sort0','sort1','sort1'], ascending=[True, True, True])\n",
    "\n",
    "\t\ttotal_count = len(concordance)\n",
    "\t\ttotal_docs = 0\n",
    "\t\ttotal_docs = len(np.unique(self.token2doc_index[list(token_index[0])]))\n",
    "\n",
    "\t\tself.results_cache[cache_id] = [positional_columns, concordance, total_count, total_docs]\n",
    "\n",
    "\tresultset_start = page_size*page_current\n",
    "\tresultset_end = page_size*(page_current+1)\n",
    "\n",
    "\toffsets_arr = np.array(self.offsets,dtype=np.uint64)\n",
    "\n",
    "\tfor concordance_index, concordance_row in concordance.iloc[resultset_start:resultset_end].iterrows():\n",
    "\t\tdocument_id = np.searchsorted(offsets_arr,concordance_index, side = 'right') - 1\n",
    "\t\t# TODO fix with metadata\n",
    "\t\tconcordance.at[concordance_index, 'document_id'] = document_id#str(document_id) + '(' + str(concordance_index - int(offsets_arr[document_id])) + ', ' + str(sequence_len) + ')' #'tmp'#loaded_corpora[corpus_name]['texts']['id'][text_index] + ':' + str(concordance_index - int(offsets_arr[text_index])) + ':' + str(sequence_len)\n",
    "\n",
    "\t\tconcordance_left = []\n",
    "\t\tconcordance_right = []\n",
    "\t\tconcordance_keyword = []\n",
    "\t\tfor pos in positional_columns:\n",
    "\t\t\ttoken_id = concordance.at[concordance_index,pos]\n",
    "\t\t\ttoken = self.vocab[token_id]\n",
    "\t\t\tif pos < 0:\n",
    "\t\t\t\tconcordance_left.append(token)\n",
    "\t\t\t\tif token_id == self.EOF_TOKEN:\n",
    "\t\t\t\t\tconcordance_left = []\n",
    "\t\t\telif pos == 0 or pos < sequence_len:\n",
    "\t\t\t\tconcordance_keyword.append(token)\n",
    "\t\t\telse:\n",
    "\t\t\t\tif token_id == self.EOF_TOKEN:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tconcordance_right.append(token)\n",
    "\n",
    "\t\tconcordance.at[concordance_index, 'left'] = ' '.join(concordance_left)\n",
    "\t\tconcordance.at[concordance_index, 'keyword'] = ' '.join(concordance_keyword)\n",
    "\t\tconcordance.at[concordance_index, 'right'] = ' '.join(concordance_right)\n",
    "\n",
    "\tlogging.info(f'Concordance report time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn concordance.iloc[resultset_start:resultset_end].sort_values(['sort0','sort1','sort1'], ascending=[True, True, True]), total_count, total_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:19:16 - INFO - Tokenization time: 0.00010 seconds\n",
      "2025-02-13 11:19:16 - INFO - Token indexing (31) time: 0.00091 seconds\n",
      "2025-02-13 11:19:16 - INFO - Generating concordance results\n",
      "2025-02-13 11:19:16 - INFO - Concordance results (31) retrieval time: 0.00019 seconds\n",
      "2025-02-13 11:19:16 - INFO - Concordance report time: 0.02063 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>left</th>\n",
       "      <th>keyword</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12649</th>\n",
       "      <td>383</td>\n",
       "      <td></td>\n",
       "      <td>Dog</td>\n",
       "      <td>owners in Hawke 's Bay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140175</th>\n",
       "      <td>3657</td>\n",
       "      <td>'s KiwiBuild scheme a \"</td>\n",
       "      <td>dog</td>\n",
       "      <td>\" and questions how some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140682</th>\n",
       "      <td>3665</td>\n",
       "      <td>the KiwiBuild scheme a \"</td>\n",
       "      <td>dog</td>\n",
       "      <td>\" and says houses are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79815</th>\n",
       "      <td>2161</td>\n",
       "      <td>,   Keanu Reeves '</td>\n",
       "      <td>dog</td>\n",
       "      <td>- loving hitman is up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222459</th>\n",
       "      <td>6126</td>\n",
       "      <td>Theatre 's season of '</td>\n",
       "      <td>Dog</td>\n",
       "      <td>' .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145153</th>\n",
       "      <td>3755</td>\n",
       "      <td>face much higher fines .</td>\n",
       "      <td>Dog</td>\n",
       "      <td>and Lemon Guide editor Clive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97261</th>\n",
       "      <td>2541</td>\n",
       "      <td>soon became a Red Cross</td>\n",
       "      <td>dog</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154580</th>\n",
       "      <td>4020</td>\n",
       "      <td>Documentary and the Pound Hound</td>\n",
       "      <td>dog</td>\n",
       "      <td>rescue show .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22844</th>\n",
       "      <td>658</td>\n",
       "      <td>Owning a</td>\n",
       "      <td>dog</td>\n",
       "      <td>can turn back the hands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8435</th>\n",
       "      <td>265</td>\n",
       "      <td>being set upon by a</td>\n",
       "      <td>dog</td>\n",
       "      <td>as its handler urges the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350277</th>\n",
       "      <td>9974</td>\n",
       "      <td>How about a musical about</td>\n",
       "      <td>dog</td>\n",
       "      <td>food or plastic wrap ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33825</th>\n",
       "      <td>960</td>\n",
       "      <td>answers ; welcoming an adopted</td>\n",
       "      <td>dog</td>\n",
       "      <td>to your family ; the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281456</th>\n",
       "      <td>7971</td>\n",
       "      <td>access to councils ' dangerous</td>\n",
       "      <td>dog</td>\n",
       "      <td>registers so it can protect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254861</th>\n",
       "      <td>7158</td>\n",
       "      <td>council staff over a dangerous</td>\n",
       "      <td>dog</td>\n",
       "      <td>that 's about to be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>207</td>\n",
       "      <td>police officer save teen from</td>\n",
       "      <td>dog</td>\n",
       "      <td>attack .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56876</th>\n",
       "      <td>1590</td>\n",
       "      <td>'d be bereft if his</td>\n",
       "      <td>dog</td>\n",
       "      <td>died and he 'd definitely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57062</th>\n",
       "      <td>1592</td>\n",
       "      <td>'d be bereft if his</td>\n",
       "      <td>dog</td>\n",
       "      <td>died and he 'd definitely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43469</th>\n",
       "      <td>1191</td>\n",
       "      <td>The first known</td>\n",
       "      <td>dog</td>\n",
       "      <td>to serve this country in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97197</th>\n",
       "      <td>2541</td>\n",
       "      <td>this country 's first known</td>\n",
       "      <td>dog</td>\n",
       "      <td>to serve in war .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97715</th>\n",
       "      <td>2549</td>\n",
       "      <td>The first known</td>\n",
       "      <td>dog</td>\n",
       "      <td>to serve New Zealand in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       document_id                             left keyword  \\\n",
       "12649          383                                      Dog   \n",
       "140175        3657          's KiwiBuild scheme a \"     dog   \n",
       "140682        3665         the KiwiBuild scheme a \"     dog   \n",
       "79815         2161               ,   Keanu Reeves '     dog   \n",
       "222459        6126           Theatre 's season of '     Dog   \n",
       "145153        3755         face much higher fines .     Dog   \n",
       "97261         2541          soon became a Red Cross     dog   \n",
       "154580        4020  Documentary and the Pound Hound     dog   \n",
       "22844          658                         Owning a     dog   \n",
       "8435           265              being set upon by a     dog   \n",
       "350277        9974        How about a musical about     dog   \n",
       "33825          960   answers ; welcoming an adopted     dog   \n",
       "281456        7971   access to councils ' dangerous     dog   \n",
       "254861        7158   council staff over a dangerous     dog   \n",
       "6526           207    police officer save teen from     dog   \n",
       "56876         1590              'd be bereft if his     dog   \n",
       "57062         1592              'd be bereft if his     dog   \n",
       "43469         1191                  The first known     dog   \n",
       "97197         2541      this country 's first known     dog   \n",
       "97715         2549                  The first known     dog   \n",
       "\n",
       "                               right  \n",
       "12649         owners in Hawke 's Bay  \n",
       "140175      \" and questions how some  \n",
       "140682         \" and says houses are  \n",
       "79815          - loving hitman is up  \n",
       "222459                           ' .  \n",
       "145153  and Lemon Guide editor Clive  \n",
       "97261                              .  \n",
       "154580                 rescue show .  \n",
       "22844        can turn back the hands  \n",
       "8435        as its handler urges the  \n",
       "350277        food or plastic wrap ?  \n",
       "33825           to your family ; the  \n",
       "281456   registers so it can protect  \n",
       "254861           that 's about to be  \n",
       "6526                        attack .  \n",
       "56876      died and he 'd definitely  \n",
       "57062      died and he 'd definitely  \n",
       "43469       to serve this country in  \n",
       "97197              to serve in war .  \n",
       "97715        to serve New Zealand in  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 rows 27 docs\n"
     ]
    }
   ],
   "source": [
    "rnz.results_cache = {}\n",
    "concordance_report, total_count, total_docs = rnz.concordance(token_str, context_words = 5, order='1L2L3L')\n",
    "if total_count > 0:\n",
    "\tdisplay(concordance_report[['document_id', 'left','keyword','right']])\n",
    "\tprint(total_count,'rows', total_docs, 'docs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:19:17 - INFO - Tokenization time: 0.00011 seconds\n",
      "2025-02-13 11:19:17 - INFO - Token indexing (3) time: 0.00008 seconds\n",
      "2025-02-13 11:19:17 - INFO - Generating concordance results\n",
      "2025-02-13 11:19:17 - INFO - Concordance results (3) retrieval time: 0.00030 seconds\n",
      "2025-02-13 11:19:17 - INFO - Concordance report time: 0.01295 seconds\n",
      "2025-02-13 11:19:17 - INFO - Tokenization time: 0.00012 seconds\n",
      "2025-02-13 11:19:17 - INFO - Token indexing (31) time: 0.00066 seconds\n",
      "2025-02-13 11:19:17 - INFO - Generating concordance results\n",
      "2025-02-13 11:19:17 - INFO - Concordance results (31) retrieval time: 0.00014 seconds\n",
      "2025-02-13 11:19:17 - INFO - Concordance report time: 0.02051 seconds\n",
      "2025-02-13 11:19:17 - INFO - Tokenization time: 0.00008 seconds\n",
      "2025-02-13 11:19:17 - INFO - Token indexing (202) time: 0.00441 seconds\n",
      "2025-02-13 11:19:17 - INFO - Generating concordance results\n",
      "2025-02-13 11:19:17 - INFO - Concordance results (202) retrieval time: 0.00040 seconds\n",
      "2025-02-13 11:19:17 - INFO - Concordance report time: 0.02472 seconds\n",
      "2025-02-13 11:19:17 - INFO - Tokenization time: 0.00012 seconds\n",
      "2025-02-13 11:19:17 - INFO - Token indexing (363) time: 0.00722 seconds\n",
      "2025-02-13 11:19:17 - INFO - Generating concordance results\n",
      "2025-02-13 11:19:17 - INFO - Concordance results (363) retrieval time: 0.00070 seconds\n",
      "2025-02-13 11:19:17 - INFO - Concordance report time: 0.03506 seconds\n",
      "2025-02-13 11:19:17 - INFO - Tokenization time: 0.00013 seconds\n",
      "2025-02-13 11:19:17 - INFO - Token indexing (894) time: 0.01680 seconds\n",
      "2025-02-13 11:19:17 - INFO - Generating concordance results\n",
      "2025-02-13 11:19:17 - INFO - Concordance results (894) retrieval time: 0.00202 seconds\n",
      "2025-02-13 11:19:17 - INFO - Concordance report time: 0.05946 seconds\n"
     ]
    }
   ],
   "source": [
    "show_concordances = False\n",
    "toy.results_cache = {}\n",
    "concordance_report, total_count, total_docs = toy.concordance(token_str, context_words = 5, order='1L2L3L')\n",
    "if total_count > 0 and show_concordances == True:\n",
    "\tdisplay(concordance_report[['document_id', 'left','keyword','right']])\n",
    "\tprint(total_count,'rows')\n",
    "\n",
    "rnz.results_cache = {}\n",
    "concordance_report, total_count, total_docs = rnz.concordance(token_str, context_words = 5, order='1L2L3L')\n",
    "if total_count > 0 and show_concordances == True:\n",
    "\tdisplay(concordance_report[['document_id', 'left','keyword','right']])\n",
    "\tprint(total_count,'rows')\n",
    "\n",
    "rnz100.results_cache = {}\n",
    "concordance_report, total_count, total_docs = rnz100.concordance(token_str, context_words = 5, order='1L2L3L')\n",
    "if total_count > 0 and show_concordances == True:\n",
    "\tdisplay(concordance_report[['document_id', 'left','keyword','right']])\n",
    "\tprint(total_count,'rows')\n",
    "\n",
    "rnz200.results_cache = {}\n",
    "concordance_report, total_count, total_docs = rnz200.concordance(token_str, context_words = 5, order='1L2L3L')\n",
    "if total_count > 0 and show_concordances == True:\n",
    "\tdisplay(concordance_report[['document_id', 'left','keyword','right']])\n",
    "\tprint(total_count,'rows')\n",
    "\n",
    "rnz500.results_cache = {}\n",
    "concordance_report, total_count, total_docs = rnz500.concordance(token_str, context_words = 5, order='1L2L3L')\n",
    "if total_count > 0 and show_concordances == True:\n",
    "\tdisplay(concordance_report[['document_id', 'left','keyword','right']])\n",
    "\tprint(total_count,'rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the cache using a query with lots of results ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:19:52 - INFO - Tokenization time: 0.00012 seconds\n",
      "2025-02-13 11:19:52 - INFO - Token indexing (939251) time: 0.03488 seconds\n",
      "2025-02-13 11:19:52 - INFO - Generating concordance results\n",
      "2025-02-13 11:19:53 - INFO - Concordance results (939251) retrieval time: 1.29022 seconds\n",
      "2025-02-13 11:19:58 - INFO - Concordance report time: 5.75223 seconds\n",
      "2025-02-13 11:19:58 - INFO - Tokenization time: 0.00008 seconds\n",
      "2025-02-13 11:19:58 - INFO - Using cached concordance results\n",
      "2025-02-13 11:19:58 - INFO - Concordance report time: 0.02924 seconds\n",
      "2025-02-13 11:19:58 - INFO - Tokenization time: 0.00008 seconds\n",
      "2025-02-13 11:19:58 - INFO - Using cached concordance results\n",
      "2025-02-13 11:19:58 - INFO - Concordance report time: 0.04076 seconds\n"
     ]
    }
   ],
   "source": [
    "rnz500.results_cache = {}\n",
    "concordance_report, total_count, total_docs = rnz500.concordance('the', context_words = 5, order='1L2L3L')\n",
    "concordance_report, total_count, total_docs = rnz500.concordance('the', context_words = 5, order='1L2L3L')\n",
    "concordance_report, total_count, total_docs = rnz500.concordance('the', context_words = 5, order='1L2L3L', page_current=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test load from file\n",
    "# toy = Corpus('toy')\n",
    "# for i, text in enumerate(toy.load_from_files('../test-corpora/toy/', file_mask='*.txt', metadata_file='../test-corpora/toy.csv', metadata_lookup_column = 'source', metadata_columns=['source', 'category'])):\n",
    "# #for i, text in enumerate(toy.load_from_files('../test-corpora/toy/')):\n",
    "# \tprint(text)\n",
    "# display(toy.metadata.to_pandas())\n",
    "# print()\n",
    "# # load from csv\n",
    "# toy = Corpus('toy')\n",
    "# for i, text in enumerate(toy.load_from_csv('../test-corpora/toy.csv', metadata_columns=['source', 'category'])):\n",
    "# \tprint(text)\n",
    "# display(toy.metadata.to_pandas())\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = Corpus('rnz')\n",
    "# corpus.build(corpus.load_from_csv('../test-corpora/rnz-200k.csv.gz', text_column='description'))\n",
    "# corpus = Corpus('rnz')\n",
    "# %lprun -f corpus.build  corpus.build(corpus.load_from_csv('../test-corpora/rnz-10k.csv.gz', text_column='description'))\n",
    "# corpus = Corpus('reuters')\n",
    "# corpus.build(corpus.load_from_files('../test-corpora/reuters'))\n",
    "#%lprun -f corpus.build  corpus.build(corpus.load_from_csv('../test-corpora/rnz-200k.csv.gz', text_column='description'))\n",
    "#%memit corpus.build(corpus.load_from_files('../test-corpora/toy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development notes\n",
    "\n",
    "Investigate storage efficiency of exploiting repetition using run-length encoding (probably just more efficient to compress)  \n",
    "* https://stackoverflow.com/questions/3098907/how-to-efficiently-store-a-matrix-with-highly-redundant-values  \n",
    "* https://pypi.org/project/python-rle/  \n",
    "* https://gist.github.com/nvictus/66627b580c13068589957d6ab0919e66  \n",
    "* https://trimsh.org/trimesh.voxel.runlength.html#trimesh.voxel.runlength.rle_gatherer_1d  \n",
    "* https://github.com/mikedh/trimesh/blob/63e35a5652c9525a6a8070271c2bac8f4d13105b/trimesh/voxel/runlength.py#L345  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing optimisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing speed of CSV reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 332 ms, total: 2.3 s\n",
      "Wall time: 1.27 s\n",
      "peak memory: 2733.00 MiB, increment: 224.18 MiB\n",
      "CPU times: user 2.03 s, sys: 405 ms, total: 2.44 s\n",
      "Wall time: 1.68 s\n",
      "peak memory: 3523.83 MiB, increment: 495.31 MiB\n"
     ]
    }
   ],
   "source": [
    "file = '../test-corpora/rnz-200k.csv.gz'\n",
    "# standard csv library\n",
    "def load_gzip_csv(file):\n",
    "\twith gzip.open(file, 'rt') as f:\n",
    "\t\treader = csv.DictReader(f)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tyield row\n",
    "\n",
    "# dtype set to string for all columns to suppress error\n",
    "def load_gzip_pandas(file):\n",
    "\tdf = pd.read_csv(file, dtype=str)\n",
    "\tfor i, row in df.iterrows():\n",
    "\t\tyield row\n",
    "\tdel df\n",
    "\n",
    "# dtype set to string for all columns to suppress error\n",
    "def load_gzip_pyarrow(file):\n",
    "\ttable = pyarrow.csv.read_csv(file, parse_options=pyarrow.csv.ParseOptions(invalid_row_handler=skip_comment))\n",
    "\tfor row in table:\n",
    "\t\tyield row\n",
    "\tdel table\n",
    "\n",
    "# polars\n",
    "def load_gzip_polars(file):\n",
    "\tdf = pl.read_csv(file)\n",
    "\tfor row in df.iter_rows():\n",
    "\t\tyield row\n",
    "\tdel df\n",
    "\n",
    "def skip_comment(row):\n",
    "\treturn 'skip'\n",
    "\n",
    "# %time _ = [row for row in load_gzip_csv()]\n",
    "# %memit _ = [row for row in load_gzip_csv()]\n",
    "\n",
    "# %time _ = [row for row in load_gzip_pandas()]\n",
    "# %memit _ = [row for row in load_gzip_pandas()]\n",
    "\n",
    "%time _ = [row for row in load_gzip_pyarrow(file)]\n",
    "%memit _ = [row for row in load_gzip_pyarrow(file)]\n",
    "\n",
    "%time _ = [row for row in load_gzip_polars(file)]\n",
    "%memit _ = [row for row in load_gzip_polars(file)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the frequency lookup using Counter or numpy. \n",
    "\n",
    "Result: numpy, at least 10x speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating frequency lookup using Counter\n",
      "795 ms ± 89.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "Generating frequency lookup using numpy\n",
      "76.4 ms ± 12.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print('Generating frequency lookup using Counter')\n",
    "%timeit Counter(corpus.lower_index)\n",
    "print()\n",
    "print('Generating frequency lookup using numpy')\n",
    "%timeit dict(zip(*np.unique(corpus.lower_index, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the speed of Spacy's vocab stringstore vs a dict.  \n",
    "\n",
    "Result: dict, at least 2x speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving from Spacy vocab stringstore:\n",
      "148 ns ± 12.3 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n",
      "265 ns ± 11.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "\n",
      "Retrieving from dict:\n",
      "65.5 ns ± 10.3 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n",
      "59.5 ns ± 8.85 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print('Retrieving from Spacy vocab stringstore:')\n",
    "%timeit nlp.vocab.strings[7425985699627899538]\n",
    "%timeit nlp.vocab.strings['the']\n",
    "vocab = {**{k:nlp.vocab.strings[k] for k in nlp.vocab.strings}, **{nlp.vocab.strings[k]:k for k in nlp.vocab.strings}}\n",
    "\n",
    "print()\n",
    "\n",
    "print('Retrieving from dict:')\n",
    "%timeit the = vocab['the']\n",
    "%timeit the = vocab[7425985699627899538]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Spacy uses unint64 for string hashes. Working with these hashes is fast, but requires more memory/storage.  Vocabulary sizes of a corpus are much smaller than the max allowed by uint64. \n",
    "\n",
    "Solution: Reindex the hashes to create an internal id to substantially reduce memory/storage. Preserve lookup to Spacy hashes if needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisations to ngram retrieval - slightly slower in micro-seconds for rare token sequences, but faster in milliseconds for common ones and format is better for using columnar approach to speed up main ngrams function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 14:54:32 - INFO - Tokenization time: 0.00008 seconds\n",
      "2025-02-12 14:54:33 - INFO - Token indexing (939251) time: 0.02845 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.5 ms ± 5.72 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 14:54:38 - INFO - Tokenization time: 0.00010 seconds\n",
      "2025-02-12 14:54:38 - INFO - Token indexing (24369) time: 0.13101 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.1 ms ± 2.16 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "1.61 ms ± 222 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 14:55:01 - INFO - Tokenization time: 0.00010 seconds\n",
      "2025-02-12 14:55:01 - INFO - Token indexing (894) time: 0.01946 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13 ms ± 142 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "61.1 μs ± 2.64 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 14:55:13 - INFO - Tokenization time: 0.00009 seconds\n",
      "2025-02-12 14:55:13 - INFO - Token indexing (72) time: 0.02603 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.1 μs ± 3.64 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "29.2 μs ± 2.01 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "54.3 μs ± 2.59 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def get_ngrams_old(Corpus, token_sequence, index_id, token_index, ngram_length = 2, ngram_word_position = 'LEFT', log=False): #TEST refactor token_sequence\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\n",
    "\tif index_id == ORTH:\n",
    "\t\tindex = 'orth_index'\n",
    "\telse:\n",
    "\t\tindex = 'lower_index'\n",
    "\n",
    "\tif ngram_word_position == 'RIGHT':\n",
    "\t\tngram_range = range(-1 * ngram_length + sequence_len, sequence_len)\n",
    "\telif ngram_word_position == 'MIDDLE':\n",
    "\t\tngram_range = range(-1 * ngram_length + sequence_len + 1, sequence_len + 1)\n",
    "\telse:\n",
    "\t\tngram_range = range(0, ngram_length)\n",
    "\n",
    "\tngrams = []\n",
    "\tfor pos in ngram_range:\n",
    "\t\tseq = token_index[0] + pos\n",
    "\t\tngrams.append(getattr(Corpus, index)[seq])\n",
    "\n",
    "\tngrams = np.vstack(ngrams).T\n",
    "\n",
    "\t#remove any ngrams that include EOF_TOKEN (this is a marker for text separation)\n",
    "\tngrams = np.delete(ngrams, np.where(ngrams == Corpus.EOF_TOKEN)[0], axis=0)\n",
    "\treturn ngrams\n",
    "\n",
    "# speed up by:\n",
    "# 1. keep as columns\n",
    "# 2. optimize removal to only search relevant columns\n",
    "# 3. return numpy array\n",
    "def get_ngrams_new(Corpus, token_sequence, index_id, token_index, ngram_length = 2, ngram_word_position = 'LEFT', log=False): #TEST refactor token_sequence\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\tvariants_len = len(token_sequence)\n",
    "\ttoken_index_len = len(token_index[0])\n",
    "\n",
    "\tif index_id == ORTH:\n",
    "\t\tindex = 'orth_index'\n",
    "\telse:\n",
    "\t\tindex = 'lower_index'\n",
    "\n",
    "\tif ngram_word_position == 'RIGHT':\n",
    "\t\tngram_range = range(-1 * ngram_length + sequence_len, sequence_len)\n",
    "\telif ngram_word_position == 'MIDDLE':\n",
    "\t\tngram_range = range(-1 * ngram_length + sequence_len + 1, sequence_len + 1)\n",
    "\telse:\n",
    "\t\tngram_range = range(0, ngram_length)\n",
    "\n",
    "\tngrams = []\n",
    "\tif log: logging.info(ngram_range)\n",
    "\tfor pos in ngram_range:\n",
    "\t\tif variants_len == 1 and pos > -1 and pos < sequence_len:\n",
    "\t\t\t# create numpy array with the same token (token_sequence[pos]) for length of token_index[0]\n",
    "\t\t\tngrams.append(np.full(token_index_len, token_sequence[0][pos]))\n",
    "\t\telse:\n",
    "\t\t\tseq = token_index[0] + pos\n",
    "\t\t\tngrams.append(getattr(Corpus, index)[seq])\n",
    "\n",
    "\tngrams = np.stack(ngrams)\n",
    "\n",
    "\t# get column positions to search for EOF_TOKEN\n",
    "\tcolumns = (np.array(ngram_range)[:, None] != np.arange(sequence_len)).all(axis=1)\n",
    "\tif log: logging.info(columns)\n",
    "\tngrams = np.delete(ngrams, np.where(ngrams[columns] == Corpus.EOF_TOKEN)[1], axis=1)\n",
    "\treturn ngrams\n",
    "\n",
    "run_time_test = True\n",
    "\n",
    "the_token_str = 'the'\n",
    "the_token_sequence, the_index_id = rnz500.tokenize(the_token_str, simple_indexing=True)\n",
    "rnz500_the_token_index = rnz500.get_token_index(the_token_sequence, the_index_id)\n",
    "# print(the_token_sequence)\n",
    "# print(the_index_id)\n",
    "\n",
    "if run_time_test:\n",
    "\t%timeit result = get_ngrams_old(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "else:\n",
    "\t%time result = get_ngrams_old(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 2, ngram_word_position = 'LEFT', log=False)\n",
    "# print(result.shape)\n",
    "# print(result[:5])\n",
    "if run_time_test:\n",
    "\t%timeit result = get_ngrams_new(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "else:\n",
    "\t%time result = get_ngrams_new(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 2, ngram_word_position = 'LEFT', log=False)\n",
    "\n",
    "\n",
    "the_token_str = 'the government'\n",
    "the_token_sequence, the_index_id = rnz500.tokenize(the_token_str, simple_indexing=True)\n",
    "rnz500_the_token_index = rnz500.get_token_index(the_token_sequence, the_index_id)\n",
    "# print(the_token_sequence)\n",
    "# print(the_index_id)\n",
    "\n",
    "if run_time_test:\n",
    "\t%timeit result = get_ngrams_old(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 3, ngram_word_position = 'LEFT')\n",
    "else:\n",
    "\t%time result = get_ngrams_old(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 3, ngram_word_position = 'LEFT', log=False)\n",
    "# print(result.shape)\n",
    "# print(result[:5])\n",
    "if run_time_test:\n",
    "\t%timeit result = get_ngrams_new(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 3, ngram_word_position = 'LEFT')\n",
    "else:\n",
    "\t%time result = get_ngrams_new(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 3, ngram_word_position = 'LEFT', log=False)\n",
    "\n",
    "\n",
    "the_token_str = 'dog'\n",
    "the_token_sequence, the_index_id = rnz500.tokenize(the_token_str, simple_indexing=True)\n",
    "rnz500_the_token_index = rnz500.get_token_index(the_token_sequence, the_index_id)\n",
    "# print(the_token_sequence)\n",
    "# print(the_index_id)\n",
    "\n",
    "if run_time_test:\n",
    "\t%timeit result = get_ngrams_old(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "else:\n",
    "\t%time result = get_ngrams_old(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 2, ngram_word_position = 'LEFT', log=False)\n",
    "# print(result.shape)\n",
    "# print(result[:5])\n",
    "if run_time_test:\n",
    "\t%timeit result = get_ngrams_new(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "else:\n",
    "\t%time result = get_ngrams_new(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 2, ngram_word_position = 'LEFT', log=False)\n",
    "\n",
    "the_token_str = 'the dog'\n",
    "the_token_sequence, the_index_id = rnz500.tokenize(the_token_str, simple_indexing=True)\n",
    "rnz500_the_token_index = rnz500.get_token_index(the_token_sequence, the_index_id)\n",
    "# print(the_token_sequence)\n",
    "# print(the_index_id)\n",
    "\n",
    "if run_time_test:\n",
    "\t%timeit result = get_ngrams_old(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 3, ngram_word_position = 'LEFT')\n",
    "else:\n",
    "\t%time result = get_ngrams_old(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 3, ngram_word_position = 'LEFT', log=False)\n",
    "# print(result.shape)\n",
    "# print(result[:5])\n",
    "if run_time_test:\n",
    "\t%timeit result = get_ngrams_new(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 3, ngram_word_position = 'LEFT')\n",
    "else:\n",
    "\t%time result = get_ngrams_new(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 3, ngram_word_position = 'LEFT', log=False)\n",
    "\n",
    "\n",
    "\n",
    "# print(result.shape)\n",
    "# # preview first 5 entries in each column\n",
    "# print(result)\n",
    "\n",
    "#%lprun -f get_ngrams_new result = get_ngrams_new(rnz500, the_token_sequence, the_index_id, rnz500_the_token_index, ngram_length = 3, ngram_word_position = 'LEFT', log=False)\n",
    "\n",
    "# columns = [0, 1]\n",
    "# # # find rnz500.EOF_TOKEN in result[columns]\n",
    "# # print(result[columns])\n",
    "# #print(rnz500.EOF_TOKEN)\n",
    "# eof_index = np.where(result[columns] == rnz500.EOF_TOKEN)\n",
    "# print(len(eof_index[0]))\n",
    "# print(len(eof_index[1]))\n",
    "# print(939251 -787900 )\n",
    "# print(eof_index)\n",
    "# print(result[0][125])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed up to ngram reporting produces 10x speed for large results. Making better use of column formats and faster unique using polars struc counts. Probably worth testing frequent and infrequent tokens. Note: caching turned off in these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 15:29:52 - INFO - Tokenization time: 0.00009 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 15:29:52 - INFO - Token indexing (939251) time: 0.03751 seconds\n",
      "2025-02-12 15:29:52 - INFO - Generating ngrams results\n",
      "2025-02-12 15:29:53 - INFO - Ngrams report time: 1.79165 seconds\n",
      "2025-02-12 15:29:53 - INFO - Tokenization time: 0.00008 seconds\n",
      "2025-02-12 15:29:53 - INFO - Token indexing (939251) time: 0.02977 seconds\n",
      "2025-02-12 15:29:53 - INFO - Generating ngrams results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 s, sys: 20.3 ms, total: 1.8 s\n",
      "Wall time: 1.79 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 15:29:55 - INFO - Ngrams report time: 1.72422 seconds\n",
      "2025-02-12 15:29:55 - INFO - Tokenization time: 0.00007 seconds\n",
      "2025-02-12 15:29:55 - INFO - Token indexing (939251) time: 0.03745 seconds\n",
      "2025-02-12 15:29:55 - INFO - Generating ngrams results\n",
      "2025-02-12 15:29:55 - INFO - Ngrams report time: 0.14558 seconds\n",
      "2025-02-12 15:29:55 - INFO - Tokenization time: 0.00008 seconds\n",
      "2025-02-12 15:29:55 - INFO - Token indexing (939251) time: 0.03609 seconds\n",
      "2025-02-12 15:29:55 - INFO - Generating ngrams results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ngram  frequency\n",
      "rank                           \n",
      "1     the government      24369\n",
      "2            the new      22748\n",
      "3         the latest      17959\n",
      "4        the country      15560\n",
      "5          the first      12916\n",
      "CPU times: user 151 ms, sys: 23 μs, total: 151 ms\n",
      "Wall time: 148 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 15:29:55 - INFO - Ngrams report time: 0.18574 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ngram  frequency\n",
      "rank                           \n",
      "1     the government      24369\n",
      "2            the new      22748\n",
      "3         the latest      17959\n",
      "4        the country      15560\n",
      "5          the first      12916\n"
     ]
    }
   ],
   "source": [
    "def ngrams_old(Corpus, token_str, ngram_length = 2, ngram_word_position = 'LEFT', page_size = PAGE_SIZE, page_current = 0):\n",
    "\ttoken_sequence, index_id = Corpus.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tcache_id = tuple(['ngram'] + list(token_sequence) + [ngram_length, ngram_word_position])\n",
    "\tif False: #cache_id in Corpus.results_cache:\n",
    "\t\tlogging.info('Using cached ngrams results')\n",
    "\t\tngrams_report = Corpus.results_cache[cache_id]\n",
    "\telse:\n",
    "\t\ttoken_index = Corpus.get_token_index(token_sequence, index_id)\n",
    "\t\t\n",
    "\t\tif len(token_index[0]) == 0:\n",
    "\t\t\tlogging.info('No tokens found')\n",
    "\t\t\treturn None, 0\n",
    "\n",
    "\t\tlogging.info('Generating ngrams results')\n",
    "\t\tngrams = get_ngrams_old(Corpus, token_sequence, index_id, token_index, ngram_length = ngram_length, ngram_word_position = ngram_word_position)\n",
    "\n",
    "\t\tunique_ngrams, counts = np.unique(ngrams, axis=0, return_counts=True)\n",
    "\t\tngrams_report = []\n",
    "\t\tunique_ngrams = list(map(tuple, unique_ngrams))\n",
    "\t\tngrams_report = pl.DataFrame({'ngram_token_ids': pl.Series(\"ngram_token_ids\", unique_ngrams, dtype=pl.Object) , 'frequency': counts, 'ngram': ''}).sort('frequency', descending=True)\n",
    "\t\tngrams_report = ngrams_report.with_row_index(name='rank', offset=1)\n",
    "\t\tCorpus.results_cache[cache_id] = ngrams_report\n",
    "\n",
    "\ttotal_count = len(ngrams_report)\n",
    "\n",
    "\tresultset_start = page_size*page_current\n",
    "\tresultset_end = page_size*(page_current+1)\n",
    "\n",
    "\t# get specific chunk of report into pandas based on resultset_start:\n",
    "\tngrams_report_page = ngrams_report.slice(resultset_start, page_size).to_pandas().set_index('ngram_token_ids')\n",
    "\tfor ngram_token_ids, row in ngrams_report_page.iterrows():\n",
    "\t\tngram_text = []\n",
    "\t\tfor token in ngram_token_ids:\n",
    "\t\t\tngram_text.append(Corpus.vocab[token])\n",
    "\t\tngrams_report_page.at[ngram_token_ids, 'ngram'] = ' '.join(ngram_text)\n",
    "\n",
    "\t# set index back to column and set new rank index\n",
    "\tngrams_report_page['token_ids'] = ngrams_report_page.index\n",
    "\tngrams_report_page = ngrams_report_page.set_index('rank')\n",
    "\t\n",
    "\t# sort column display - add normalized_frequency and rank, optional ngram_token_ids\n",
    "\tlogging.info(f'Ngrams report time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn ngrams_report_page[['ngram', 'frequency']], total_count\n",
    "\n",
    "def ngrams_new(Corpus, token_str, ngram_length = 2, ngram_word_position = 'LEFT', page_size = PAGE_SIZE, page_current = 0):\n",
    "\ttoken_sequence, index_id = Corpus.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tcache_id = tuple(['ngram'] + list(token_sequence) + [ngram_length, ngram_word_position])\n",
    "\tif False: #cache_id in Corpus.results_cache:\n",
    "\t\tlogging.info('Using cached ngrams results')\n",
    "\t\tngrams_report = Corpus.results_cache[cache_id]\n",
    "\telse:\n",
    "\t\ttoken_index = Corpus.get_token_index(token_sequence, index_id)\n",
    "\t\t\n",
    "\t\tif len(token_index[0]) == 0:\n",
    "\t\t\tlogging.info('No tokens found')\n",
    "\t\t\treturn None, 0\n",
    "\n",
    "\t\tlogging.info('Generating ngrams results')\n",
    "\t\tngrams = get_ngrams_new(Corpus, token_sequence, index_id, token_index, ngram_length = ngram_length, ngram_word_position = ngram_word_position)\n",
    "\t\t# get schema token_1, token_2 etc based on ngram_length\n",
    "\t\tschema = [f'token_{i+1}' for i in range(ngram_length)]\n",
    "\t\tngrams_report = pl.DataFrame(ngrams.T, schema=schema).to_struct(name = 'ngram_token_ids').value_counts(sort=True).rename({\"count\": \"frequency\"})\n",
    "\t\t# add ngram column with str type\n",
    "\t\tngrams_report = ngrams_report.with_row_index(name='rank', offset=1)\n",
    "\t\tCorpus.results_cache[cache_id] = ngrams_report\n",
    "\n",
    "\ttotal_count = len(ngrams_report)\n",
    "\n",
    "\tresultset_start = page_size*page_current\n",
    "\tresultset_end = page_size*(page_current+1)\n",
    "\n",
    "\t# get specific chunk of report into pandas based on resultset_start:\n",
    "\tngrams_report_page = ngrams_report.slice(resultset_start, page_size).unnest('ngram_token_ids')#.to_pandas().set_index('ngram_token_ids')\n",
    "\ttoken_strs = []\n",
    "\tfor i in range(ngram_length):\n",
    "\t\ttoken_ids = ngrams_report_page[f'token_{i+1}'].to_numpy()\n",
    "\t\tCorpus.results_cache['token_ids_for_test'] = token_ids\n",
    "\t\t_token_strs = Corpus.token_ids_to_tokens(token_ids)\n",
    "\t\ttoken_strs.append(_token_strs)\n",
    "\t\t#token_strs.append()\n",
    "\ttoken_strs = np.array(token_strs)\n",
    "\tngram_text = [' '.join(column) for column in token_strs.T]\n",
    "\tngrams_report_page = ngrams_report_page.with_columns(pl.Series(name=\"ngram\", values=ngram_text))\n",
    "\tngrams_report_page = ngrams_report_page.to_pandas().set_index('rank')\n",
    "\t\n",
    "\t# sort column display - add normalized_frequency and rank, optional ngram_token_ids\n",
    "\tlogging.info(f'Ngrams report time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn ngrams_report_page[['ngram', 'frequency']], total_count\n",
    "\n",
    "#REMEMBER - disabled result cache in these functions \n",
    "# \n",
    "run_time_test = False\n",
    "\n",
    "the_token_str = 'the'\n",
    "if run_time_test:\n",
    "\t%timeit result = ngrams_old(rnz500, the_token_str, ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "else:\n",
    "\t%time result = ngrams_old(rnz500, the_token_str, ngram_length = 2, ngram_word_position = 'LEFT', page_size = PAGE_SIZE, page_current = 0)\n",
    "result = ngrams_old(rnz500, the_token_str, ngram_length = 2, ngram_word_position = 'LEFT', page_size = PAGE_SIZE, page_current = 0)\n",
    "print(result[0][:5])\n",
    "\n",
    "if run_time_test:\n",
    "\t%timeit result = ngrams_new(rnz500, the_token_str, ngram_length = 2, ngram_word_position = 'LEFT')\n",
    "else:\n",
    "\t%time result = ngrams_new(rnz500, the_token_str, ngram_length = 2, ngram_word_position = 'LEFT', page_size = PAGE_SIZE, page_current = 0)\n",
    "result = ngrams_new(rnz500, the_token_str, ngram_length = 2, ngram_word_position = 'LEFT', page_size = PAGE_SIZE, page_current = 0)\n",
    "print(result[0][:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data structure</th>\n",
       "      <th>size (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original array</td>\n",
       "      <td>24.672913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reindexed array</td>\n",
       "      <td>12.336456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>original_to_new</td>\n",
       "      <td>2.500084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new_to_original</td>\n",
       "      <td>2.500084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>original vocab</td>\n",
       "      <td>10.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reindexed vocab</td>\n",
       "      <td>5.000076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data structure  size (MB)\n",
       "0   original array  24.672913\n",
       "1  reindexed array  12.336456\n",
       "2  original_to_new   2.500084\n",
       "3  new_to_original   2.500084\n",
       "4   original vocab  10.000084\n",
       "5  reindexed vocab   5.000076"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sizes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data structure</th>\n",
       "      <th>size (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>corpus source</td>\n",
       "      <td>38.819672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vocab (pkl.gz)</td>\n",
       "      <td>3.710016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>original array (npz)</td>\n",
       "      <td>9.917922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reindexed vocab (pkl.gz)</td>\n",
       "      <td>0.781233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>original_to_new (pkl.gz)</td>\n",
       "      <td>0.507931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>new_to_original (pkl.gz)</td>\n",
       "      <td>0.507943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reindexed array (npz)</td>\n",
       "      <td>4.914656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             data structure  size (MB)\n",
       "0             corpus source  38.819672\n",
       "1            vocab (pkl.gz)   3.710016\n",
       "2      original array (npz)   9.917922\n",
       "3  reindexed vocab (pkl.gz)   0.781233\n",
       "4  original_to_new (pkl.gz)   0.507931\n",
       "5  new_to_original (pkl.gz)   0.507943\n",
       "6     reindexed array (npz)   4.914656"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reindex(token_index):\n",
    "\t\"\"\" Takes as input a list of token ids (np.uint64) and reindexes that outputting a lookup and the reindexed token_index. \"\"\"\n",
    "\tunique_values = np.unique(token_index)\n",
    "\toriginal_to_new = dict(zip(unique_values, range(len(unique_values))))\n",
    "\tnew_to_original = dict(zip(range(len(unique_values)), unique_values))\n",
    "\treindexed_array = np.array([original_to_new[x] for x in token_index], dtype=np.uint32)\n",
    "\tvocab = {k:nlp.vocab.strings[k] for k in unique_values}\n",
    "\treindexed_vocab = {**{int(original_to_new[k]):nlp.vocab.strings[k] for k in vocab}, **{nlp.vocab.strings[k]:int(original_to_new[k]) for k in vocab}}\n",
    "\treturn reindexed_array, reindexed_vocab, original_to_new, new_to_original\n",
    "\n",
    "original_array = corpus.lower_index\n",
    "reindexed_array, reindexed_vocab, original_to_new, new_to_original = reindex(original_array)\n",
    "\n",
    "results = []\n",
    "results.append(['original array', original_array.nbytes / 1024 / 1024])\n",
    "results.append(['reindexed array', reindexed_array.nbytes / 1024 / 1024])\n",
    "results.append(['original_to_new', sys.getsizeof(original_to_new) / 1024 / 1024])\n",
    "results.append(['new_to_original', sys.getsizeof(new_to_original) / 1024 / 1024])\n",
    "results.append(['original vocab', sys.getsizeof(vocab) / 1024 / 1024])\n",
    "results.append(['reindexed vocab', sys.getsizeof(reindexed_vocab) / 1024 / 1024])\n",
    "\n",
    "print('Memory usage:')\n",
    "display(pd.DataFrame(results, columns=['data structure', 'size (MB)']))\n",
    "\n",
    "with gzip.open('../test-corpora/rnz-vocab.pkl.gz', 'wb') as f:\n",
    "\tpickle.dump(vocab, f)\n",
    "with gzip.open('../test-corpora/rnz-reindexed-vocab.pkl.gz', 'wb') as f:\n",
    "\tpickle.dump(reindexed_vocab, f)\n",
    "with gzip.open('../test-corpora/rnz-original-to-new.pkl.gz', 'wb') as f:\n",
    "\tpickle.dump(original_to_new, f)\n",
    "with gzip.open('../test-corpora/rnz-new-to-original.pkl.gz', 'wb') as f:\n",
    "\tpickle.dump(new_to_original, f)\n",
    "np.savez_compressed('../test-corpora/rnz-original-array.npz', original_array)\n",
    "np.savez_compressed('../test-corpora/rnz-reindexed-array.npz', reindexed_array)\n",
    "\n",
    "results = []\n",
    "results.append(['corpus source', os.path.getsize(corpus.path) / 1024 / 1024]) \n",
    "\n",
    "results.append(['vocab (pkl.gz)', os.path.getsize('../test-corpora/rnz-vocab.pkl.gz') / 1024 / 1024])\n",
    "results.append(['original array (npz)', os.path.getsize('../test-corpora/rnz-original-array.npz') / 1024 / 1024])\n",
    "\n",
    "results.append(['reindexed vocab (pkl.gz)', os.path.getsize('../test-corpora/rnz-reindexed-vocab.pkl.gz') / 1024 / 1024])\n",
    "results.append(['original_to_new (pkl.gz)', os.path.getsize('../test-corpora/rnz-original-to-new.pkl.gz') / 1024 / 1024])\n",
    "results.append(['new_to_original (pkl.gz)', os.path.getsize('../test-corpora/rnz-new-to-original.pkl.gz') / 1024 / 1024])\n",
    "results.append(['reindexed array (npz)', os.path.getsize('../test-corpora/rnz-reindexed-array.npz') / 1024 / 1024])\n",
    "\n",
    "print('File sizes:')\n",
    "display(pd.DataFrame(results, columns=['data structure', 'size (MB)']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
