{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Helper functions and classes for Conc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "from great_tables import GT\n",
    "import polars as pl\n",
    "import msgspec\n",
    "import spacy\n",
    "from memory_profiler import _get_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "PAGE_SIZE = 20\n",
    "EOF_TOKEN_STR = ' conc-end-of-file-token'\n",
    "ERR_TOKEN_STR = 'ERROR: not a token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DOCUMENTATION_URL = 'https://geoffford.nz/conc'\n",
    "REPOSITORY_URL = 'https://github.com/polsci/conc'\n",
    "PYPI_URL = ''\n",
    "CITATION_STR = '''If you use Conc in your work, please cite it as follows:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = f'{os.environ.get(\"HOME\")}/data/'\n",
    "save_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "polars_conf = pl.Config.set_tbl_width_chars(300)\n",
    "polars_conf = pl.Config.set_fmt_str_lengths(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ConcLogger(logging.Logger):\n",
    "\t\"\"\" Custom logger for conc module. \"\"\"\n",
    "\tdef __init__(self, name, level=logging.WARNING, log_file=None):\n",
    "\t\tsuper().__init__(name, level)\n",
    "\t\tself._setup_handler(log_file)\n",
    "\t\tself.last_memory_usage = None\n",
    "\n",
    "\tdef _setup_handler(self, log_file = None):\n",
    "\t\tconsole_handler = logging.StreamHandler()\n",
    "\t\tformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(funcName)s - %(message)s', \n",
    "\t\t\t\t\t\t\t\t\t  datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\t\tconsole_handler.setFormatter(formatter)\n",
    "\t\tself.addHandler(console_handler)\n",
    "\n",
    "\t\tif log_file is not None:\n",
    "\t\t\tfile_handler = logging.FileHandler(log_file)\n",
    "\t\t\tfile_handler.setFormatter(formatter)\n",
    "\t\t\tself.addHandler(file_handler)\n",
    "\n",
    "\tdef set_state(self, state:str # 'quiet' or 'verbose'\n",
    "\t\t\t\t  ):\n",
    "\t\tif state == 'quiet':\n",
    "\t\t\tlevel = logging.WARNING\n",
    "\t\telif state == 'verbose':\n",
    "\t\t\tlevel = logging.DEBUG\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"Invalid state: {state}\")\n",
    "\t\t\n",
    "\t\tself.setLevel(level)\n",
    "\n",
    "\tdef memory_usage(self, message = '', init=False):\n",
    "\t\tif init:\n",
    "\t\t\tself.last_memory_usage = None\n",
    "\t\tusage = _get_memory(-1, 'psutil', include_children=True)\n",
    "\t\tif self.last_memory_usage is not None:\n",
    "\t\t\tdifference = usage - self.last_memory_usage\n",
    "\t\t\tmemory_message = f', memory usage: {usage} MB, difference: {difference} MB'\n",
    "\t\telse:\n",
    "\t\t\tmemory_message = f', memory usage: {usage} MB'\n",
    "\t\tself.info(f\"{message}{memory_message}\")\n",
    "\t\tself.last_memory_usage = usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logging.setLoggerClass(ConcLogger)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def set_logger_state(state:str # 'quiet' or 'verbose'\n",
    "\t\t\t\t\t ):\n",
    "\t\"\"\" Set the state of the conc logger to either 'quiet' or 'verbose' \"\"\"\n",
    "\tlogger.set_state(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_ALPHA: 1\n",
      "IS_ASCII: 2\n",
      "IS_DIGIT: 3\n",
      "IS_LOWER: 4\n",
      "IS_PUNCT: 5\n",
      "IS_SPACE: 6\n",
      "IS_TITLE: 7\n",
      "IS_UPPER: 8\n",
      "LIKE_URL: 9\n",
      "LIKE_NUM: 10\n",
      "LIKE_EMAIL: 11\n",
      "IS_STOP: 12\n",
      "IS_OOV_DEPRECATED: 13\n",
      "IS_BRACKET: 14\n",
      "IS_QUOTE: 15\n",
      "IS_LEFT_PUNCT: 16\n",
      "IS_RIGHT_PUNCT: 17\n",
      "IS_CURRENCY: 18\n",
      "ID: 64\n",
      "ORTH: 65\n",
      "LOWER: 66\n",
      "NORM: 67\n",
      "SHAPE: 68\n",
      "PREFIX: 69\n",
      "SUFFIX: 70\n",
      "LENGTH: 71\n",
      "LEMMA: 73\n",
      "POS: 74\n",
      "TAG: 75\n",
      "DEP: 76\n",
      "ENT_IOB: 77\n",
      "ENT_TYPE: 78\n",
      "ENT_ID: 454\n",
      "ENT_KB_ID: 452\n",
      "HEAD: 79\n",
      "SENT_START: 80\n",
      "SPACY: 81\n",
      "LANG: 83\n",
      "MORPH: 453\n",
      "IDX: 455\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# This is a quick reminder of the available spacy attributes that can be output for a doc (depending on the model and pipe settings)\n",
    "for attr in spacy.attrs.IDS:\n",
    "\tif attr and not attr.startswith('FLAG'):\n",
    "\t\tprint(f'{attr}: {spacy.attrs.IDS[attr]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def spacy_attribute_name(index):\n",
    "\t\"\"\"Get name of index from spacy.\"\"\"\n",
    "\n",
    "\treturn list(spacy.attrs.IDS.keys())[list(spacy.attrs.IDS.values()).index(index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus metadata schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CorpusMetadata(msgspec.Struct): \n",
    "    \"\"\" JSON validation schema for corpus metadata \"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    slug: str\n",
    "    conc_version: str\n",
    "    document_count: int\n",
    "    token_count: int\n",
    "    word_token_count: int\n",
    "    punct_token_count: int\n",
    "    space_token_count: int\n",
    "    unique_tokens: int\n",
    "    unique_word_tokens: int\n",
    "    date_created: str\n",
    "    #source_path: str\n",
    "    EOF_TOKEN: int\n",
    "    SPACY_EOF_TOKEN: int\n",
    "    SPACY_MODEL: str\n",
    "    SPACY_MODEL_VERSION: str\n",
    "    punct_tokens: list[int]\n",
    "    space_tokens: list[int]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': {'type': 'string'},\n",
       " 'description': {'type': 'string'},\n",
       " 'slug': {'type': 'string'},\n",
       " 'conc_version': {'type': 'string'},\n",
       " 'document_count': {'type': 'integer'},\n",
       " 'token_count': {'type': 'integer'},\n",
       " 'word_token_count': {'type': 'integer'},\n",
       " 'punct_token_count': {'type': 'integer'},\n",
       " 'space_token_count': {'type': 'integer'},\n",
       " 'unique_tokens': {'type': 'integer'},\n",
       " 'unique_word_tokens': {'type': 'integer'},\n",
       " 'date_created': {'type': 'string'},\n",
       " 'EOF_TOKEN': {'type': 'integer'},\n",
       " 'SPACY_EOF_TOKEN': {'type': 'integer'},\n",
       " 'SPACY_MODEL': {'type': 'string'},\n",
       " 'SPACY_MODEL_VERSION': {'type': 'string'},\n",
       " 'punct_tokens': {'type': 'array', 'items': {'type': 'integer'}},\n",
       " 'space_tokens': {'type': 'array', 'items': {'type': 'integer'}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: true\n",
    "properties = msgspec.json.schema(CorpusMetadata)['$defs']['CorpusMetadata']['properties']\n",
    "display(properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def list_corpora(\n",
    "\t\tpath: str # path to load corpus\n",
    "\t\t) -> pl.DataFrame: # Dataframe with path, corpus, corpus name, document count, token count\n",
    "\t\"\"\" Scan a directory for available corpora \"\"\"\n",
    "\t\n",
    "\tavailable_corpora = {'corpus': [], 'name': [], 'date_created': [], 'document_count': [], 'token_count': []}\n",
    "\tfor dir in os.listdir(path):\n",
    "\t\tif os.path.isdir(os.path.join(path, dir)) and os.path.isfile( os.path.join(path, dir, 'corpus.json')):\n",
    "\t\t\twith open(os.path.join(path, dir, 'corpus.json'), 'rb') as f:\n",
    "\t\t\t\tdata = msgspec.json.decode(f.read(), type=CorpusMetadata)\n",
    "\n",
    "\t\t\tavailable_corpora['corpus'].append(dir)\n",
    "\t\t\tfor k in ['name', 'document_count', 'token_count', 'date_created']:\n",
    "\t\t\t\tattr = getattr(data, k)\n",
    "\t\t\t\tif isinstance(attr, int):\n",
    "\t\t\t\t\tattr = f'{attr:,}'\n",
    "\t\t\t\tavailable_corpora[k].append(attr)\n",
    "\n",
    "\treturn pl.DataFrame(available_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (8, 5)\n",
      "┌──────────────────────────────────────────────┬───────────────────────────────────────┬─────────────────────┬────────────────┬─────────────┐\n",
      "│ corpus                                       ┆ name                                  ┆ date_created        ┆ document_count ┆ token_count │\n",
      "│ ---                                          ┆ ---                                   ┆ ---                 ┆ ---            ┆ ---         │\n",
      "│ str                                          ┆ str                                   ┆ str                 ┆ str            ┆ str         │\n",
      "╞══════════════════════════════════════════════╪═══════════════════════════════════════╪═════════════════════╪════════════════╪═════════════╡\n",
      "│ introduce-yourself.corpus                    ┆ Introduce Yourself                    ┆ 2025-06-03 12:06:23 ┆ 28             ┆ 10,034      │\n",
      "│ gutenberg.corpus                             ┆ Gutenberg Corpus                      ┆ 2025-06-03 12:05:42 ┆ 18             ┆ 2,777,046   │\n",
      "│ us-congressional-speeches-subset-100k.corpus ┆ US Congressional Speeches Subset 100k ┆ 2025-06-03 12:29:33 ┆ 100,000        ┆ 20,027,241  │\n",
      "│ garden-party.corpus                          ┆ Garden Party Corpus                   ┆ 2025-06-03 12:05:43 ┆ 15             ┆ 79,940      │\n",
      "│ us-congressional-speeches-subset-10k.corpus  ┆ US Congressional Speeches Subset 10k  ┆ 2025-06-03 12:29:01 ┆ 10,000         ┆ 1,964,972   │\n",
      "│ brown.corpus                                 ┆ Brown Corpus                          ┆ 2025-06-03 12:16:02 ┆ 500            ┆ 1,140,905   │\n",
      "│ toy.corpus                                   ┆ Toy Corpus                            ┆ 2025-06-03 12:16:03 ┆ 6              ┆ 38          │\n",
      "│ reuters.corpus                               ┆ Reuters Corpus                        ┆ 2025-06-03 12:05:33 ┆ 10,788         ┆ 1,726,826   │\n",
      "└──────────────────────────────────────────────┴───────────────────────────────────────┴─────────────────────┴────────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(list_corpora(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stop_words(save_path:str, # directory to save stop words to, file name will be created based on spaCy model name\n",
    "\t\t\t\t   spacy_model:str = 'en_core_web_sm' # model to get stop words for\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\" Get stop words from spaCy and cache to disk \"\"\"\n",
    "\n",
    "\tstop_words = None\n",
    "\n",
    "\tfilename = f'{spacy_model}_stop_words.txt'\n",
    "\tsave_to = os.path.join(save_path, filename)\n",
    "\n",
    "\tif os.path.exists(save_to):\n",
    "\t\twith open(save_to, 'r', encoding='utf-8') as f:\n",
    "\t\t\tstop_words = set(f.read().splitlines())\n",
    "\n",
    "\tif stop_words is None:\n",
    "\t\tnlp = spacy.load(spacy_model)\n",
    "\t\tstop_words = nlp.Defaults.stop_words\n",
    "\t\tdel nlp\n",
    "\n",
    "\t\tif not os.path.exists(save_path):\n",
    "\t\t\tos.makedirs(save_path)\n",
    "\n",
    "\t\twith open(save_to, 'w', encoding='utf-8') as f:\n",
    "\t\t\tfor word in stop_words:\n",
    "\t\t\t\tf.write(word + '\\n')\n",
    "\n",
    "\treturn stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thru', 'also', 'whereafter', 'perhaps', 'behind', 'me', 'again', 'while', 'yourself', 'n’t', 'your', 'to', 'into', '’ve', 'would', 'each', 'see', 'other', 'am', 'somewhere', 'five', 'ever', 'you', 'besides', 'those', 'is', 'will', 'do', 'in', 'two', 'from', 'everywhere', 'an', 'off', 'therefore', 'already', 'sometimes', 'may', 'whose', 'why', 'has', 'it', 'up', 'her', 'keep', 'a', 'next', \"'d\", 'name', 'are', 'something', 'rather', 'does', 'eleven', 'part', 'herself', 'thence', 'we', 'now', 'moreover', 'out', 'of', 'due', 'mine', 'everything', 'as', 'ca', 'under', 'former', 'could', 'nine', 'what', 'without', 'n‘t', 'on', 'both', 'nothing', 'third', 'him', 'most', 'their', 'until', 'wherever', 'myself', 'often', 'among', 'whereby', 'never', 'many', 'say', '’re', 'regarding', 'seems', 'wherein', 'make', 'and', 'anyhow', 'put', 'beside', 'thereafter', 'always', 'afterwards', 'against', 'only', 'who', 'been', 'upon', 'by', 'enough', 'us', 'four', 'really', 'yourselves', 'few', 'none', 'various', 'how', 'whenever', 'alone', 'did', 'forty', 'whither', 'no', 'himself', 'themselves', 'our', 'she', 'side', 'where', \"'ll\", 'becomes', '‘ve', 'whereas', 'when', 'therein', 'were', 'six', 'last', \"'s\", 'anyway', 'cannot', '’d', 'latterly', 'had', 'though', 'for', 'call', 'nevertheless', 'made', 'being', 'not', 'seeming', 'unless', 'all', 'whatever', 'thereupon', 'fifteen', 'hereby', 'less', 'its', 'mostly', 'quite', 'above', 'move', 'between', 'same', 'indeed', 'although', 'once', 'so', '’s', 'sixty', 'however', 'just', '’m', 'go', 'first', 'full', 'very', 'here', '‘d', \"'re\", 'otherwise', 'namely', 'these', 'own', \"n't\", 'whence', 'three', 'done', 'than', 'after', 'might', 'through', 'he', 'yet', 'twenty', '‘ll', 'neither', 'whoever', 'ours', 'herein', 'if', 'still', 'well', 'my', 'beforehand', \"'ve\", 'get', 'i', 'whether', 'back', 'thereby', 'they', 'formerly', 'should', 'almost', 'whole', 'give', 'anyone', 'such', 'hence', 'his', 'became', 'nobody', 'this', 'per', 'since', 'along', 'elsewhere', 'everyone', 'either', 'eight', 'top', 'then', 'using', 'via', 'several', 'fifty', 'please', '‘re', 'them', 'empty', 'across', '‘s', 'seemed', 'much', 'except', 'somehow', 'nor', 'latter', 'serious', 'below', 'toward', 'nowhere', 'take', 'the', 'during', 'within', 'ten', 'that', 'which', 'beyond', 'be', 'meanwhile', 'hers', 'anywhere', 'used', 'else', 'at', 'more', 'yours', 'because', 'whom', 'ourselves', 'show', 'onto', 'someone', 'others', 'amount', 'hereupon', 'can', 'itself', 'thus', 'with', \"'m\", 'seem', 'anything', 'some', 'have', 'hereafter', 'every', 'doing', 'any', 'about', 'twelve', '‘m', 'there', 'but', 'must', 'sometime', 'over', 'front', 'throughout', 'down', 'one', 'bottom', 'whereupon', '’ll', 'amongst', 'too', 'was', 'towards', 'becoming', 'become', 're', 'together', 'around', 'before', 'hundred', 'or', 'another', 'further', 'noone', 'even', 'least'}\n"
     ]
    }
   ],
   "source": [
    "print(get_stop_words(save_path = save_path, spacy_model='en_core_web_sm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "toy_data = []\n",
    "toy_data.append(['1.txt', 'The cat sat on the mat.', 'feline', 'cat'])\n",
    "toy_data.append(['2.txt', 'The dog sat on the mat.', 'canine', 'dog'])\n",
    "toy_data.append(['3.txt', 'The cat is meowing.', 'feline', 'cat'])\n",
    "toy_data.append(['4.txt', 'The dog is barking.', 'canine', 'dog'])\n",
    "toy_data.append(['5.txt', 'The cat is climbing a tree.', 'feline', 'cat'])\n",
    "toy_data.append(['6.txt', 'The dog is digging a hole.', 'canine', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "32\n",
      "15\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# checking on counts above\n",
    "toy_data_test = [doc[1] for doc in toy_data] \n",
    "toy_data_test = [re.findall(r'\\b\\w+\\b|[^\\w\\s]', text) for text in toy_data_test]\n",
    "toy_data_test = [token.lower() for sublist in toy_data_test for token in sublist if token.strip()]\n",
    "#print(toy_data_test)\n",
    "# token count\n",
    "print(len(toy_data_test)) # should be 38\n",
    "# word token count\n",
    "print(len(toy_data_test) - sum([1 for token in toy_data_test if token == '.'])) # should be 32\n",
    "toy_data_test_unique = set(toy_data_test)\n",
    "# unique tokens\n",
    "print(len(toy_data_test_unique))\n",
    "toy_data_test_unique_word = set([token for token in toy_data_test_unique if token != '.'])\n",
    "# unique word tokens\n",
    "print(len(toy_data_test_unique_word))\n",
    "\n",
    "# based on this - toy corpus should have ... \n",
    "# document_count = 6\n",
    "# token_count = 38\n",
    "# word_token_count = 32\n",
    "# unique_tokens = 15\n",
    "# unique_word_tokens = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_toy_corpus_sources(source_path:str # path to location of sources for building corpora\n",
    "\t\t\t\t\t\t\t ):\n",
    "\t\"\"\" Create txt files and csv to test build of toy corpus. \"\"\"\n",
    "\n",
    "\ttoy_path = os.path.join(source_path, 'toy')\n",
    "\tif not os.path.exists(toy_path):\n",
    "\t\tos.makedirs(toy_path, exist_ok=True)\n",
    "\tfor row in toy_data:\n",
    "\t\twith open(f'{source_path}/toy/{row[0]}', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(row[1])\n",
    "\tdf = pl.DataFrame(toy_data, orient='row', schema=(('source', str), ('text', str), ('category', str), ('species', str)))\n",
    "\tdf.write_csv(os.path.join(source_path, 'toy.csv'))\n",
    "\tdf.write_csv(os.path.join(source_path, 'toy.csv.gz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "create_toy_corpus_sources(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_toy_corpus(\n",
    "        csv_path:str # path to location of csv for building corpora\n",
    "        ) -> GT: \n",
    "    \"\"\" Show toy corpus in a table. \"\"\"\n",
    "    \n",
    "    toy_corpus_df = pl.read_csv(csv_path)\n",
    "    GT(toy_corpus_df).tab_options(table_margin_left = 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"vmujympesc\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#vmujympesc table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#vmujympesc thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#vmujympesc p { margin: 0; padding: 0; }\n",
       " #vmujympesc .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #vmujympesc .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #vmujympesc .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #vmujympesc .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #vmujympesc .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #vmujympesc .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #vmujympesc .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #vmujympesc .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #vmujympesc .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #vmujympesc .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #vmujympesc .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #vmujympesc .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #vmujympesc .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #vmujympesc .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #vmujympesc .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #vmujympesc .gt_from_md> :first-child { margin-top: 0; }\n",
       " #vmujympesc .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #vmujympesc .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #vmujympesc .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #vmujympesc .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #vmujympesc .gt_row_group_first td { border-top-width: 2px; }\n",
       " #vmujympesc .gt_row_group_first th { border-top-width: 2px; }\n",
       " #vmujympesc .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #vmujympesc .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #vmujympesc .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #vmujympesc .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #vmujympesc .gt_left { text-align: left; }\n",
       " #vmujympesc .gt_center { text-align: center; }\n",
       " #vmujympesc .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #vmujympesc .gt_font_normal { font-weight: normal; }\n",
       " #vmujympesc .gt_font_bold { font-weight: bold; }\n",
       " #vmujympesc .gt_font_italic { font-style: italic; }\n",
       " #vmujympesc .gt_super { font-size: 65%; }\n",
       " #vmujympesc .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #vmujympesc .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"source\">source</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"text\">text</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"category\">category</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"species\">species</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">1.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat sat on the mat.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">2.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog sat on the mat.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">3.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat is meowing.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">4.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog is barking.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">5.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat is climbing a tree.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">6.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog is digging a hole.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_toy_corpus(os.path.join(source_path, 'toy.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_nltk_corpus_sources(source_path:str # path to location of sources for building corpora\n",
    "\t\t\t\t\t\t\t ):\n",
    "\t\"\"\" Get NLTK corpora as sources for development or testing Conc functionality. \"\"\"\n",
    "\n",
    "\ttry:\n",
    "\t\timport nltk\n",
    "\texcept ImportError as e:\n",
    "\t\traise ImportError('This function requires NLTK. To minimise requirements this is not installed by default. You can install NLTK with \"pip install nltk\"')\n",
    "\n",
    "\timport nltk\n",
    "\tnltk.download('gutenberg')\n",
    "\tnltk.download('brown')\n",
    "\tnltk.download('reuters')\n",
    "\tfrom nltk.corpus import gutenberg\n",
    "\tfrom nltk.corpus import reuters\n",
    "\tfrom nltk.corpus import brown\n",
    "\n",
    "\tdef clean_text(text):\n",
    "\t\t# to match words/punc that followed by /tags\n",
    "\t\tpattern = re.compile(r\"(\\S+)(/[^ ]+)\") # match non-space followed by / and non-space\n",
    "\t\treturn pattern.sub(r\"\\1\", text)\n",
    "\n",
    "\tif not os.path.exists(source_path):\n",
    "\t\tos.makedirs(source_path, exist_ok=True)\n",
    "\tif not os.path.exists(f'{source_path}/brown'):\n",
    "\t\tos.makedirs(f'{source_path}/brown', exist_ok=True)\n",
    "\tbrown_path = os.path.join(source_path, 'brown.csv.gz')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in brown.fileids():\n",
    "\t\tcorpus_data.append([fileid, clean_text(brown.raw(fileid))])\n",
    "\t\twith open(f'{source_path}/brown/{fileid}.txt', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(clean_text(brown.raw(fileid)))\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str)))\n",
    "\tdf.write_csv(brown_path)\n",
    "\n",
    "\tgutenberg_path = os.path.join(source_path, 'gutenberg.csv.gz')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in gutenberg.fileids():\n",
    "\t\tcorpus_data.append([fileid, clean_text(gutenberg.raw(fileid))])\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str)))\n",
    "\tdf.write_csv(gutenberg_path)\n",
    "\n",
    "\treuters_path = os.path.join(source_path, 'reuters.csv.gz')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in reuters.fileids():\n",
    "\t\tfileid_name = fileid.split('/')[1]\n",
    "\t\tcorpus_data.append([fileid_name, clean_text(reuters.raw(fileid))])\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str)))\n",
    "\tdf.write_csv(reuters_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "get_nltk_corpus_sources(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts for the Brown corpus from nltk can be used to test Conc functionality. The Reuters and Gutenberg corpora are also prepared by `get_nltk_corpus_sources`. Running the function will download the texts and save the texts as a .csv.gz files with columns: source and text. The Brown Corpus is also saved as .txt files to test the Corpus.build_from_texts method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_garden_party(source_path: str #path to location of sources for building corpora\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\" Get corpus of The Garden Party by Katherine Mansfield for development of Conc and testing Conc functionality. \"\"\"\n",
    "\n",
    "\tpath = 'https://github.com/ucdh/scraping-garden-party/raw/master/garden-party-corpus.zip'\n",
    "\n",
    "\timport requests\n",
    "\ttry:\n",
    "\t\timport requests\n",
    "\texcept ImportError as e:\n",
    "\t\traise ImportError('This function requires the requests library. To minimise requirements this is not installed by default. You can install requests with \"pip install requests\"')\n",
    "\n",
    "\tr = requests.get(path)\n",
    "\twith open(f'{source_path}/garden-party-corpus.zip', 'wb') as f:\n",
    "\t\tf.write(r.content)\n",
    "\t# converting to .tar and tar.gz files for testing\n",
    "\timport zipfile\n",
    "\twith zipfile.ZipFile(f'{source_path}/garden-party-corpus.zip', 'r') as z:\n",
    "\t\tz.extractall(f'{source_path}/garden-party-corpus')\n",
    "\timport shutil # make tar.gz\n",
    "\tshutil.make_archive(f'{source_path}/garden-party-corpus', 'gztar', f'{source_path}/garden-party-corpus')\n",
    "\tshutil.move(f'{source_path}/garden-party-corpus.tar.gz', f'{source_path}/garden-party-corpus.tar.gz')\n",
    "\tshutil.make_archive(f'{source_path}/garden-party-corpus', 'tar', f'{source_path}/garden-party-corpus')\n",
    "\tshutil.move(f'{source_path}/garden-party-corpus.tar', f'{source_path}/garden-party-corpus.tar')\n",
    "\tshutil.rmtree(f'{source_path}/garden-party-corpus')\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_garden_party` function downloads a zip file of an example corpus based on Katherine Mansfield short stories. This function creates a .tar and a .tar.gz version of the texts for testing Corpus build methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "get_garden_party(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create large corpora for development and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_large_dataset(source_path: str #path to location of sources for building corpora\n",
    "                    ):\n",
    "    \"\"\" Get 1m rows of https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset for testing. \"\"\"\n",
    "    df = pl.read_parquet('hf://datasets/Eugleo/us-congressional-speeches-subset/data/train-*.parquet')\n",
    "    df.sample(1000000).select(['speech_id', 'date', 'speaker', 'chamber', 'state', 'text']).write_csv(f'{source_path}/us-congressional-speeches-subset-1m.csv.gz')\n",
    "    del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# get_large_dataset(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Define the chunk size\n",
    "# chunk_size = 100_000  # Adjust based on your memory constraints\n",
    "\n",
    "# # Lazily load the CSV file\n",
    "# df = pl.scan_csv(f'{source_path}us-congressional-speeches-subset-1m.csv.gz')\n",
    "\n",
    "# # Add the new column 'is_empty'\n",
    "# df = df.with_columns(\n",
    "#     (pl.col('text').str.strip_chars().eq('')).alias('is_empty')\n",
    "# )\n",
    "\n",
    "# # get length of is_empty where True\n",
    "# count = df.filter(pl.col(\"is_empty\") == True).collect().height\n",
    "# print(f\"Number of empty rows: {count}\")\n",
    "\n",
    "# any empty?\n",
    "#len(df[df['text'].is_null()])\n",
    "\n",
    "# get distribution of date (by year), speaker, chamber, state\n",
    "# dates are in iso format - extract year and summarize\n",
    "# df = df.with_columns(pl.col('date').str.slice(0, 4).alias('year'))\n",
    "# df.group_by('year').agg(pl.count('year').alias('count')).sort('year', descending=True).head(10).collect()\n",
    "\n",
    "# #df.group_by('speaker').agg(pl.count('speaker').alias('count')).sort('count', descending=True).head(20)\n",
    "# #df.group_by('chamber').agg(pl.count('chamber').alias('count')).sort('count', descending=True).head(20)\n",
    "# df.group_by('state').agg(pl.count('state').alias('count')).sort('count', descending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_large_dataset_sizes(source_path: str, #path to location of sources for building corpora\n",
    "\t\t\t\t\t\tsizes: list = [10000, 100000, 200000, 500000] # list of sizes for test data-sets\n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Create datasets of different sizes from data source retrieved by get_large_dataset for testing. \"\"\"\n",
    "\tfor max_i in sizes:\n",
    "\t\tmax_i_label = int(max_i / 1000)\n",
    "\t\tdf = pl.read_csv(f'{source_path}/us-congressional-speeches-subset-1m.csv.gz')\n",
    "\t\tdf.sample(max_i).write_csv(f'{source_path}/us-congressional-speeches-subset-{max_i_label}k.csv.gz')\n",
    "\t\tlogger.info(f'Creating dataset of {max_i_label}k rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# create_large_dataset_sizes(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
