{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Helper functions and classes for Conc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# requirements - numpy polars spacy great_tables, nltk, requests, python-slugify\n",
    "# dev requirements - nbdev, jupyterlab, jupyterlab-quarto, memory_profiler, line_profiler\n",
    "# TODO check all necessary\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import polars as pl\n",
    "from great_tables import GT\n",
    "import polars as pl\n",
    "\n",
    "from memory_profiler import _get_memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "PAGE_SIZE = 20\n",
    "EOF_TOKEN_STR = ' conc-end-of-file-token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DOCUMENTATION_URL = 'https://geoffford.nz/conc'\n",
    "REPOSITORY_URL = 'https://github.com/polsci/conc'\n",
    "CITATION_STR = '''If you use Conc in your work, please cite it as follows:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ConcLogger(logging.Logger):\n",
    "\t\"\"\" Custom logger for conc module. \"\"\"\n",
    "\tdef __init__(self, name, level=logging.WARNING, log_file=None):\n",
    "\t\tsuper().__init__(name, level)\n",
    "\t\tself._setup_handler(log_file)\n",
    "\t\tself.last_memory_usage = None\n",
    "\n",
    "\tdef _setup_handler(self, log_file = None):\n",
    "\t\tconsole_handler = logging.StreamHandler()\n",
    "\t\tformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(funcName)s - %(message)s', \n",
    "\t\t\t\t\t\t\t\t\t  datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\t\tconsole_handler.setFormatter(formatter)\n",
    "\t\tself.addHandler(console_handler)\n",
    "\n",
    "\t\tif log_file is not None:\n",
    "\t\t\tfile_handler = logging.FileHandler(log_file)\n",
    "\t\t\tfile_handler.setFormatter(formatter)\n",
    "\t\t\tself.addHandler(file_handler)\n",
    "\n",
    "\tdef set_state(self, state:str # 'quiet' or 'verbose'\n",
    "\t\t\t\t  ):\n",
    "\t\tif state == 'quiet':\n",
    "\t\t\tlevel = logging.WARNING\n",
    "\t\telif state == 'verbose':\n",
    "\t\t\tlevel = logging.INFO\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"Invalid state: {state}\")\n",
    "\t\t\n",
    "\t\tself.setLevel(level)\n",
    "\n",
    "\tdef memory_usage(self, message = '', init=False):\n",
    "\t\tif init:\n",
    "\t\t\tself.last_memory_usage = None\n",
    "\t\tusage = _get_memory(-1, 'psutil', include_children=True)\n",
    "\t\tif self.last_memory_usage is not None:\n",
    "\t\t\tdifference = usage - self.last_memory_usage\n",
    "\t\t\tmemory_message = f', memory usage: {usage} MB, difference: {difference} MB'\n",
    "\t\telse:\n",
    "\t\t\tmemory_message = f', memory usage: {usage} MB'\n",
    "\t\tself.info(f\"{message}{memory_message}\")\n",
    "\t\tself.last_memory_usage = usage\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logging.setLoggerClass(ConcLogger)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def set_logger_state(state:str # 'quiet' or 'verbose'\n",
    "\t\t\t\t\t ):\n",
    "\t\"\"\" Set the state of the conc logger to either 'quiet' or 'verbose' \"\"\"\n",
    "\tlogger.set_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "toy_data = []\n",
    "toy_data.append(['1.txt', 'The cat sat on the mat.', 'feline', 'cat'])\n",
    "toy_data.append(['2.txt', 'The dog sat on the mat.', 'canine', 'dog'])\n",
    "toy_data.append(['3.txt', 'The cat is meowing.', 'feline', 'cat'])\n",
    "toy_data.append(['4.txt', 'The dog is barking.', 'canine', 'dog'])\n",
    "toy_data.append(['5.txt', 'The cat is climbing a tree.', 'feline', 'cat'])\n",
    "toy_data.append(['6.txt', 'The dog is digging a hole.', 'canine', 'dog'])\n",
    "\n",
    "source_path = '../test-corpora/source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_toy_corpus_sources(source_path:str # path to location of sources for building corpora\n",
    "\t\t\t\t\t\t\t ):\n",
    "\t\"\"\" Create txt files and csv to test build of toy corpus. \"\"\"\n",
    "\n",
    "\ttoy_path = os.path.join(source_path, 'toy')\n",
    "\tif not os.path.exists(toy_path):\n",
    "\t\tos.makedirs(toy_path, exist_ok=True)\n",
    "\tfor row in toy_data:\n",
    "\t\twith open(f'{source_path}/toy/{row[0]}', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(row[1])\n",
    "\tdf = pl.DataFrame(toy_data, orient='row', schema=(('source', str), ('text', str), ('category', str), ('species', str)))\n",
    "\tdf.write_csv(os.path.join(source_path, 'toy.csv'))\n",
    "\tdf.write_csv(os.path.join(source_path, 'toy.csv.gz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "create_toy_corpus_sources(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"anbqrtnwmw\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#anbqrtnwmw table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#anbqrtnwmw thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#anbqrtnwmw p { margin: 0; padding: 0; }\n",
       " #anbqrtnwmw .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #anbqrtnwmw .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #anbqrtnwmw .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #anbqrtnwmw .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #anbqrtnwmw .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #anbqrtnwmw .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #anbqrtnwmw .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #anbqrtnwmw .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #anbqrtnwmw .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #anbqrtnwmw .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #anbqrtnwmw .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #anbqrtnwmw .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #anbqrtnwmw .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #anbqrtnwmw .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #anbqrtnwmw .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #anbqrtnwmw .gt_from_md> :first-child { margin-top: 0; }\n",
       " #anbqrtnwmw .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #anbqrtnwmw .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #anbqrtnwmw .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #anbqrtnwmw .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #anbqrtnwmw .gt_row_group_first td { border-top-width: 2px; }\n",
       " #anbqrtnwmw .gt_row_group_first th { border-top-width: 2px; }\n",
       " #anbqrtnwmw .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #anbqrtnwmw .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #anbqrtnwmw .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #anbqrtnwmw .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #anbqrtnwmw .gt_left { text-align: left; }\n",
       " #anbqrtnwmw .gt_center { text-align: center; }\n",
       " #anbqrtnwmw .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #anbqrtnwmw .gt_font_normal { font-weight: normal; }\n",
       " #anbqrtnwmw .gt_font_bold { font-weight: bold; }\n",
       " #anbqrtnwmw .gt_font_italic { font-style: italic; }\n",
       " #anbqrtnwmw .gt_super { font-size: 65%; }\n",
       " #anbqrtnwmw .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #anbqrtnwmw .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"source\">source</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"text\">text</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"category\">category</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"species\">species</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">1.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat sat on the mat.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">2.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog sat on the mat.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">3.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat is meowing.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">4.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog is barking.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">5.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat is climbing a tree.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">6.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog is digging a hole.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "toy_corpus_df = pl.read_csv(os.path.join(source_path, 'toy.csv'))\n",
    "GT(toy_corpus_df).tab_options(table_margin_left = 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_nltk_corpus_sources(source_path:str # path to location of sources for building corpora\n",
    "\t\t\t\t\t\t\t ):\n",
    "\t\"\"\" Get nltk corpora as sources for testing. \"\"\"\n",
    "\n",
    "\timport nltk\n",
    "\tnltk.download('gutenberg')\n",
    "\tnltk.download('brown')\n",
    "\tnltk.download('reuters')\n",
    "\tfrom nltk.corpus import gutenberg\n",
    "\tfrom nltk.corpus import reuters\n",
    "\tfrom nltk.corpus import brown\n",
    "\n",
    "\tdef clean_text(text):\n",
    "\t\t# to match words/punc that followed by /tags\n",
    "\t\tpattern = re.compile(r\"(\\S+)(/[^ ]+)\") # match non-space followed by / and non-space\n",
    "\t\treturn pattern.sub(r\"\\1\", text)\n",
    "\n",
    "\tif not os.path.exists(source_path):\n",
    "\t\tos.makedirs(source_path, exist_ok=True)\n",
    "\tif not os.path.exists(f'{source_path}/brown'):\n",
    "\t\tos.makedirs(f'{source_path}/brown', exist_ok=True)\n",
    "\tbrown_path = os.path.join(source_path, 'brown.csv.gz')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in brown.fileids():\n",
    "\t\tcorpus_data.append([fileid, clean_text(brown.raw(fileid))])\n",
    "\t\twith open(f'{source_path}/brown/{fileid}.txt', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(clean_text(brown.raw(fileid)))\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str)))\n",
    "\tdf.write_csv(brown_path)\n",
    "\n",
    "\tgutenberg_path = os.path.join(source_path, 'gutenberg.csv.gz')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in gutenberg.fileids():\n",
    "\t\tcorpus_data.append([fileid, clean_text(gutenberg.raw(fileid))])\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str)))\n",
    "\tdf.write_csv(gutenberg_path)\n",
    "\n",
    "\treuters_path = os.path.join(source_path, 'reuters.csv.gz')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in reuters.fileids():\n",
    "\t\tfileid_name = fileid.split('/')[1]\n",
    "\t\tcorpus_data.append([fileid_name, clean_text(reuters.raw(fileid))])\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str)))\n",
    "\tdf.write_csv(reuters_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "get_nltk_corpus_sources(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts for the Brown corpus from nltk can be used for testing Conc functionality. The Reuters and Gutenberg corpora are also prepared by `get_nltk_corpus_sources`. Running the function will download the texts and save the texts as a .csv.gz files with columns: source and text. The Brown Corpus is also saved as .txt files to test the Corpus.build_from_texts method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_garden_party(source_path: str #path to location of sources for building corpora\n",
    "\t\t\t\t\t):\n",
    "    \"\"\" Get corpus of The Garden Party by Katherine Mansfield for testing. \"\"\"\n",
    "    path = 'https://github.com/ucdh/scraping-garden-party/raw/master/garden-party-corpus.zip'\n",
    "    import requests\n",
    "    r = requests.get(path)\n",
    "    with open(f'{source_path}/garden-party-corpus.zip', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    # convert to .tar and tar.gz files for testing\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(f'{source_path}/garden-party-corpus.zip', 'r') as z:\n",
    "        z.extractall(f'{source_path}/garden-party-corpus')\n",
    "    import shutil # make tar.gz\n",
    "    shutil.make_archive(f'{source_path}/garden-party-corpus', 'gztar', f'{source_path}/garden-party-corpus')\n",
    "    shutil.move(f'{source_path}/garden-party-corpus.tar.gz', f'{source_path}/garden-party-corpus.tar.gz')\n",
    "    shutil.make_archive(f'{source_path}/garden-party-corpus', 'tar', f'{source_path}/garden-party-corpus')\n",
    "    shutil.move(f'{source_path}/garden-party-corpus.tar', f'{source_path}/garden-party-corpus.tar')\n",
    "    shutil.rmtree(f'{source_path}/garden-party-corpus')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "get_garden_party(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_large_dataset(source_path: str #path to location of sources for building corpora\n",
    "                    ):\n",
    "    \"\"\" Get 1m rows of https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset for testing. \"\"\"\n",
    "    df = pl.read_parquet('hf://datasets/Eugleo/us-congressional-speeches-subset/data/train-*.parquet')\n",
    "    df.sample(1000000).select(['speech_id', 'date', 'speaker', 'chamber', 'state', 'text']).write_csv(f'{source_path}/us-congressional-speeches-subset-1m.csv.gz')\n",
    "    del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Define the chunk size\n",
    "chunk_size = 100_000  # Adjust based on your memory constraints\n",
    "\n",
    "# Lazily load the CSV file\n",
    "df = pl.scan_csv('../test-corpora/source/us-congressional-speeches-subset-1m.csv.gz')\n",
    "\n",
    "# Add the new column 'is_empty'\n",
    "df = df.with_columns(\n",
    "    (pl.col('text').str.strip_chars().eq('')).alias('is_empty')\n",
    ")\n",
    "\n",
    "# get length of is_empty where True\n",
    "count = df.filter(pl.col(\"is_empty\") == True).collect().height\n",
    "print(f\"Number of empty rows: {count}\")\n",
    "\n",
    "# any empty?\n",
    "#len(df[df['text'].is_null()])\n",
    "\n",
    "# # get distribution of date (by year), speaker, chamber, state\n",
    "# # dates are in iso format - extract year and summarize\n",
    "# df = df.with_columns(pl.col('date').str.slice(0, 4).alias('year'))\n",
    "# #df.group_by('year').agg(pl.count('year').alias('count')).sort('year', descending=False).head(200)\n",
    "\n",
    "# #df.group_by('speaker').agg(pl.count('speaker').alias('count')).sort('count', descending=True).head(20)\n",
    "# #df.group_by('chamber').agg(pl.count('chamber').alias('count')).sort('count', descending=True).head(20)\n",
    "# df.group_by('state').agg(pl.count('state').alias('count')).sort('count', descending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_dataset_sizes(source_path: str, #path to location of sources for building corpora\n",
    "\t\t\t\t\t\tsizes: list = [10000, 100000, 200000, 500000] # list of sizes for test data-sets\n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Create datasets of different sizes for testing. \"\"\"\n",
    "\tfor max_i in sizes:\n",
    "\t\tmax_i_label = int(max_i / 1000)\n",
    "\t\tdf = pl.read_csv(f'{source_path}/us-congressional-speeches-subset-1m.csv.gz')\n",
    "\t\tdf.sample(max_i).write_csv(f'{source_path}/us-congressional-speeches-subset-{max_i_label}k.csv.gz')\n",
    "\t\tlogger.info(f'Creating dataset of {max_i_label}k rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "create_dataset_sizes(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
