{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpora\n",
    "\n",
    "> Functions to work with multiple corpora and download and build sample corpora.\n",
    "- toc: false\n",
    "- page-layout: full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import os\n",
    "from great_tables import GT\n",
    "import polars as pl\n",
    "import msgspec\n",
    "from fastcore.script import call_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc.core import CorpusMetadata, logger\n",
    "from conc.corpus import Corpus\n",
    "from conc.listcorpus import ListCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = f'{os.environ.get(\"HOME\")}/data/'\n",
    "save_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "polars_conf = pl.Config.set_tbl_width_chars(300)\n",
    "polars_conf = pl.Config.set_fmt_str_lengths(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def list_corpora(\n",
    "\t\tpath: str # path to load corpus\n",
    "\t\t) -> pl.DataFrame: # Dataframe with path, corpus, format (Corpus or List Corpus), corpus name, document count, token count\n",
    "\t\"\"\" Scan a directory for available corpora \"\"\"\n",
    "\t\n",
    "\tavailable_corpora = {'corpus': [], 'name': [], 'format': [], 'date_created': [], 'document_count': [], 'token_count': []}\n",
    "\tfor dir in os.listdir(path):\n",
    "\t\tif os.path.isdir(os.path.join(path, dir)):\n",
    "\t\t\tif os.path.isfile( os.path.join(path, dir, 'corpus.json')):\n",
    "\t\t\t\twith open(os.path.join(path, dir, 'corpus.json'), 'rb') as f:\n",
    "\t\t\t\t\tdata = msgspec.json.decode(f.read(), type=CorpusMetadata)\n",
    "\t\t\t\tavailable_corpora['format'].append('Corpus')\n",
    "\t\t\telif os.path.isfile( os.path.join(path, dir, 'listcorpus.json')):\n",
    "\t\t\t\twith open(os.path.join(path, dir, 'listcorpus.json'), 'rb') as f:\n",
    "\t\t\t\t\tdata = msgspec.json.decode(f.read(), type=CorpusMetadata)\n",
    "\t\t\t\tavailable_corpora['format'].append('List Corpus')\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tavailable_corpora['corpus'].append(dir)\n",
    "\t\t\tfor k in ['name', 'document_count', 'token_count', 'date_created']:\n",
    "\t\t\t\tattr = getattr(data, k)\n",
    "\t\t\t\tif isinstance(attr, int):\n",
    "\t\t\t\t\tattr = f'{attr:,}'\n",
    "\t\t\t\tavailable_corpora[k].append(attr)\n",
    "\n",
    "\treturn pl.DataFrame(available_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────────────┬─────────────────────┬─────────────┬─────────────────────┬────────────────┬─────────────┐\n",
      "│ corpus                    ┆ name                ┆ format      ┆ date_created        ┆ document_count ┆ token_count │\n",
      "╞═══════════════════════════╪═════════════════════╪═════════════╪═════════════════════╪════════════════╪═════════════╡\n",
      "│ bnc.listcorpus            ┆ BNC                 ┆ List Corpus ┆ 2025-07-09 08:54:00 ┆ 4,049          ┆ 113,536,056 │\n",
      "│ quake-stories-v2.corpus   ┆ Quake Stories v2    ┆ Corpus      ┆ 2025-07-02 09:42:43 ┆ 487            ┆ 472,876     │\n",
      "│ gutenberg.corpus          ┆ Gutenberg Corpus    ┆ Corpus      ┆ 2025-07-09 09:22:17 ┆ 18             ┆ 2,546,286   │\n",
      "│ brown.corpus              ┆ Brown Corpus        ┆ Corpus      ┆ 2025-07-09 09:21:45 ┆ 500            ┆ 1,138,566   │\n",
      "│ brown.listcorpus          ┆ Brown Corpus        ┆ List Corpus ┆ 2025-07-09 09:21:45 ┆ 500            ┆ 1,138,566   │\n",
      "│ bnc.corpus                ┆ BNC                 ┆ Corpus      ┆ 2025-07-09 08:54:00 ┆ 4,049          ┆ 113,536,056 │\n",
      "│ introduce-yourself.corpus ┆ Introduce Yourself  ┆ Corpus      ┆ 2025-07-01 11:58:45 ┆ 28             ┆ 9,913       │\n",
      "│ reuters.corpus            ┆ Reuters Corpus      ┆ Corpus      ┆ 2025-07-09 09:21:55 ┆ 10,788         ┆ 1,552,919   │\n",
      "│ toy.listcorpus            ┆ Toy Corpus          ┆ List Corpus ┆ 2025-07-09 09:21:39 ┆ 6              ┆ 38          │\n",
      "│ garden-party.corpus       ┆ Garden Party Corpus ┆ Corpus      ┆ 2025-07-09 09:22:19 ┆ 15             ┆ 74,664      │\n",
      "│ toy.corpus                ┆ Toy Corpus          ┆ Corpus      ┆ 2025-07-09 09:21:39 ┆ 6              ┆ 38          │\n",
      "│ garden-party.listcorpus   ┆ Garden Party Corpus ┆ List Corpus ┆ 2025-07-09 09:22:19 ┆ 15             ┆ 74,664      │\n",
      "│ baby-bnc.listcorpus       ┆ Baby BNC            ┆ List Corpus ┆ 2025-07-09 08:44:05 ┆ 182            ┆ 4,674,632   │\n",
      "│ baby-bnc.corpus           ┆ Baby BNC            ┆ Corpus      ┆ 2025-07-09 08:44:05 ┆ 182            ┆ 4,674,632   │\n",
      "└───────────────────────────┴─────────────────────┴─────────────┴─────────────────────┴────────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(list_corpora(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "toy_data = []\n",
    "toy_data.append(['1.txt', 'The cat sat on the mat.', 'feline', 'cat'])\n",
    "toy_data.append(['2.txt', 'The dog sat on the mat.', 'canine', 'dog'])\n",
    "toy_data.append(['3.txt', 'The cat is meowing.', 'feline', 'cat'])\n",
    "toy_data.append(['4.txt', 'The dog is barking.', 'canine', 'dog'])\n",
    "toy_data.append(['5.txt', 'The cat is climbing a tree.', 'feline', 'cat'])\n",
    "toy_data.append(['6.txt', 'The dog is digging a hole.', 'canine', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "32\n",
      "15\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# checking on counts above\n",
    "toy_data_test = [doc[1] for doc in toy_data] \n",
    "toy_data_test = [re.findall(r'\\b\\w+\\b|[^\\w\\s]', text) for text in toy_data_test]\n",
    "toy_data_test = [token.lower() for sublist in toy_data_test for token in sublist if token.strip()]\n",
    "#print(toy_data_test)\n",
    "# token count\n",
    "print(len(toy_data_test)) # should be 38\n",
    "# word token count\n",
    "print(len(toy_data_test) - sum([1 for token in toy_data_test if token == '.'])) # should be 32\n",
    "toy_data_test_unique = set(toy_data_test)\n",
    "# unique tokens\n",
    "print(len(toy_data_test_unique))\n",
    "toy_data_test_unique_word = set([token for token in toy_data_test_unique if token != '.'])\n",
    "# unique word tokens\n",
    "print(len(toy_data_test_unique_word))\n",
    "\n",
    "# based on this - toy corpus should have ... \n",
    "# document_count = 6\n",
    "# token_count = 38\n",
    "# word_token_count = 32\n",
    "# unique_tokens = 15\n",
    "# unique_word_tokens = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_toy_corpus_sources(source_path:str # path to location of sources for building corpora\n",
    "\t\t\t\t\t\t\t ):\n",
    "\t\"\"\" Create txt files and csv to test build of toy corpus. \"\"\"\n",
    "\n",
    "\ttoy_path = os.path.join(source_path, 'toy')\n",
    "\tif not os.path.exists(toy_path):\n",
    "\t\tos.makedirs(toy_path, exist_ok=True)\n",
    "\tfor row in toy_data:\n",
    "\t\twith open(f'{source_path}/toy/{row[0]}', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(row[1])\n",
    "\tdf = pl.DataFrame(toy_data, orient='row', schema=(('source', str), ('text', str), ('category', str), ('species', str)))\n",
    "\tdf.write_csv(os.path.join(source_path, 'toy.csv'))\n",
    "\tdf.write_csv(os.path.join(source_path, 'toy.csv.gz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "create_toy_corpus_sources(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_toy_corpus(\n",
    "        csv_path:str # path to location of csv for building corpora\n",
    "        ) -> GT: \n",
    "    \"\"\" Show toy corpus in a table. \"\"\"\n",
    "    \n",
    "    toy_corpus_df = pl.read_csv(csv_path)\n",
    "    GT(toy_corpus_df).tab_options(table_margin_left = 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"ifavxjswhw\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#ifavxjswhw table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#ifavxjswhw thead, tbody, tfoot, tr, td, th { border-style: none !important; }\n",
       " tr { background-color: transparent !important; }\n",
       "#ifavxjswhw p { margin: 0 !important; padding: 0 !important; }\n",
       " #ifavxjswhw .gt_table { display: table !important; border-collapse: collapse !important; line-height: normal !important; margin-left: 0 !important; margin-right: auto !important; color: #333333 !important; font-size: 16px !important; font-weight: normal !important; font-style: normal !important; background-color: #FFFFFF !important; width: auto !important; border-top-style: solid !important; border-top-width: 2px !important; border-top-color: #A8A8A8 !important; border-right-style: none !important; border-right-width: 2px !important; border-right-color: #D3D3D3 !important; border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #A8A8A8 !important; border-left-style: none !important; border-left-width: 2px !important; border-left-color: #D3D3D3 !important; }\n",
       " #ifavxjswhw .gt_caption { padding-top: 4px !important; padding-bottom: 4px !important; }\n",
       " #ifavxjswhw .gt_title { color: #333333 !important; font-size: 125% !important; font-weight: initial !important; padding-top: 4px !important; padding-bottom: 4px !important; padding-left: 5px !important; padding-right: 5px !important; border-bottom-color: #FFFFFF !important; border-bottom-width: 0 !important; }\n",
       " #ifavxjswhw .gt_subtitle { color: #333333 !important; font-size: 85% !important; font-weight: initial !important; padding-top: 3px !important; padding-bottom: 5px !important; padding-left: 5px !important; padding-right: 5px !important; border-top-color: #FFFFFF !important; border-top-width: 0 !important; }\n",
       " #ifavxjswhw .gt_heading { background-color: #FFFFFF !important; text-align: center !important; border-bottom-color: #FFFFFF !important; border-left-style: none !important; border-left-width: 1px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 1px !important; border-right-color: #D3D3D3 !important; }\n",
       " #ifavxjswhw .gt_bottom_border { border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; }\n",
       " #ifavxjswhw .gt_col_headings { border-top-style: solid !important; border-top-width: 2px !important; border-top-color: #D3D3D3 !important; border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; border-left-style: none !important; border-left-width: 1px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 1px !important; border-right-color: #D3D3D3 !important; }\n",
       " #ifavxjswhw .gt_col_heading { color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: normal !important; text-transform: inherit !important; border-left-style: none !important; border-left-width: 1px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 1px !important; border-right-color: #D3D3D3 !important; vertical-align: bottom !important; padding-top: 5px !important; padding-bottom: 5px !important; padding-left: 5px !important; padding-right: 5px !important; overflow-x: hidden !important; }\n",
       " #ifavxjswhw .gt_column_spanner_outer { color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: normal !important; text-transform: inherit !important; padding-top: 0 !important; padding-bottom: 0 !important; padding-left: 4px !important; padding-right: 4px !important; }\n",
       " #ifavxjswhw .gt_column_spanner_outer:first-child { padding-left: 0 !important; }\n",
       " #ifavxjswhw .gt_column_spanner_outer:last-child { padding-right: 0 !important; }\n",
       " #ifavxjswhw .gt_column_spanner { border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; vertical-align: bottom !important; padding-top: 5px !important; padding-bottom: 5px !important; overflow-x: hidden !important; display: inline-block !important; width: 100% !important; }\n",
       " #ifavxjswhw .gt_spanner_row { border-bottom-style: hidden !important; }\n",
       " #ifavxjswhw .gt_group_heading { padding-top: 8px !important; padding-bottom: 8px !important; padding-left: 5px !important; padding-right: 5px !important; color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: initial !important; text-transform: inherit !important; border-top-style: solid !important; border-top-width: 2px !important; border-top-color: #D3D3D3 !important; border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; border-left-style: none !important; border-left-width: 1px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 1px !important; border-right-color: #D3D3D3 !important; vertical-align: middle !important; text-align: left !important; }\n",
       " #ifavxjswhw .gt_empty_group_heading { padding: 0.5px !important; color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: initial !important; border-top-style: solid !important; border-top-width: 2px !important; border-top-color: #D3D3D3 !important; border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; vertical-align: middle !important; }\n",
       " #ifavxjswhw .gt_from_md> :first-child { margin-top: 0 !important; }\n",
       " #ifavxjswhw .gt_from_md> :last-child { margin-bottom: 0 !important; }\n",
       " #ifavxjswhw .gt_row { padding-top: 8px !important; padding-bottom: 8px !important; padding-left: 5px !important; padding-right: 5px !important; margin: 10px !important; border-top-style: solid !important; border-top-width: 1px !important; border-top-color: #D3D3D3 !important; border-left-style: none !important; border-left-width: 1px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 1px !important; border-right-color: #D3D3D3 !important; vertical-align: middle !important; overflow-x: hidden !important; }\n",
       " #ifavxjswhw .gt_stub { color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: initial !important; text-transform: inherit !important; border-right-style: solid !important; border-right-width: 2px !important; border-right-color: #D3D3D3 !important; padding-left: 5px !important; padding-right: 5px !important; }\n",
       " #ifavxjswhw .gt_stub_row_group { color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: initial !important; text-transform: inherit !important; border-right-style: solid !important; border-right-width: 2px !important; border-right-color: #D3D3D3 !important; padding-left: 5px !important; padding-right: 5px !important; vertical-align: top !important; }\n",
       " #ifavxjswhw .gt_row_group_first td { border-top-width: 2px !important; }\n",
       " #ifavxjswhw .gt_row_group_first th { border-top-width: 2px !important; }\n",
       " #ifavxjswhw .gt_striped { background-color: rgba(128,128,128,0.05) !important; }\n",
       " #ifavxjswhw .gt_table_body { border-top-style: solid !important; border-top-width: 2px !important; border-top-color: #D3D3D3 !important; border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; }\n",
       " #ifavxjswhw .gt_sourcenotes { color: #333333 !important; background-color: #FFFFFF !important; border-bottom-style: none !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; border-left-style: none !important; border-left-width: 2px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 2px !important; border-right-color: #D3D3D3 !important; }\n",
       " #ifavxjswhw .gt_sourcenote { font-size: 90% !important; padding-top: 4px !important; padding-bottom: 4px !important; padding-left: 5px !important; padding-right: 5px !important; text-align: left !important; }\n",
       " #ifavxjswhw .gt_left { text-align: left !important; }\n",
       " #ifavxjswhw .gt_center { text-align: center !important; }\n",
       " #ifavxjswhw .gt_right { text-align: right !important; font-variant-numeric: tabular-nums !important; }\n",
       " #ifavxjswhw .gt_font_normal { font-weight: normal !important; }\n",
       " #ifavxjswhw .gt_font_bold { font-weight: bold !important; }\n",
       " #ifavxjswhw .gt_font_italic { font-style: italic !important; }\n",
       " #ifavxjswhw .gt_super { font-size: 65% !important; }\n",
       " #ifavxjswhw .gt_footnote_marks { font-size: 75% !important; vertical-align: 0.4em !important; position: initial !important; }\n",
       " #ifavxjswhw .gt_asterisk { font-size: 100% !important; vertical-align: 0 !important; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"source\">source</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"text\">text</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"category\">category</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"species\">species</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">1.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat sat on the mat.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">2.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog sat on the mat.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">3.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat is meowing.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">4.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog is barking.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">5.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat is climbing a tree.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">6.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog is digging a hole.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_toy_corpus(os.path.join(source_path, 'toy.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_nltk_corpus_sources(source_path:str # path to location of sources for building corpora\n",
    "\t\t\t\t\t\t\t ):\n",
    "\t\"\"\" Get NLTK corpora as sources for development or testing Conc functionality. \"\"\"\n",
    "\n",
    "\ttry:\n",
    "\t\timport nltk\n",
    "\texcept ImportError as e:\n",
    "\t\traise ImportError('This function requires NLTK. To minimise requirements this is not installed by default. You can install NLTK with \"pip install nltk\"')\n",
    "\n",
    "\timport nltk\n",
    "\tnltk.download('gutenberg')\n",
    "\tnltk.download('brown')\n",
    "\tnltk.download('reuters')\n",
    "\tfrom nltk.corpus import gutenberg\n",
    "\tfrom nltk.corpus import reuters\n",
    "\tfrom nltk.corpus import brown\n",
    "\timport gzip\n",
    "\timport shutil\n",
    "\n",
    "\tdef clean_text(text):\n",
    "\t\t# to match words/punc that followed by /tags\n",
    "\t\tpattern = re.compile(r\"(\\S+)(/[^ ]+)\") # match non-space followed by / and non-space\n",
    "\t\treturn pattern.sub(r\"\\1\", text)\n",
    "\n",
    "\tif not os.path.exists(source_path):\n",
    "\t\tos.makedirs(source_path, exist_ok=True)\n",
    "\tif not os.path.exists(f'{source_path}/brown'):\n",
    "\t\tos.makedirs(f'{source_path}/brown', exist_ok=True)\n",
    "\tbrown_path = os.path.join(source_path, 'brown.csv')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in brown.fileids():\n",
    "\t\tcorpus_data.append([fileid, clean_text(brown.raw(fileid)), ', '.join(brown.categories(fileid))])\n",
    "\t\twith open(f'{source_path}/brown/{fileid}.txt', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(clean_text(brown.raw(fileid)))\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str), ('category', str)))\n",
    "\tdf.write_csv(brown_path)\n",
    "\n",
    "\twith open(brown_path, 'rb') as f_in:\n",
    "\t\twith gzip.open(f'{brown_path}.gz', 'wb') as f_out:\n",
    "\t\t\tshutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\tif os.path.exists(brown_path):\n",
    "\t\tos.remove(brown_path)\n",
    "\n",
    "\tgutenberg_path = os.path.join(source_path, 'gutenberg.csv')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in gutenberg.fileids():\n",
    "\t\tcorpus_data.append([fileid, clean_text(gutenberg.raw(fileid))])\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str)))\n",
    "\tdf.write_csv(gutenberg_path)\n",
    "\n",
    "\twith open(gutenberg_path, 'rb') as f_in:\n",
    "\t\twith gzip.open(f'{gutenberg_path}.gz', 'wb') as f_out:\n",
    "\t\t\tshutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\tif os.path.exists(gutenberg_path):\n",
    "\t\tos.remove(gutenberg_path)\n",
    "\n",
    "\treuters_path = os.path.join(source_path, 'reuters.csv')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in reuters.fileids():\n",
    "\t\tfileid_name = fileid.split('/')[1]\n",
    "\t\tcorpus_data.append([fileid_name, clean_text(reuters.raw(fileid)), ', '.join(reuters.categories(fileid))])\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str), ('categories', str)))\n",
    "\tdf.write_csv(reuters_path)\n",
    "\n",
    "\twith open(reuters_path, 'rb') as f_in:\n",
    "\t\twith gzip.open(f'{reuters_path}.gz', 'wb') as f_out:\n",
    "\t\t\tshutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\tif os.path.exists(reuters_path):\n",
    "\t\tos.remove(reuters_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "get_nltk_corpus_sources(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts for the Brown corpus from nltk can be used to test Conc functionality. The Reuters and Gutenberg corpora are also prepared by `get_nltk_corpus_sources`. Running the function will download the texts and save the texts as a .csv.gz files with columns: source and text. The Brown Corpus is also saved as .txt files to test the Corpus.build_from_texts method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_bnc_to_csv(source_path:str, # path to location of sources for building corpora, this is like to be a path ending with 'Texts'\n",
    "\t\t\t\t\t save_path:str, # path to save the csv\n",
    "\t\t\t\t\t output_filename:str = 'bnc.csv.gz' # name of the output file e.g. bnc.csv.gz or bnc-baby.csv.gz\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\" Converts BNC XML files, available via the British National Corpus, XML edition and the British National Corpus, Baby edition to a compressed CSV with title information retained. \"\"\"\n",
    "\n",
    "\timport os\n",
    "\timport gzip\n",
    "\timport csv\n",
    "\tfrom lxml import etree\n",
    "\tfrom glob import glob\n",
    "\n",
    "\tdef extract_text_and_title_from_bnc_xml(xml_path: str # path to the XML file\n",
    "\t\t\t\t\t\t\t\t\t\t ) -> tuple[str, str]: # title, text, bibliographic data, keywords, classcode, catref_targets\n",
    "\t\t\"\"\" Extract title (from sourceDesc) and text via sentences from a BNC XML file. \"\"\"\n",
    "\n",
    "\t\ttree = etree.parse(xml_path)\n",
    "\n",
    "\t\t# print(etree.tostring(tree.find('.//teiHeader'), pretty_print=True, encoding='unicode'))\n",
    "\t\ttitle_el = tree.find('.//title')\n",
    "\t\ttitle = title_el.text.strip() if title_el is not None and title_el.text else ''\n",
    "\n",
    "\t\tbibl_el = tree.find('.//bibl')\n",
    "\t\tif bibl_el is None:\n",
    "\t\t\tlogger.debug(f'No bibl element found in {xml_path}. This may not be a valid BNC XML file.')\n",
    "\t\t\tbibliographic_data = ''\n",
    "\t\telse:\n",
    "\t\t\tbibliographic_data = ''.join(bibl_el.itertext()) if bibl_el is not None else ''\n",
    "\t\tprofiledesc_el = tree.find('.//profileDesc')\n",
    "\t\tif profiledesc_el is None:\n",
    "\t\t\tlogger.debug(f'No profileDesc found in {xml_path}. This may not be a valid BNC XML file.')\n",
    "\t\t\tkeywords, classcode, catref_targets = '', '', ''\n",
    "\t\telse:\n",
    "\t\t\tkeywords_el = profiledesc_el.find('.//keywords')\n",
    "\t\t\tkeywords = ''.join(keywords_el.itertext()) if keywords_el is not None else ''\n",
    "\t\t\tif keywords.strip() == '(none)':\n",
    "\t\t\t\tkeywords = ''\n",
    "\t\t\tclasscode_el = profiledesc_el.find('.//classCode')\n",
    "\t\t\tclasscode = ''.join(classcode_el.itertext()) if classcode_el is not None else ''\n",
    "\t\t\tcatref_el = profiledesc_el.find('.//catRef')\n",
    "\t\t\tcatref_targets = ''\n",
    "\t\t\tif catref_el is not None:\n",
    "\t\t\t\tcatref_targets = catref_el.attrib.get('targets', '') \n",
    "\n",
    "\t\tsentences = []\n",
    "\t\tfor s in tree.xpath('.//s'):\n",
    "\t\t\ttokens = [t.text for t in s.iter() if t.text and t.text.strip()]\n",
    "\t\t\tif tokens:\n",
    "\t\t\t\tsentences.append(''.join(tokens))\n",
    "\t\tif len(sentences) == 0:\n",
    "\t\t\tlogger.warning(f'No sentences found in {xml_path}. This may not be a valid BNC XML file.')\n",
    "\n",
    "\t\treturn title, '\\n'.join(sentences), bibliographic_data, keywords, classcode, catref_targets\n",
    "\n",
    "\toutput_path = os.path.join(save_path, output_filename)\n",
    "\txml_files = glob(os.path.join(source_path, '**', '*.xml'), recursive=True)\n",
    "\tos.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\twith gzip.open(output_path, 'wt', encoding='utf-8', newline='') as csvfile:\n",
    "\t\twriter = csv.writer(csvfile)\n",
    "\t\twriter.writerow(['source', 'title', 'bibliographic_data', 'keywords', 'classcode', 'catref_targets', 'text'])\n",
    "\n",
    "\t\tfor xml_file in xml_files:\n",
    "\t\t\ttry:\n",
    "\t\t\t\ttitle, text, bibliographic_data, keywords, classcode, catref_targets = extract_text_and_title_from_bnc_xml(xml_file)\n",
    "\t\t\t\twriter.writerow([os.path.relpath(xml_file, source_path), title, bibliographic_data, keywords, classcode, catref_targets, text])\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tlogger.warning(f\"Error processing {xml_file}: {e}\")\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# EXPERIMENTING WITH GETTING METADATA FROM BNC - teiheader contains metadata both in text and attributes\n",
    "# very rough parsing below to json\n",
    "# ALSO  - see http://www.natcorp.ox.ac.uk/archive/worldURG/urg.pdf\n",
    "# content of each text is marked up with paragraphs etc - consider reformatting using these tags to get nicer flowing text\n",
    "# see sections on spoken and written texts\n",
    "\n",
    "# from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "# from bs4 import BeautifulSoup\n",
    "# import json\n",
    "\n",
    "# def bs4_to_dict(tag):\n",
    "#     node = {}\n",
    "#     # Add attributes\n",
    "#     if tag.attrs:\n",
    "#         node.update(('@' + k, v) for k, v in tag.attrs.items())\n",
    "#     # Add children or text\n",
    "#     children = [child for child in tag.children if child.name]\n",
    "#     if children:\n",
    "#         for child in children:\n",
    "#             child_dict = bs4_to_dict(child)\n",
    "#             if child.name in node:\n",
    "#                 if not isinstance(node[child.name], list):\n",
    "#                     node[child.name] = [node[child.name]]\n",
    "#                 node[child.name].append(child_dict)\n",
    "#             else:\n",
    "#                 node[child.name] = child_dict\n",
    "#     if tag.string and tag.string.strip():\n",
    "#         node['#text'] = tag.string.strip()\n",
    "#     return node\n",
    "\n",
    "# path_to_bnc_texts = f'{source_path}bnc-baby-xml/download/Texts'\n",
    "# corpus = BNCCorpusReader(root=path_to_bnc_texts, fileids=r'.*\\.xml')\n",
    "\n",
    "# i = 0\n",
    "# for fileid in corpus.fileids():\n",
    "#     # open using lxml\n",
    "#     print(f'Processing {fileid}')\n",
    "#     raw_xml = open(os.path.join(path_to_bnc_texts, fileid), 'r', encoding='utf-8').read()\n",
    "\n",
    "#     soup = BeautifulSoup(raw_xml, 'lxml')\n",
    "#     item = soup.find('teiheader')\n",
    "\n",
    "#     item_dict = {item.name: bs4_to_dict(item)}\n",
    "#     print(json.dumps(item_dict, indent=2))\n",
    "\n",
    "\n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1994 version of the British National Corpus (BNC) is available from the [Oxford Text Archive](https://llds.ling-phil.ox.ac.uk/llds/xmlui/discover?filtertype=author&filter_relational_operator=equals&filter=BNC+Consortium). The `parse_bnc_to_csv` function assumes you have downloaded and unziped the files. You need to specify the directory containing the XML files for the texts. This will be a path ending with `Texts`. Conc does not provide a way to download the BNC zip files directly. You need to go to the Oxford Text Archive and read the notes about restricted use. Note: this function was previously using NLTK's BNC parser, but this has been rewritten for lxml, which is much (2x) faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# LEAVE COMMENTED OUT FOR CI AS BNC NOT AVAILABLE - takes about 40 seconds to run\n",
    "# parse_bnc_to_csv(source_path = f'{source_path}bnc-baby-xml/download/Texts', save_path = source_path, output_filename = 'bnc-baby.csv.gz')\n",
    "# pl.scan_csv(os.path.join(source_path, 'bnc-baby.csv.gz')).head(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# LEAVE COMMENTED OUT FOR CI AS BNC NOT AVAILABLE \n",
    "# parse_bnc_to_csv(source_path = f'{source_path}bnc-xml/download/Texts', save_path = source_path, output_filename = 'bnc.csv.gz')\n",
    "# pl.scan_csv(os.path.join(source_path, 'bnc.csv.gz')).head(2).select().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# LEAVE COMMENTED OUT FOR CI AS BNC NOT AVAILABLE - takes about 40 seconds to run\n",
    "# corpus_name = 'Baby BNC'\n",
    "# corpus_description = 'British National Corpus, Baby edition. Please use the following text to cite this item or export to a predefined format: BNC Consortium, 2007, British National Corpus, Baby edition, Literary and Linguistic Data Service, http://hdl.handle.net/20.500.14106/2553.'\n",
    "# metadata_columns = ['source', 'title']\n",
    "# corpus = Corpus(name = corpus_name, description = corpus_description).build_from_csv(source_path = f'{source_path}bnc-baby.csv.gz', text_column='text', metadata_columns=metadata_columns, save_path = save_path, standardize_word_token_punctuation_characters = True)\n",
    "# corpus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# LEAVE COMMENTED OUT FOR CI AS BNC NOT AVAILABLE - takes about 10 minutes to run\n",
    "# corpus_name = 'BNC'\n",
    "# corpus_description = 'British National Corpus, XML edition. Please use the following text to cite this item or export to a predefined format: BNC Consortium, 2007, British National Corpus, XML edition, Literary and Linguistic Data Service, http://hdl.handle.net/20.500.14106/2554. '\n",
    "# metadata_columns = ['source']\n",
    "# corpus = Corpus(name = corpus_name, description = corpus_description).build_from_csv(source_path = f'{source_path}bnc.csv.gz', text_column='text', metadata_columns=metadata_columns, save_path = save_path, standardize_word_token_punctuation_characters = True)\n",
    "# corpus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# LEAVE COMMENTED OUT FOR CI AS BNC NOT AVAILABLE\n",
    "# listcorpus = ListCorpus().build_from_corpus(source_corpus_path = f'{save_path}bnc.corpus', save_path = save_path)\n",
    "# listcorpus = ListCorpus().build_from_corpus(source_corpus_path = f'{save_path}baby-bnc.corpus', save_path = save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_garden_party(source_path: str, #path to location of sources for building corpora\n",
    "\t\t\t\t\t create_archive_variations: bool = False # create .tar and .tar.gz files for dev/testing (leave False if you just want the zip)\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\" Get corpus of The Garden Party by Katherine Mansfield for development of Conc and testing Conc functionality. \"\"\"\n",
    "\n",
    "\tpath = 'https://github.com/ucdh/scraping-garden-party/raw/master/garden-party-corpus.zip'\n",
    "\n",
    "\timport requests\n",
    "\ttry:\n",
    "\t\timport requests\n",
    "\texcept ImportError as e:\n",
    "\t\traise ImportError('This function requires the requests library. To minimise requirements this is not installed by default. You can install requests with \"pip install requests\"')\n",
    "\n",
    "\tif not os.path.exists(source_path):\n",
    "\t\tos.makedirs(source_path)\n",
    "\n",
    "\tr = requests.get(path)\n",
    "\twith open(f'{source_path}/garden-party-corpus.zip', 'wb') as f:\n",
    "\t\tf.write(r.content)\n",
    "\n",
    "\tif create_archive_variations: \t# converting to .tar and tar.gz files for testing\n",
    "\t\timport zipfile\n",
    "\t\twith zipfile.ZipFile(f'{source_path}/garden-party-corpus.zip', 'r') as z:\n",
    "\t\t\tz.extractall(f'{source_path}/garden-party-corpus')\n",
    "\t\timport shutil # make tar.gz\n",
    "\t\tshutil.make_archive(f'{source_path}/garden-party-corpus', 'gztar', f'{source_path}/garden-party-corpus')\n",
    "\t\tshutil.make_archive(f'{source_path}/garden-party-corpus', 'tar', f'{source_path}/garden-party-corpus')\n",
    "\t\tshutil.rmtree(f'{source_path}/garden-party-corpus')\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_garden_party` function downloads a zip file of an example corpus based on Katherine Mansfield short stories. This function creates a .tar and a .tar.gz version of the texts for testing Corpus build methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "get_garden_party(source_path, create_archive_variations = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create large corpora for development and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_large_dataset(source_path: str #path to location of sources for building corpora\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\" Get 1m rows of https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset for testing. \"\"\"\n",
    "\timport gzip\n",
    "\timport shutil\n",
    "\t\n",
    "\tpath = f'{source_path}/us-congressional-speeches-subset-1m.csv'\n",
    "\tdf = pl.read_parquet('hf://datasets/Eugleo/us-congressional-speeches-subset/data/train-*.parquet')\n",
    "\tdf.sample(1000000).select(['speech_id', 'date', 'speaker', 'chamber', 'state', 'text']).write_csv(path)\n",
    "\tdel df\n",
    "\n",
    "\twith open(path, 'rb') as f_in:\n",
    "\t\twith gzip.open(f'{path}.gz', 'wb') as f_out:\n",
    "\t\t\tshutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\tif os.path.exists(path):\n",
    "\t\tos.remove(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# get_large_dataset(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Define the chunk size\n",
    "# chunk_size = 100_000  # Adjust based on your memory constraints\n",
    "\n",
    "# # Lazily load the CSV file\n",
    "# df = pl.scan_csv(f'{source_path}us-congressional-speeches-subset-1m.csv.gz')\n",
    "\n",
    "# # Add the new column 'is_empty'\n",
    "# df = df.with_columns(\n",
    "#     (pl.col('text').str.strip_chars().eq('')).alias('is_empty')\n",
    "# )\n",
    "\n",
    "# # get length of is_empty where True\n",
    "# count = df.filter(pl.col(\"is_empty\") == True).collect().height\n",
    "# print(f\"Number of empty rows: {count}\")\n",
    "\n",
    "# any empty?\n",
    "#len(df[df['text'].is_null()])\n",
    "\n",
    "# get distribution of date (by year), speaker, chamber, state\n",
    "# dates are in iso format - extract year and summarize\n",
    "# df = df.with_columns(pl.col('date').str.slice(0, 4).alias('year'))\n",
    "# df.group_by('year').agg(pl.count('year').alias('count')).sort('year', descending=True).head(10).collect()\n",
    "\n",
    "# #df.group_by('speaker').agg(pl.count('speaker').alias('count')).sort('count', descending=True).head(20)\n",
    "# #df.group_by('chamber').agg(pl.count('chamber').alias('count')).sort('count', descending=True).head(20)\n",
    "# df.group_by('state').agg(pl.count('state').alias('count')).sort('count', descending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_large_dataset_sizes(source_path: str, #path to location of sources for building corpora\n",
    "\t\t\t\t\t\tsizes: list = [10000, 100000, 200000, 500000] # list of sizes for test data-sets\n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Create datasets of different sizes from data source retrieved by get_large_dataset for testing. \"\"\"\n",
    "\timport gzip\n",
    "\timport shutil\n",
    "\n",
    "\tfor max_i in sizes:\n",
    "\t\tmax_i_label = int(max_i / 1000)\n",
    "\t\tpath = f'{source_path}/us-congressional-speeches-subset-{max_i_label}k.csv'\n",
    "\t\tdf = pl.read_csv(f'{source_path}/us-congressional-speeches-subset-1m.csv.gz')\n",
    "\t\tdf.sample(max_i).write_csv(path)\n",
    "\t\tlogger.info(f'Creating dataset of {max_i_label}k rows')\n",
    "\n",
    "\t\twith open(path, 'rb') as f_in:\n",
    "\t\t\twith gzip.open(f'{path}.gz', 'wb') as f_out:\n",
    "\t\t\t\tshutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\t\tif os.path.exists(path):\n",
    "\t\t\tos.remove(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# create_large_dataset_sizes(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build sample corpora\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def build_sample_corpora(\n",
    "\t\tsource_path:str, # path to folder with corpora\n",
    "\t\tsave_path:str, # path to save corpora\n",
    "\t\tforce_rebuild:bool = False # force rebuild of corpora, useful for development and testing\n",
    "\t\t):\n",
    "\t\"\"\"Build all test corpora from source files.\"\"\"\n",
    "\n",
    "\tif force_rebuild:\n",
    "\t\timport shutil\n",
    "\n",
    "\tcorpora = {}\n",
    "\tcorpora['toy'] = {'name': 'Toy Corpus', 'slug': 'toy', 'description': 'Toy corpus is a very small dataset for testing and library development. ', 'extension': '.csv.gz', 'metadata_columns': ['source', 'category', 'species']}\n",
    "\tcorpora['brown'] = {'name': 'Brown Corpus', 'slug': 'brown', 'description': 'A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.', 'extension': '.csv.gz', 'metadata_columns': ['source', 'category']}\n",
    "\tcorpora['reuters'] = {'name': 'Reuters Corpus', 'slug': 'reuters', 'description': 'Reuters corpus (Reuters-21578, Distribution 1.0). \"The copyright for the text of newswire articles and Reuters annotations in the Reuters-21578 collection resides with Reuters Ltd. Reuters Ltd. and Carnegie Group, Inc. have agreed to allow the free distribution of this data *for research purposes only*. If you publish results based on this data set, please acknowledge its use, refer to the data set by the name (Reuters-21578, Distribution 1.0), and inform your readers of the current location of the data set.\" https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.', 'extension': '.csv.gz', 'metadata_columns': ['source', 'categories']}\n",
    "\tcorpora['gutenberg'] = {'name': 'Gutenberg Corpus', 'slug': 'gutenberg', 'description': 'Project Gutenberg Selections NLTK Corpus. Source: https://gutenberg.org/. Public domain. This version downloaded via NLTK https://www.nltk.org/nltk_data/.', 'extension': '.csv.gz', 'metadata_columns': ['source']}\n",
    "\tcorpora['garden-party-corpus'] = {'name': 'Garden Party Corpus', 'slug': 'garden-party', 'description': 'A corpus of short stories from The Garden Party: and Other Stories by Katherine Mansfield. Texts downloaded from Project Gutenberg https://gutenberg.org/ and are in the public domain. The text files contain the short story without the title. https://github.com/ucdh/scraping-garden-party', 'extension': '.zip', 'metadata_columns': None}\n",
    "\n",
    "\tif not os.path.exists(source_path):\n",
    "\t\tos.makedirs(source_path)\n",
    "\tif not os.path.exists(save_path):\n",
    "\t\tos.makedirs(save_path)\n",
    "\n",
    "\tif not os.path.exists(f'{source_path}toy.csv.gz'):\n",
    "\t\tcreate_toy_corpus_sources(source_path)\n",
    "\n",
    "\tif not os.path.exists(f'{source_path}garden-party-corpus.zip'):\n",
    "\t\tget_garden_party(source_path)\n",
    "\n",
    "\tif not os.path.exists(f'{source_path}brown.csv.gz'):\n",
    "\t\tget_nltk_corpus_sources(source_path)\n",
    "\n",
    "\tfor corpus_name, corpus_details in corpora.items():\n",
    "\t\tif force_rebuild and os.path.isdir(f'{save_path}{corpus_details[\"slug\"]}.corpus'):\n",
    "\t\t\tshutil.rmtree(f'{save_path}{corpus_details[\"slug\"]}.corpus', ignore_errors=True)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tcorpus = Corpus().load(f\"{save_path}{corpus_details['slug']}.corpus\")\n",
    "\t\texcept FileNotFoundError:\n",
    "\n",
    "\t\t\tif 'csv' in corpus_details['extension']:\n",
    "\t\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_csv(source_path = f'{source_path}{corpus_name}.csv.gz', text_column='text', metadata_columns=corpus_details['metadata_columns'], save_path = save_path)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_files(source_path = f'{source_path}{corpus_name}{corpus_details[\"extension\"]}', save_path = save_path)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise e\n",
    "\t\tdel corpus\n",
    "\t\n",
    "\tif force_rebuild and os.path.isdir(f'{save_path}brown.listcorpus'):\n",
    "\t\tshutil.rmtree(f'{save_path}brown.listcorpus', ignore_errors=True)\n",
    "\ttry:\n",
    "\t\tlistcorpus = ListCorpus().load(f\"{save_path}brown.listcorpus\")\n",
    "\texcept FileNotFoundError:\n",
    "\t\tlistcorpus = ListCorpus().build_from_corpus(source_corpus_path = f'{save_path}brown.corpus', save_path = save_path)\n",
    "\tdel listcorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_sample_corpora` function downloads sources and creates sample corpora for development and testing for releases. These datasets are a good way to get started working with Conc. Sample corpora available are:\n",
    "\n",
    "* Brown Corpus (via NLTK)  \n",
    "* Gutenberg Corpus (via NLTK)  \n",
    "* Reuters Corpus (via NLTK)  \n",
    "* Garden Party Corpus (Katherine Mansfield short stories)  \n",
    "* Toy Corpus (6 very short texts for testing only)  \n",
    "* Brown List Corpus (a lightweight ListCorpus version of the Brown Corpus for use with keywords functionality)\n",
    "\n",
    "After installing Conc you can invoke this function from the command line to download and build the sample corpora:\n",
    "```sh\n",
    "conc_build_sample_corpora path/to/save/sources path/to/save/corpora\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `build_sample_corpora` was accessible via conc.corpus as `build_test_corpora` up to version 0.1.1. This functionality is only accessible from `conc.corpora` now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from conc.core import set_logger_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sample_corpora(source_path, save_path, force_rebuild=False) # must be left as False after dev, otherwise could destroy corpora mid test in CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
