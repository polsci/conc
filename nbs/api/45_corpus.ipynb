{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus\n",
    "\n",
    "> Create a conc corpus.\n",
    "- toc: false\n",
    "- page-layout: full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from great_tables import GT\n",
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "from spacy.attrs import ORTH, LOWER, SPACY # May extend to POS, TAG, SENT_START, LEMMA\n",
    "import string\n",
    "from fastcore.basics import patch\n",
    "import time\n",
    "from slugify import slugify\n",
    "import msgspec # tested against orjson - with validation was faster, without around the same\n",
    "import unicodedata\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc import __version__\n",
    "from conc.core import logger, CorpusMetadata, PAGE_SIZE, EOF_TOKEN_STR, ERR_TOKEN_STR, REPOSITORY_URL, DOCUMENTATION_URL, CITATION_STR, PYPI_URL\n",
    "from conc.result import Result\n",
    "from conc.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from conc.core import set_logger_state, spacy_attribute_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "polars_conf = pl.Config.set_tbl_hide_column_data_types(True)\n",
    "polars_conf = pl.Config.set_tbl_hide_dataframe_shape(True)\n",
    "polars_conf = pl.Config.set_tbl_rows(50)\n",
    "polars_conf = pl.Config.set_tbl_width_chars(300)\n",
    "polars_conf = pl.Config.set_fmt_str_lengths(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "_RE_PUNCT = re.compile(r\"^[^\\s^\\w^\\d]$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "NOT_DOC_TOKEN = -1\n",
    "INDEX_HEADER_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "PUNCTUATION_STRINGS = ''.join(set(list(string.punctuation) + \n",
    "                                [chr(i) for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)).startswith(\"P\")] + \n",
    "                                [chr(i) for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)).startswith(\"Sc\")]\n",
    "\t\t\t\t\t\t\t\t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = f'{os.environ.get(\"HOME\")}/data/'\n",
    "save_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from conc.core import create_toy_corpus_sources, get_garden_party, get_nltk_corpus_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# files that use test corpora need build_test_corpora here - not needed here\n",
    "if not os.path.exists(source_path):\n",
    "    os.makedirs(source_path)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "if not os.path.exists(f'{source_path}toy.csv.gz'):\n",
    "    create_toy_corpus_sources(source_path)\n",
    "\n",
    "if  not os.path.exists(f'{source_path}garden-party-corpus.zip'):\n",
    "    get_garden_party(source_path)\n",
    "\n",
    "if not os.path.exists(f'{source_path}brown.csv.gz'):\n",
    "    get_nltk_corpus_sources(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Corpus:\n",
    "\t\"\"\"Represention of text corpus, with methods to build, load and save a corpus from a variety of formats and to work with the corpus data.\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, \n",
    "\t\t\t\tname: str = '', # name of corpus\n",
    "\t\t\t\tdescription: str = '' # description of corpus\n",
    "\t\t\t\t):\n",
    "\t\t# information about corpus\n",
    "\t\tself.name = name\n",
    "\t\tself.description = description\n",
    "\t\tself.slug = None\n",
    "\n",
    "\t\t# conc version that built the corpus\n",
    "\t\tself.conc_version = None\n",
    "\t\t\n",
    "\t\t# paths\n",
    "\t\tself.corpus_path = None\n",
    "\t\tself.source_path = None\n",
    "\n",
    "\t\t# settings\n",
    "\t\tself.SPACY_MODEL = None\n",
    "\t\tself.SPACY_MODEL_VERSION = None\n",
    "\t\tself.SPACY_EOF_TOKEN = None # set below as nlp.vocab[EOF_TOKEN_STR].orth in build or through load  - EOF_TOKEN_STR starts with space so eof_token can't match anything from corpus\n",
    "\t\tself.EOF_TOKEN = None\n",
    "\n",
    "\t\t# special token ids\n",
    "\t\tself.punct_tokens = None\n",
    "\t\tself.space_tokens = None\n",
    "\n",
    "\t\t# metadata for corpus\n",
    "\t\tself.document_count = None\n",
    "\t\tself.token_count = None\n",
    "\t\tself.unique_tokens = None\n",
    "\n",
    "\t\tself.word_token_count = None\n",
    "\t\tself.unique_word_tokens = None\n",
    "\n",
    "\t\tself.date_created = None\n",
    "\n",
    "\t\t# token data\n",
    "\t\tself.tokens = None\n",
    "\t\t# self.orth_index = None\n",
    "\t\t# self.lower_index = None\n",
    "\n",
    "\t\t# lookup mapping doc_id to every token in doc\n",
    "\t\t# self.token2doc_index = None\n",
    "\n",
    "\t\t# lookups to get token string or frequency \n",
    "\t\tself.vocab = None\n",
    "\t\t# self.frequency_lookup = None\n",
    "\n",
    "\t\t# offsets for each document in token data\n",
    "\t\t# self.offsets = None\n",
    "\n",
    "\t\t# punct and space positions in token data\n",
    "\t\t# self.punct_positions = None\n",
    "\t\t# self.space_positions = None\n",
    "\t\tself.puncts = None\n",
    "\t\tself.spaces = None\n",
    "\n",
    "\t\t# metadata for each document\n",
    "\t\tself.metadata = None\n",
    "\n",
    "\t\t# lookups to get spacy tokenizer or internal ids\n",
    "\t\t# self.original_to_new = None\n",
    "\t\t# self.new_to_original = None\n",
    "\t\t\n",
    "\t\t# temporary data used when processing text, not \n",
    "\t\t# \n",
    "\t\t# \n",
    "\t\t# \n",
    "\t\t# d to disk permanently on save\n",
    "\t\t\n",
    "\t\t# self.frequency_table = None\n",
    "\t\tself.ngram_index = {}\n",
    "\t\tself.results_cache = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and save a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_spacy_model(self: Corpus,\n",
    "                model: str = 'en_core_web_sm', # spacy model to use for tokenization\n",
    "\t\t\t\tversion: str|None = None # version of spacy model expected, if mismatch will raise a warning\n",
    "\t\t\t\t):\n",
    "\ttry:\n",
    "\t\tself._nlp = spacy.load(model)\n",
    "\t\tself._nlp.disable_pipes(['parser', 'ner', 'lemmatizer', 'tagger', 'senter', 'tok2vec', 'attribute_ruler'])\n",
    "\t\tself._nlp.max_length = 10_000_000 # set max length to a large number to avoid issues with long documents\n",
    "\texcept OSError as e:\n",
    "\t\tlogger.error(f'Error loading model {model}. You need to run python -m spacy download YOURMODEL to download the model. See https://spacy.io/models for available models.')\n",
    "\t\traise e\n",
    "\t\n",
    "\tif version is not None:\n",
    "\t\tif self._nlp.meta['version'] != version:\n",
    "\t\t\tlogger.warning(f'Spacy model version mismatch: expecting {version}, got {self._nlp.meta[\"version\"]}. This may cause issues with tokenization.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _process_punct_positions(self: Corpus):\n",
    "\t\"\"\" Process punctuation positions in token data and populates punct_tokens and punct_positions. \"\"\"\n",
    "\n",
    "\tself.punct_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip(PUNCTUATION_STRINGS) == ''}.keys()))\n",
    "\tpunct_mask = np.isin(self.lower_index, self.punct_tokens) # faster to retrieve with isin than where\n",
    "\tself.punct_positions = np.nonzero(punct_mask)[0] # storing this as smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _process_space_positions(self: Corpus):\n",
    "\t\"\"\" Process whitespace positions in token data and populates space_tokens and space_positions. \"\"\"\n",
    "\n",
    "\tself.space_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip() == ''}.keys()))\n",
    "\tspace_mask = np.isin(self.lower_index, self.space_tokens) \t# faster to retrieve with isin than where\n",
    "\tself.space_positions = np.nonzero(space_mask)[0] # storing this as smaller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conc defines a punctuation token as a token consisting only of punctuation characters. Punctuation characters are defined by combining the Python `string.punctuation` characters with unicode characters categorised as punctuation (i.e. [unicode characters with general category starting with P](https://en.wikipedia.org/wiki/Unicode_character_property#General_Category)) or currency symbols (i.e. [general category Sc](https://en.wikipedia.org/wiki/Unicode_character_property#General_Category)). This means, for example, that various versions of a dashes or quotation marks can be identified as punctuation. This also means that any emoticons that are based on sequences of punctuation characters, like :), will be defined as a punctuation token. To access reporting on punctuation is still possible in Conc reports using the relevant parameters. There are still many unicode symbol characters that are not defined as punctuation by Conc. This may change in future versions of Conc, including the ability to define punctuation strings or exceptions. Any changes will be documented.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890\n",
      "ù™ã·õ≠Ôºõ‚ù∞‚ÅúÍ´∞Ôºª„Äé·üòë™ö„Äõë±Å‚Ä†Ôπñ‚∏£ﬂø‹ãÔΩ†ëëè‚≥ø„Ç†ﬂæ‚Ç™ëóíëÅâ·•Ö‚∏ç‚πú`ëóáÔ∏¥‡ºäëÉÄ‡∑¥‚∏ÅÔº†‚¶ñ’öÍßâ÷äÿåëô†ñ∫ö‡ß≥;ëëö‡ßΩÔ∏Ω‚¶ï‡ß≤Í£é‚Ä¢-ñ∫ó·™§êéüë©ÉÍßäëô™‚∏¥‡ºî◊≥‡ºáëÖµ‚∏∑‡ºè‚∏°‚¶é¬ß‡≤Ñ‚∏åë™ü„ÄåÔπè‚ÅëëóÉÿçÿâêΩï·≥ìÔ∏ø‚ÅñÔπú‚åâ‚Ç∑ëàπ‚üÆëúºÔπí‚πô„ÄÅ|‚Äê‚Äµ„Äî‚¶ä·™™‚∏Ü‚ÇçÔ∏±Ôπïëøü‡øë‹ÇÔπ°ë±Ñëëã‚¶èëàª‚πÄ÷æ:·™†‚¶Ö‚µ∞·≠ùÍ°µ]·†àÔºÅ·∞Ωëô£‹Ö‚Äñ&‚üÜ‚ù≤·™£‚ÅÉÔπÇ·Éª¬´·üô‚Äíñ´µ‚Çéê©ñ·ç•‚ÄΩ‚∏•‚∏è·≥ÜëÇªëøø‚Ç£Ôπù·üö·ç°‚≥∫íë∞Íßå‚∏´ÍßÜ·†á‡†ª‚∏∏Íôæ‡•∞‚Åï‚πÑê¨ø·öõ‡øîÔπÉëø†·üñ‡øö‚Äö‚ù™‡øêëô©ê¨∫‚Ä∞íø≤‚∏à·™¢>Ô∏µ‚Åç‚ù®‡∏ø‚Äô‚∏æÍßÑ_·ç¢‚πï‚ßº‹áŒá‚ÄìÍßü‚∏ø‚∏ÄêÆô‚Ç¨/„ÄüÍ©û·™©‡æÖ‚∏≥ëúΩ‡ºâ<ë™°ëô¶ñ≠Ñ·≠ûñ©Æê´µëª∏ÔΩõ[Ôø•û•û‚Ç∏ÔºΩ}‡øì‚Åâ·†Ä‚¶É·ç£ÔπÄ‡†ºÔπûëóçﬂ∑‚≥ªÔ∏íëóï‚Çπ‚Çæ◊ÉëÅã‚∏µñ¨∏ê¨π‚Äº„ÄΩÍ´üÔø†·≠ú+ëóã‚ÅÅ‚Åé‚åä·öúÔºÇ¬¢ëÖÇ‚Ç≥·õ´‚ÇØ‚å©„Äã’üëô•êïØëÇæ‚∏õêæáÔπÖÔΩ§¬∂ëáç¬§◊Üëëù‚Åì‚πòÔ∏∑Ÿ™ëøûÍ†∏’ûÍ´±êæà‚πõ·üîÍõµÔ∏≥ëÅà‡†æ‡•§ëàΩñ¨πñ¨∫·Åé„Äò‚ßΩÔ∏πíë≤‚Ä§ëáüë±ÇëóÅ‚Äú‡©∂ë±∞„ÄÇû≤∞(‚πâ‚∏äÍ°∑„Äà‚¶ó·ú∂·†Ñ·Øº‚∏ë‚∏¨ÔπöÔ∏≤·ØøÔπ†Ô¥æ¬£‚Ä±ëô°Ôπô‚∏™‚∏ó‚Åô‚≥π‚ùµë©ÑÔ∏æêæÜëôÉÔπë@‚∏Ñ·Åç\\ëÇø‚πéëóÇë™õ÷è‚ÇºëøùÔºâ‡ºã‚∏Ç‚ÇÆ‚Ç¥‚πÖÔºöë±É‚∏≤‚Ä∫‚∏ú‚πóÍ£º‡†∫Ô∏ïﬂπ‚Åõ‡†¥Ô∏∏‚ù≠ÔºøÍØ´‚∏òëóÑ‚∏ô·™°ëáÜ‚πè¬•·†Ç‚ÅóëóéÔø°·≥É„Äè‚ü≠ë©ÇÔπâÍ°∂·†É‚∏±·†Å‚πöÕæÔπçëôß‚∏âëÉÅ‚Äû‡°û‚∏¶‚∏û‚∏ãÍìæ‡†π÷â‚Äπ‚Åû‡πöÍ£∏ÔΩ°ëÖÉ.Í°¥Ôπîê§ø,ë©Ö‚Ä∂êÆú‚ü®‡†≥ù™äñø¢Ô¥øÔºåÔºÑ·≠üëìÜê´≤êÑÇ·™´ê©í‚∏π·≠Ωëª∑Ô∏∞‡ººëóÖ·∞øê§ü‚∏ï‚πù‚Åä‡†∏‚Äó‚ü´ñ∫ô‹à·≥ÑÍòè„Äö‚Ç±#ÍßÖ‚Ä•Ô∏êêΩò‚πÉ‚Ä£ù™à‚ßö‚∏®·∞ªÔπ™ëô¢‚¶ìÔººëÅáÔπáÍ£èíø±ûãø¬∑‚Ç∫‚Ç®Í©ú‚πåÔπÜ‡ºÖêæâ‚ÇΩ‚Äò‚ÅêÔ∏òÔ∏ë‚Ç•ÔπÅù™âﬂ∏·≠õ‚¶í‡ºÜëÇºëÅä·ç†ëóë‚ÅåÔ∏∫‚¶ç‚∏áêÆõ‚¶ò‚πãê©óë©ÅëëåÔΩü·™¨Ÿ´íë¥ë•Ö‚ÇµÔ∏îÔπü‚ù≥êÑÅëëõ‚ü™ÔπÑê°óê´¥ëä©‚Åæ‚∏∞‚¶à‚Ç≠ëóÜ‚πî‚ßôëáù‚ßõ‚ù¨‚ÅèêΩóë™úñ¨ª‡´±‚Ç©‚Äæ‚∏öÍßç‚ÅÇ·®ü‡ßª{ëóèê©øñ©Ø‡Øπ‡ºåÍßã·ç§‚Çß‚Ç¢?ë©Äëà∏‚Åù„Äù‡ºàÔπå‚Äëëàº‚≥æ‹åëôÇ‚Ä¶íë≥„ÄâÍßá‹ç‚¶ãê´±„Äôëáà‡º∫‚ü¨‚¶Ü‡†≤‚∏ºëÖÄë©Ü‚ù©Ô∏ñ‡†±êΩñëÅå‚¶êÔ∏∂ê´∞‚ÅöÔπê‚ÅΩÍ©ùê©î‚∏Æê´∂‚üß„Äû‚Ä∏‚¶î„Äï‚Ç∂‚ù¥ëóê·†ä‚∏í‚∏üÍõ∂‚ÅÄëô®ê´≥„ÄñÿüÍô≥„Äó‚Åò‚Äøíë±ë±Ö‡±∑‚Äß‚∏é‚∏îõ≤üÍ£∫·≠æ‚Ä¥ê©ì„Äç·≥áëëçë•ÜÔπõ‚Ç†‚ÅáÔπ©ÔºÖÔΩ¢¬ªëóâ·†Ö‚Ç¶Íßà‚πç‚ùÆÍßÉù™áÔ∏ô·™≠·êÄêÑÄëôÅ‡ºí‚Ä≥‡†∞·±ø‚üØ‚å™‚πíÍßû‚∏ê‚πà‹É◊ÄŸ≠Ôπó‡øô·Åå„Äú·∞º‚ÄùÔ∑ºëô§‡ºë‚¶ëÍõ≤‚Ä≤‚∏ßë•Ñ‡†µÔ∏º‚Åã„Äê‚ÅîÔΩ£·Øæÿù‚∏ùëà∫·†Üëëé‚¶Ñ‚Åà·∞æ^ÔΩùÔºüê©ê‚ÇªÍ´ûëÖÅ‡†∂·üõ·üï‚ÉÄÔø¶‚∏©‚πÜÍòé·ôÆÿû‚ÅÜ·®ûÔπã„Éª·úµ‹Ñ·ç¶ÔºÉ‚πä‚πÇ‚∏∫‹Ä‚≥º‚∏ñ‚åãë±±Í£π·†â‚Äõ‚∏†Íõ∑%ëóîÔ∏ì‚¶åÔºè¬ø’ù‚ù´·™®‚¶âÔºçñ∫ò‚Ç°·•Ñ¬°Ôπ®‚Ç§Ôπ£ñ¨∑‡øíÍìøê¨ªŸ¨‚∏∂·≥Ä‡´∞ëÖ¥Í§Æê∫≠Í©ü‚Ç∞‡†Ωû•üÍßÅÔ∏óê©ò„ÄÉÍòç~‡ºçÍõ¥Ô∏ª=‡ºê‚ù±ë™†·ØΩë™¢ëáÖ‚∏Ö‚Äï‚∏ì·Åè‚ßò‚πáëóó·™¶êΩô◊¥êèê‡ºª·õ¨ÿõ‡πè·çßÔºáÔºé·Åä·±æ·≥Å‚Äªë™ûÔπò·≥Ç‚πñëáõ·ç®‚∏§ëááë®øÍ•üÿãëáû‡ºéê¨æ’õÔºà‹Å!’úëóä‚ÇøÔºä‚∏ª‹âÔπàÔπé‡†∑ê¨Ω‚åà'‡ºΩëô´‡πõÿä·™•ëóå„Ääëúæê©ëê¨º*‚Ç≤‚∏Ω‚Ç´ê©ï‡••êÆöëóìÔΩ•‚ü¶‚üÖëß¢€îëóà‚Äî„Ä∞‚∏¢Ôπä\"‚Ä∑‚∏É‚∏≠$·≠ö‚¶á·≠†ëóñÍõ≥‹ÜÍ§Ø‚ÅÖÔπ´ëÅçë†ªÔºÜ‡ºÑ·Åã‚Ä°„Äë‹äÍßÇ‚Äü‚πÅ‚ü©‚πìëô¨‚ùØëöπ·≥Ö)\n"
     ]
    }
   ],
   "source": [
    "print(len(PUNCTUATION_STRINGS))\n",
    "print(PUNCTUATION_STRINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy includes space tokens in the vocab for non-destructive tokenisation. Positions of space tokens are stored so they can be filtered out for analysis and reporting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens consisting of only punctuation are defined as punctuation tokens. These can be removed or included in analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_build_process(self:Corpus,\n",
    "\t\t\t\t\t\tsave_path: str, # path to save corpus data \n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Create slug, corpus_path, and create directory if needed. \"\"\"\n",
    "\n",
    "\tself.conc_version = __version__\n",
    "\tself.slug = slugify(self.name, stopwords=['corpus'])\n",
    "\tself.corpus_path = os.path.join(save_path, f'{self.slug}.corpus')\n",
    "\n",
    "\tif not os.path.isdir(self.corpus_path):\n",
    "\t\tos.makedirs(self.corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _update_build_process(self: Corpus, \n",
    "                           orth_index: list[np.ndarray], # orthographic token ids\n",
    "                           lower_index: list[np.ndarray], # lower case token ids\n",
    "                           token2doc_index: list[np.ndarray], # token to document mapping\n",
    "                           has_spaces: list[np.ndarray], # arrays of whether token has space\n",
    "                           store_pos: int # current store pos\n",
    "                           ) -> int: # next store pos\n",
    "    \"\"\" Write in-progress build data to Parquet disk store. \"\"\"\n",
    "\n",
    "    pl.DataFrame([np.concatenate(orth_index), np.concatenate(lower_index), np.concatenate(token2doc_index), np.concatenate(has_spaces)], schema = [('orth_index', pl.UInt64), ('lower_index', pl.UInt64), ('token2doc_index', pl.Int32), ('has_spaces', pl.Boolean)] ).write_parquet(f'{self.corpus_path}/build_{store_pos}.parquet')\n",
    "    return store_pos + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: currently streaming either with sink_parquet or collect(engine='streaming') can break the order of the dataframe (not just whole rows, but within specific columns leading to misaligned data). Streaming is not being used for the build, this will be reassessed in the future as the new Polars streaming functionality matures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _complete_build_process(self: Corpus, \n",
    "\t\t\t\t\t\t\tbuild_process_cleanup: bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t\t\t\t\t\t):\n",
    "\t\"\"\" Complete the disk-based build to create representation of the corpus. \"\"\"\n",
    "\n",
    "\tlogger.memory_usage('init', init=True)\n",
    "\ttokens_df = pl.scan_parquet(f'{self.corpus_path}/build_*.parquet')\n",
    "\n",
    "\t# get unique vocab ids (combining orth and lower) and create new index\n",
    "\tvocab_df = pl.concat([tokens_df.select(pl.col('orth_index').unique().alias('index')), tokens_df.select(pl.col('lower_index').unique().alias('index'))])\n",
    "\t#vocab_df  = combined_df.select(pl.col('index').unique().sort().alias('source_id')).with_row_index('token_id', offset=1) #.collect(engine='streaming')\n",
    "\tvocab_df = vocab_df.select(pl.col('index').unique().sort().alias('source_id')).with_row_index('token_id', offset=1) \n",
    "\tlogger.memory_usage('collected vocab')\n",
    "\n",
    "\t# combined_df = (combined_df.with_columns(pl.col('index').replace(vocab_df.select(pl.col('source_id'))['source_id'], vocab_df.select(pl.col('token_id'))['token_id']).cast(pl.UInt32)))\n",
    "\t# combined_df = combined_df.with_columns(pl.col('index').cast(pl.UInt32))\n",
    "\n",
    "\ttokens_df = (\n",
    "\t\ttokens_df\n",
    "\t\t.join(vocab_df, left_on=\"orth_index\", right_on=\"source_id\", how=\"left\", maintain_order=\"left\")\n",
    "\t\t.drop(\"orth_index\")\n",
    "\t\t.rename({\"token_id\": \"orth_index\"})\n",
    "\t\t.with_columns(pl.col(\"orth_index\").cast(pl.UInt32).alias(\"orth_index\"))\n",
    "\t)\n",
    "\n",
    "\ttokens_df = (\n",
    "\t\ttokens_df\n",
    "\t\t.join(vocab_df, left_on=\"lower_index\", right_on=\"source_id\", how=\"left\", maintain_order=\"left\")\n",
    "\t\t.drop(\"lower_index\")\n",
    "\t\t.rename({\"token_id\": \"lower_index\"})\n",
    "\t\t.with_columns(pl.col(\"lower_index\").cast(pl.UInt32).alias(\"lower_index\"))\n",
    "\t) # should have aligned data for token2doc_index and has_spaces\n",
    "\n",
    "\t# this is currently the most intensive operation in terms of memory usage - this needs attention - can't use sink_parquet currently or collect(engine='streaming') as it doesn't maintain order (including changing order between two columns)\n",
    "\ttokens_df.select([pl.col('orth_index'), pl.col('lower_index'), pl.col('token2doc_index'), pl.col('has_spaces')]).collect().write_parquet(f'{self.corpus_path}/tokens.parquet') # if using streaming or sink will need to verify ordering - check for maintain_order parameters\n",
    "\tlogger.memory_usage('wrote pending tokens to disk')\n",
    "\ttokens_df = pl.scan_parquet(f'{self.corpus_path}/tokens.parquet') # re-read as lazy frame\n",
    "\n",
    "\t# could batch this to reduce memory usage - but leaving for now\n",
    "\tvocab_query = vocab_df.select(pl.col('source_id')).collect().to_numpy().flatten() # get vocab ids as numpy array for faster processing\n",
    "\tvocab = {k:self._nlp.vocab[k].text for k in vocab_query} # get vocab strings from spacy vocab\n",
    "\ttoken_strs = list(vocab.values())\n",
    "\tvocab_df = vocab_df.with_columns(pl.Series(token_strs).alias('token'))\n",
    "\tdel vocab_query\n",
    "\tlogger.memory_usage('added vocab strings')\n",
    "\n",
    "\tself.EOF_TOKEN = vocab_df.filter(pl.col('source_id') == self.SPACY_EOF_TOKEN).select(pl.col('token_id')).collect().item() # casting to int for storage\n",
    "\t\n",
    "\tself.punct_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip(PUNCTUATION_STRINGS) == '']\n",
    "\tlogger.memory_usage(f'got punct tokens')\n",
    "\tself.space_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip() == '']\n",
    "\tlogger.memory_usage(f'got space tokens')\n",
    "\tdel token_strs\n",
    "\n",
    "\t# new spaces handling\n",
    "\tspaces_df = tokens_df.with_row_index('position').filter(pl.col('lower_index').is_in(self.space_tokens))\n",
    "\tspaces_df = spaces_df.with_row_index('adjust_by').with_columns((pl.col('position') - pl.col('adjust_by')).alias('corrected'))\n",
    "\tspaces_df = spaces_df.with_columns(pl.col('corrected').alias('position')).drop('adjust_by').drop('corrected')\n",
    "\tspaces_df.collect().write_parquet(f'{self.corpus_path}/spaces.parquet') \n",
    "\tlogger.memory_usage('saved space positions')\n",
    "\n",
    "\t# remove spaces from tokens_df\n",
    "\ttokens_df = tokens_df.filter(~pl.col('lower_index').is_in(self.space_tokens))\n",
    "\ttokens_df.collect().write_parquet(f'{self.corpus_path}/tokens.parquet') # if using streaming or sink will need to verify ordering - check for maintain_order parameters\n",
    "\tlogger.memory_usage('wrote final tokens to disk')\n",
    "\ttokens_df = pl.scan_parquet(f'{self.corpus_path}/tokens.parquet') # re-read as lazy frame\n",
    "\n",
    "\t# Create LazyFrames for punct_positions\n",
    "\ttokens_df.select(pl.col('lower_index')).with_row_index('position').filter(pl.col('lower_index').is_in(self.punct_tokens)).select('position').sink_parquet(f'{self.corpus_path}/puncts.parquet', maintain_order = True) #.collect(engine='streaming').to_numpy().flatten()\n",
    "\tlogger.memory_usage('saved punct positions')\n",
    "\n",
    "\t# get counts from tokens_df\n",
    "\tfrequency_lower = tokens_df.filter(pl.col('lower_index') != self.EOF_TOKEN).select(pl.col('lower_index')).group_by('lower_index').agg(pl.count('lower_index').alias('frequency_lower')) #.collect(engine='streaming')\n",
    "\tfrequency_orth = tokens_df.filter(pl.col('orth_index') != self.EOF_TOKEN).select(pl.col('orth_index')).group_by('orth_index').agg(pl.count('orth_index').alias('frequency_orth')) #.collect(engine='streaming')\n",
    "\tvocab_df = vocab_df.join(frequency_lower, left_on = 'token_id', right_on = 'lower_index', how='left', maintain_order=\"left\").join(frequency_orth, left_on = 'token_id', right_on = 'orth_index', how='left', maintain_order=\"left\")\n",
    "\tlogger.memory_usage('added frequency to vocab')\n",
    "\n",
    "\tself.unique_tokens = frequency_lower.select(pl.len()).collect(engine='streaming').item() # was len(frequency_lower) before used polars streaming\n",
    "\tlogger.memory_usage(f'got unique tokens {self.unique_tokens}')\n",
    "\n",
    "\tdel frequency_lower\n",
    "\tdel frequency_orth\n",
    "\n",
    "\t# add column for is_punct and is_space based on punct_tokens and space_tokens and token_id\n",
    "\tvocab_df = vocab_df.with_columns((pl.col(\"token_id\").is_in(self.punct_tokens)).alias(\"is_punct\"))\n",
    "\tvocab_df = vocab_df.with_columns((pl.col(\"token_id\").is_in(self.space_tokens)).alias(\"is_space\"))\n",
    "\tvocab_df = vocab_df.sort(by = pl.col('token').str.to_lowercase(), descending = False).with_row_index('tokens_sort_order', offset=1) # leave with no zero for handling of error tokens\n",
    "\tvocab_df = vocab_df.drop('source_id').sort(by = pl.col('frequency_orth'), descending = True, nulls_last = True).with_row_index(name='rank', offset=1)\n",
    "\tlogger.memory_usage('added is_punct is_space to vocab')\n",
    "\n",
    "\tvocab_df.collect().write_parquet(f'{self.corpus_path}/vocab.parquet') #, maintain_order = True \n",
    "\tdel vocab_df\n",
    "\tlogger.memory_usage('wrote vocab to disk')\n",
    "\n",
    "\t#self.document_count = tokens_df.select(pl.col('token2doc_index').filter(pl.col('token2doc_index') != NOT_DOC_TOKEN).unique().count()).collect(engine='streaming').item()\n",
    "\tself.document_count = tokens_df.select(pl.col('token2doc_index')).max().collect().item()\n",
    "\tlogger.memory_usage(f'got doc count {self.document_count}')\n",
    "\t# reading now excludes spaces\n",
    "\tinput_length = tokens_df.select(pl.len()).collect(engine='streaming').item() # tested vs count - len seems to have slight memory overhead, but more correct (i.e. count only counts non-null)\n",
    "\tlogger.memory_usage(f'got input length {input_length} (with eof headers)')\n",
    "\n",
    "\t# adjusting token count for text breaks and headers at start and end of index\n",
    "\tself.token_count = input_length - self.document_count - INDEX_HEADER_LENGTH - INDEX_HEADER_LENGTH \n",
    "\tlogger.memory_usage(f'got token count {self.token_count}')\n",
    "\n",
    "\tself.punct_token_count = pl.scan_parquet(f'{self.corpus_path}/puncts.parquet').select(pl.len()).collect(engine='streaming').item() # may be more efficient to do this prior to disk write\n",
    "\tlogger.memory_usage(f'got punct token count ({self.punct_token_count})')\n",
    "\tself.space_token_count = pl.scan_parquet(f'{self.corpus_path}/spaces.parquet').select(pl.len()).collect(engine='streaming').item() # may be more efficient to do this prior to disk write\n",
    "\tlogger.memory_usage(f'got space token count ({self.space_token_count})')\n",
    "\tself.word_token_count = self.token_count - self.punct_token_count\n",
    "\tself.unique_word_tokens = self.unique_tokens - len(self.punct_tokens)\n",
    "\t\n",
    "\tself.date_created = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "\tif build_process_cleanup:\n",
    "\t\tfor f in glob.glob(f'{self.corpus_path}/build_*.parquet'):\n",
    "\t\t\tos.remove(f)\n",
    "\t\tlogger.memory_usage('removed build files')\n",
    "\t\n",
    "\tlogger.memory_usage('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _create_indices(self: Corpus, \n",
    "\t\t\t\t   orth_index: list[np.ndarray], # list of np arrays of orth token ids \n",
    "\t\t\t\t   lower_index: list[np.ndarray], # list of np arrays of lower token ids\n",
    "\t\t\t\t   token2doc_index: list[np.ndarray] # list of np arrays of doc ids\n",
    "\t\t\t\t   ):\n",
    "\t\"\"\" (Deprecated) Use Numpy to create internal representation of the corpus for faster analysis and efficient representation on disk. Only used when the disk-based build process is not used. \"\"\"\n",
    "\t\n",
    "\traise DeprecationWarning('This method is deprecated, the current build process uses _complete_build_process instead.')\n",
    "\n",
    "\tself.token2doc_index = np.concatenate(token2doc_index)\n",
    "\tunique_values, inverse = np.unique(np.concatenate(orth_index + lower_index), return_inverse=True)\n",
    "\n",
    "\t# adding a dummy value at the 0 index to avoid 0 being used as a token id\n",
    "\tunique_values = np.insert(unique_values, 0, 0)\n",
    "\tinverse += 1\n",
    "\tnew_values = np.arange(len(unique_values), dtype=np.uint32)\n",
    "\tself.original_to_new = dict(zip(unique_values, new_values))\n",
    "\tself.new_to_original = dict(zip(new_values, unique_values))\n",
    "\n",
    "\tself.orth_index = np.array(np.split(inverse, 2)[0], dtype=np.uint32)\n",
    "\tself.lower_index = np.array(np.split(inverse, 2)[1], dtype=np.uint32)\n",
    "\tdel inverse\n",
    "\n",
    "\tvocab = {k:self._nlp.vocab.strings[k] for k in unique_values}\n",
    "\tvocab[0] = ERR_TOKEN_STR\n",
    "\n",
    "\tself.vocab = {**{k:vocab[self.new_to_original[k]] for k in new_values}}\n",
    "\n",
    "\tself.EOF_TOKEN = self.original_to_new[self.SPACY_EOF_TOKEN]\n",
    "\n",
    "\tself._process_punct_positions()\n",
    "\tself._process_space_positions()\n",
    "\n",
    "\tself.frequency_lookup = dict(zip(*np.unique(self.lower_index, return_counts=True)))\n",
    "\tdel self.frequency_lookup[self.EOF_TOKEN]\n",
    "\tdel unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_corpus_dataframes(self: Corpus):\n",
    "\t\"\"\" Initialize dataframes after build or load \"\"\"\n",
    "\t\n",
    "\tself.vocab = pl.scan_parquet(f'{self.corpus_path}/vocab.parquet')\n",
    "\tself.tokens = pl.scan_parquet(f'{self.corpus_path}/tokens.parquet')\n",
    "\tself.puncts = pl.scan_parquet(f'{self.corpus_path}/puncts.parquet')\n",
    "\tself.spaces = pl.scan_parquet(f'{self.corpus_path}/spaces.parquet')\n",
    "\tself.metadata = pl.scan_parquet(f'{self.corpus_path}/metadata.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "README_TEMPLATE = \"\"\"# {name}\n",
    "\n",
    "## About\n",
    "\n",
    "This directory contains a corpus created using the [Conc]({REPOSITORY_URL}) Python library. \n",
    "\n",
    "## Corpus Information\n",
    "\n",
    "{description}\n",
    "\n",
    "Date created: {date_created}  \n",
    "Document count: {document_count}  \n",
    "Token count: {token_count}  \n",
    "Word token count: {word_token_count}  \n",
    "Unique tokens: {unique_tokens}  \n",
    "Unique word tokens: {unique_word_tokens}  \n",
    "Conc Version Number: {conc_version}  \n",
    "spaCy model: {SPACY_MODEL}, version {SPACY_MODEL_VERSION}  \n",
    "\n",
    "## Using this corpus\n",
    " \n",
    "Conc can be installed [via pip]({PYPI_URL}). The [Conc documentation site]({DOCUMENTATION_URL}) \n",
    "has tutorials and detailed information to get you started with Conc or to work with the corpus \n",
    "data directly.  \n",
    "\n",
    "## Cite Conc\n",
    "\n",
    "{CITATION_STR}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_corpus_metadata(self: Corpus, \n",
    "\t\t ):\n",
    "\t\"\"\" Save corpus metadata. \"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tjson_bytes = msgspec.json.encode(CorpusMetadata(**{k: getattr(self, k) for k in ['name', 'description', 'slug', 'conc_version', 'document_count', 'token_count', 'word_token_count', 'punct_token_count', 'space_token_count', 'unique_tokens', 'unique_word_tokens', 'date_created', 'EOF_TOKEN', 'SPACY_EOF_TOKEN', 'SPACY_MODEL', 'SPACY_MODEL_VERSION', 'punct_tokens', 'space_tokens']}))\n",
    "\n",
    "\twith open(f'{self.corpus_path}/corpus.json', 'wb') as f:\n",
    "\t\tf.write(json_bytes)\n",
    "\n",
    "\twith open(f'{self.corpus_path}/README.md', 'w', encoding='utf-8') as f:\n",
    "\t\tf.write(README_TEMPLATE.format(\n",
    "\t\t\tname=self.name,\n",
    "\t\t\tREPOSITORY_URL=REPOSITORY_URL,\n",
    "\t\t\tPYPI_URL=PYPI_URL,\n",
    "\t\t\tDOCUMENTATION_URL=DOCUMENTATION_URL,\n",
    "\t\t\tCITATION_STR=CITATION_STR,\n",
    "\t\t\tdescription=self.description,\n",
    "\t\t\tdate_created=self.date_created,\n",
    "\t\t\tdocument_count=self.document_count,\n",
    "\t\t\ttoken_count=self.token_count,\n",
    "\t\t\tword_token_count=self.word_token_count,\n",
    "\t\t\tunique_tokens=self.unique_tokens,\n",
    "\t\t\tunique_word_tokens=self.unique_word_tokens,\n",
    "\t\t\tconc_version=self.conc_version,\n",
    "\t\t\tSPACY_MODEL=self.SPACY_MODEL,\n",
    "\t\t\tSPACY_MODEL_VERSION=self.SPACY_MODEL_VERSION\n",
    "\t\t))\n",
    "\t\t\n",
    "\tlogger.info(f'Saved corpus metadata time: {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build(self: Corpus, \n",
    "\t\t  save_path:str, # directory where corpus will be created, a subdirectory will be automatically created with the corpus content\n",
    "\t\t  iterator: iter, # iterator of texts\n",
    "\t\t  model: str='en_core_web_sm', # spacy model to use for tokenisation\n",
    "\t\t  spacy_batch_size:int=500, # batch size for spacy tokenizer\n",
    "\t\t  build_process_batch_size:int=5000, # save in-progress build to disk every n docs\n",
    "\t\t  build_process_cleanup:bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t  ):\n",
    "\t\"\"\"Build a corpus from an iterator of texts.\"\"\"\n",
    "\n",
    "\tself._init_spacy_model(model)\n",
    "\t\n",
    "\tself.SPACY_MODEL = model\n",
    "\tself.SPACY_MODEL_VERSION = self._nlp.meta['version']\n",
    "\tself.SPACY_EOF_TOKEN = self._nlp.vocab[EOF_TOKEN_STR].orth\n",
    "\t\n",
    "\tif self.corpus_path is None: # leaving for testing ... this should already be set if build has been initiated in standard way via build_from_csv, build_from_files or whatever other methods are implemented to handle build/imports in future\n",
    "\t\tself._init_build_process(save_path)\n",
    "\t\n",
    "\tlogger.memory_usage('init', init=True)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\teof_arr = np.array([self.SPACY_EOF_TOKEN], dtype=np.uint64)\n",
    "\tnot_doc_arr = np.array([NOT_DOC_TOKEN], dtype=np.int16)\n",
    "\tindex_header_arr = np.array([self.SPACY_EOF_TOKEN] * INDEX_HEADER_LENGTH, dtype=np.uint64) # this is added to start and end of index to prevent out of bound issues on searches\n",
    "\thas_spaces_eof_arr = np.array([False], dtype=np.bool)\n",
    "\n",
    "\torth_index = [index_header_arr]\n",
    "\tlower_index = [index_header_arr]\n",
    "\ttoken2doc_index = [np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32)]\n",
    "\thas_spaces = [np.array([0] * len(index_header_arr), dtype=np.bool)]\n",
    "\n",
    "\toffset = INDEX_HEADER_LENGTH\n",
    "\n",
    "\tstore_pos = 0\n",
    "\n",
    "\tdoc_order = 1\n",
    "\tfor doc in self._nlp.pipe(iterator, batch_size = spacy_batch_size): # was previously using self._nlp.tokenizer.pipe(iterator, batch_size=batch_size): but this is faster, test other options at some point\n",
    "\t\torth_index.append(doc.to_array(ORTH))\n",
    "\t\torth_index.append(eof_arr)\n",
    "\n",
    "\t\tlower_index_tmp = doc.to_array(LOWER)\n",
    "\t\tlower_index.append(lower_index_tmp)\n",
    "\t\tlower_index.append(eof_arr)\n",
    "\n",
    "\t\ttoken2doc_index.append(np.array([doc_order] * len(lower_index_tmp), dtype=np.int32))\n",
    "\t\ttoken2doc_index.append(not_doc_arr)\n",
    "\n",
    "\t\thas_spaces.append(doc.to_array(SPACY))\n",
    "\t\thas_spaces.append(has_spaces_eof_arr)\n",
    "\t\t# self.offsets.append(offset) \n",
    "\t\t# offset = offset + len(lower_index_tmp) + 1\n",
    "\t\tdoc_order += 1\n",
    "\n",
    "\t\t# update store every build_process_batch_size docs\n",
    "\t\tif doc_order % build_process_batch_size == 0:\n",
    "\t\t\t#was based on condition build_process_path is not None before disk-based build process\n",
    "\t\t\tstore_pos = self._update_build_process(orth_index, lower_index, token2doc_index, has_spaces, store_pos)\n",
    "\t\t\tlower_index, orth_index, token2doc_index, has_spaces = [], [], [], []\n",
    "\t\t\tlogger.memory_usage(f'processed {doc_order} documents')\n",
    "\t\t\t\n",
    "\tdel iterator\n",
    "\torth_index.append(index_header_arr)\n",
    "\tlower_index.append(index_header_arr)\n",
    "\ttoken2doc_index.append(np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32))\n",
    "\thas_spaces.append(np.array([0] * len(index_header_arr), dtype=np.bool))\n",
    "\n",
    "\tlogger.memory_usage(f'Completing build process')\n",
    "\tif save_path is not None:\n",
    "\t\tstore_pos = self._update_build_process(orth_index, lower_index, token2doc_index, has_spaces, store_pos)\n",
    "\t\tlower_index, orth_index, token2doc_index, has_spaces = [], [], [], []\n",
    "\t\tself._complete_build_process(build_process_cleanup = build_process_cleanup)\n",
    "\telse:\n",
    "\t\t# deprecated - leaving for now\n",
    "\t\tself._create_indices(orth_index, lower_index, token2doc_index)\n",
    "\t\t# self.document_count = len(self.offsets)\n",
    "\n",
    "\t\tself.token_count = self.lower_index.shape[0] - self.document_count - len(index_header_arr) - len(index_header_arr) \n",
    "\t\tself.unique_tokens = len(self.frequency_lookup)\n",
    "\n",
    "\t\tself.word_token_count = self.token_count - len(self.punct_positions) - len(self.space_positions)\n",
    "\t\tself.unique_word_tokens = len(self.frequency_lookup) - len(self.punct_tokens) - len(self.space_tokens)\n",
    "\n",
    "\tdel orth_index\n",
    "\tdel lower_index\n",
    "\tdel token2doc_index\n",
    "\tdel has_spaces\n",
    "\t\n",
    "\tlogger.memory_usage(f'Completed build process')\n",
    "\n",
    "\t# save corpus metadata\n",
    "\tself.save_corpus_metadata()\n",
    "\n",
    "\tself._init_corpus_dataframes()\n",
    "\n",
    "\tlogger.info(f'Build time: {(time.time() - start_time):.3f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _prepare_files(self: Corpus, \n",
    "\t\t\t\t\tsource_path: str, # path to folder with text files, path can be a directory, zip or tar/tar.gz file\n",
    "\t\t\t\t\tfile_mask:str='*.txt', # mask to select files \n",
    "\t\t\t\t\tmetadata_file: str|None=None, # path to a CSV with metadata\n",
    "\t\t\t\t\tmetadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from metadata\n",
    "\t\t\t\t\tencoding:str='utf8' # encoding of text files\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Prepare text files and metadata for building a corpus. Returns an iterator to get file text for processing.\"\"\"\n",
    "\n",
    "\t# allowing import from zip and tar files\n",
    "\tif os.path.isdir(source_path):\n",
    "\t\tfiles = glob.glob(os.path.join(source_path, file_mask))\n",
    "\t\ttype = 'folder'\n",
    "\telif os.path.isfile(source_path):\n",
    "\t\timport fnmatch\n",
    "\t\tif source_path.endswith('.zip'):\n",
    "\t\t\timport zipfile\n",
    "\t\t\twith zipfile.ZipFile(source_path, 'r') as z:\n",
    "\t\t\t\tfiles = []\n",
    "\t\t\t\tfor f in z.namelist():\n",
    "\t\t\t\t\tif fnmatch.fnmatch(f, file_mask):\n",
    "\t\t\t\t\t\tfiles.append(f)\n",
    "\t\t\t\tif len(files) > 0:\n",
    "\t\t\t\t\ttype = 'zip'\n",
    "\t\telif source_path.endswith('.tar') or source_path.endswith('.tar.gz'):\n",
    "\t\t\timport tarfile\n",
    "\t\t\twith tarfile.open(source_path, 'r') as t:\n",
    "\t\t\t\tfiles = []\n",
    "\t\t\t\tfor f in t.getnames():\n",
    "\t\t\t\t\tif fnmatch.fnmatch(f, file_mask):\n",
    "\t\t\t\t\t\tfiles.append(f)\n",
    "\t\t\t\tif len(files) > 0:\n",
    "\t\t\t\t\ttype = 'tar'\n",
    "\t\telse:\n",
    "\t\t\traise FileNotFoundError(f\"Path '{source_path}' is not a directory, zip or tar file\")\n",
    "\t\n",
    "\tif not files:\n",
    "\t\traise FileNotFoundError(f\"No files matching {file_mask} found in '{source_path}'\")\n",
    "\n",
    "\tmetadata = pl.LazyFrame({metadata_file_column: [os.path.basename(p) for p in files]})\n",
    "\n",
    "\tif metadata_file:\n",
    "\t\tif not os.path.isfile(metadata_file):\n",
    "\t\t\traise FileNotFoundError(f\"Metadata file '{metadata_file}' not found\")\n",
    "\t\ttry:\n",
    "\t\t\tif metadata_file_column not in metadata_columns:\n",
    "\t\t\t\tmetadata_columns.insert(0, metadata_file_column)\n",
    "\t\t\t\n",
    "\t\t\tmetadata = pl.scan_csv(metadata_file).select(metadata_columns)\n",
    "\t\t\t# reordering files on metadata so token data and metadata aligned\n",
    "\t\t\tfiles = metadata.select(pl.col(metadata_file_column)).collect(engine='streaming').to_numpy().flatten().tolist() # get file names from metadata\n",
    "\t\t\tfiles = [os.path.join(source_path, f) for f in files if os.path.basename(f) in files] \n",
    "\t\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\t\traise\n",
    "\t\n",
    "\tmetadata.sink_parquet(f'{self.corpus_path}/metadata.parquet')\n",
    "\n",
    "\tself.source_path = source_path\n",
    "\n",
    "\tif type == 'folder':\n",
    "\t\tfor p in files:\n",
    "\t\t\tyield open(p, \"rb\").read().decode(encoding)\n",
    "\telif type == 'zip':\n",
    "\t\twith zipfile.ZipFile(source_path, 'r') as z:\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tyield z.read(f).decode(encoding)\n",
    "\telif type == 'tar':\n",
    "\t\twith tarfile.open(source_path, 'r') as t:\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tyield t.extractfile(f).read().decode(encoding)\t\t\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_from_files(self: Corpus,\n",
    "\t\t\t\t\tsource_path: str, # path to folder with text files \n",
    "\t\t\t\t\tsave_path: str, # path to save corpus\n",
    "\t\t\t\t\tfile_mask:str='*.txt', # mask to select files \n",
    "\t\t\t\t\tmetadata_file: str|None=None, # path to a CSV with metadata\n",
    "\t\t\t\t\tmetadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from metadata\n",
    "\t\t\t\t\tencoding:str='utf-8', # encoding of text files\n",
    "\t\t\t\t\tmodel:str='en_core_web_sm', # spacy model to use for tokenisation\n",
    "\t\t\t\t\tspacy_batch_size:int=1000, # batch size for spacy tokenizer\n",
    "\t\t\t\t\tbuild_process_batch_size:int=5000, # save in-progress build to disk every n docs\n",
    "\t\t\t\t\tbuild_process_cleanup:bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Build a corpus from text files in a folder.\"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tself._init_build_process(save_path)\n",
    "\titerator = self._prepare_files(source_path, file_mask, metadata_file, metadata_file_column, metadata_columns, encoding) #, build_process_path=build_process_path\n",
    "\tself.build(save_path = save_path, iterator = iterator, model = model, spacy_batch_size = spacy_batch_size, build_process_batch_size = build_process_batch_size, build_process_cleanup = build_process_cleanup) #build_process_path = build_process_path, \n",
    "\tlogger.info(f'Build from files time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _prepare_csv(self: Corpus, \n",
    "\t\t\t\t\tsource_path:str, # path to csv file\n",
    "\t\t\t\t\ttext_column:str='text', # column in csv with text\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from csv\n",
    "\t\t\t\t\tencoding:str='utf8', # encoding of csv passed to Polars read_csv, see their documentation\n",
    "\t\t\t\t\tbuild_process_batch_size:int=5000 # save in-progress build to disk every n rows\n",
    "\t\t\t\t\t) -> iter: # iterator to return rows for processing\n",
    "\t\"\"\"Prepare to import from CSV, including metadata. Returns an iterator to process the text column.\"\"\"\n",
    "\n",
    "\tif not os.path.isfile(source_path):\n",
    "\t\traise FileNotFoundError(f'Path ({source_path}) is not a file')\n",
    "\t\n",
    "\ttry:\n",
    "\t\tdf = pl.scan_csv(source_path, encoding = encoding).select([text_column] + metadata_columns)\n",
    "\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\traise\n",
    "\n",
    "\tself.source_path = source_path\n",
    "\t\n",
    "\tdf.select(metadata_columns).sink_parquet(f'{self.corpus_path}/metadata.parquet')\n",
    "\n",
    "\tfor slice_df in df.collect(engine='streaming').iter_slices(n_rows=build_process_batch_size):  \n",
    "\t\tfor row in slice_df.iter_rows():\n",
    "\t\t\tyield row[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_from_csv(self: Corpus, \n",
    "\t\t\t\t   source_path:str, # path to csv file\n",
    "\t\t\t\t   save_path: str, # path to save corpus\n",
    "\t\t\t\t   text_column:str='text', # column in csv with text\n",
    "\t\t\t\t   metadata_columns:list[str]=[], # list of column names to import from csv\n",
    "\t\t\t\t   encoding:str='utf8', # encoding of csv passed to Polars read_csv, see their documentation\n",
    "\t\t\t\t   model:str='en_core_web_sm', # spacy model to use for tokenisation\n",
    "\t\t\t\t   spacy_batch_size:int=1000, # batch size for Spacy tokenizer\n",
    "\t\t\t\t   #build_process_path:str=None, # path to save an in-progress build to disk to reduce memory usage\n",
    "\t\t\t\t   build_process_batch_size:int=5000, # save in-progress build to disk every n docs\n",
    "\t\t\t\t   build_process_cleanup:bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t\t\t   ):\n",
    "\t\"\"\"Build a corpus from a csv file.\"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tself._init_build_process(save_path)\n",
    "\titerator = self._prepare_csv(source_path = source_path, text_column = text_column, metadata_columns = metadata_columns, encoding = encoding, build_process_batch_size = build_process_batch_size)\n",
    "\tself.build(save_path = save_path, iterator = iterator, model = model, spacy_batch_size = spacy_batch_size, build_process_batch_size = build_process_batch_size, build_process_cleanup = build_process_cleanup)\n",
    "\tlogger.info(f'Build from csv time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# document_count = 6\n",
    "# token_count = 38\n",
    "# word_token_count = 32\n",
    "# unique_tokens = 15\n",
    "# unique_word_tokens = 14\n",
    "\n",
    "test = Corpus('test').build_from_files(source_path = f'{source_path}toy', save_path = save_path, file_mask='*.txt', metadata_file=f'{source_path}toy.csv', metadata_file_column = 'source', metadata_columns=['category'], model='en_core_web_sm', spacy_batch_size=1000, build_process_batch_size=5000, build_process_cleanup=True)\n",
    "\n",
    "assert test.document_count == 6\n",
    "assert test.token_count == 38\n",
    "assert test.word_token_count == 32\n",
    "assert test.unique_tokens == 15\n",
    "assert test.unique_word_tokens == 14\n",
    "\n",
    "assert type(test.metadata) == pl.LazyFrame\n",
    "test.metadata = test.metadata.collect()\n",
    "assert test.metadata.columns == ['source', 'category']\n",
    "\n",
    "#display(test.metadata.head(20))\n",
    "\n",
    "assert os.path.isfile(f'{test.corpus_path}/corpus.json')\n",
    "assert os.path.isfile(f'{test.corpus_path}/README.md')\n",
    "assert os.path.isfile(f'{test.corpus_path}/vocab.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/tokens.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/puncts.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/spaces.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/metadata.parquet')\n",
    "#display(test.vocab.collect().head(20))\n",
    "test_result = test.vocab.filter(pl.col('token') == 'the').collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token_id')).item() == 10\n",
    "assert test_result.select(pl.col('frequency_lower')).item() == 8\n",
    "assert test_result.select(pl.col('frequency_orth')).item() == 2\n",
    "assert test_result.select(pl.col('is_punct')).item() == False\n",
    "assert test_result.select(pl.col('is_space')).item() == False\n",
    "\n",
    "test_result = test.vocab.filter(pl.col('token') == '.').collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token_id')).item() == 15\n",
    "assert test_result.select(pl.col('frequency_lower')).item() == 6\n",
    "assert test_result.select(pl.col('frequency_orth')).item() == 6\n",
    "assert test_result.select(pl.col('is_punct')).item() == True\n",
    "assert test_result.select(pl.col('is_space')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 0).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('lower_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == NOT_DOC_TOKEN\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 99).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 100).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('lower_index')).item() == 10\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == True\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 104).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == 10\n",
    "assert test_result.select(pl.col('lower_index')).item() == 10\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == True\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 105).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 106).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == 15\n",
    "assert test_result.select(pl.col('lower_index')).item() == 15\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 7).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('lower_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == NOT_DOC_TOKEN\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "#test_result = test.tokens.with_row_index('position').filter(pl.col('position') > 99).collect(engine='streaming')\n",
    "#print(test_result.head(10))\n",
    "\n",
    "if os.path.isdir(test.corpus_path):\n",
    "\tshutil.rmtree(test.corpus_path)\n",
    "\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# document_count = 6\n",
    "# token_count = 38\n",
    "# word_token_count = 32\n",
    "# unique_tokens = 15\n",
    "# unique_word_tokens = 14\n",
    "\n",
    "test = Corpus('test').build_from_csv(source_path = f'{source_path}toy.csv', save_path = save_path, text_column='text', metadata_columns=['source', 'category'], model='en_core_web_sm', spacy_batch_size=1000, build_process_batch_size=5000, build_process_cleanup=True)\n",
    "\n",
    "assert test.document_count == 6\n",
    "assert test.token_count == 38\n",
    "assert test.word_token_count == 32\n",
    "assert test.unique_tokens == 15\n",
    "assert test.unique_word_tokens == 14\n",
    "\n",
    "assert type(test.metadata) == pl.LazyFrame\n",
    "test.metadata = test.metadata.collect()\n",
    "assert test.metadata.columns == ['source', 'category']\n",
    "\n",
    "#display(test.metadata.head(20))\n",
    "\n",
    "assert os.path.isfile(f'{test.corpus_path}/corpus.json')\n",
    "assert os.path.isfile(f'{test.corpus_path}/README.md')\n",
    "assert os.path.isfile(f'{test.corpus_path}/vocab.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/tokens.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/puncts.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/spaces.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/metadata.parquet')\n",
    "#display(test.vocab.collect().head(20))\n",
    "test_result = test.vocab.filter(pl.col('token') == 'the').collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token_id')).item() == 10\n",
    "assert test_result.select(pl.col('frequency_lower')).item() == 8\n",
    "assert test_result.select(pl.col('frequency_orth')).item() == 2\n",
    "assert test_result.select(pl.col('is_punct')).item() == False\n",
    "assert test_result.select(pl.col('is_space')).item() == False\n",
    "\n",
    "test_result = test.vocab.filter(pl.col('token') == '.').collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token_id')).item() == 15\n",
    "assert test_result.select(pl.col('frequency_lower')).item() == 6\n",
    "assert test_result.select(pl.col('frequency_orth')).item() == 6\n",
    "assert test_result.select(pl.col('is_punct')).item() == True\n",
    "assert test_result.select(pl.col('is_space')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 0).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('lower_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == NOT_DOC_TOKEN\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 99).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 100).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('lower_index')).item() == 10\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == True\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 104).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == 10\n",
    "assert test_result.select(pl.col('lower_index')).item() == 10\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == True\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 105).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 106).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == 15\n",
    "assert test_result.select(pl.col('lower_index')).item() == 15\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 7).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('lower_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == NOT_DOC_TOKEN\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "#test_result = test.tokens.with_row_index('position').filter(pl.col('position') > 99).collect(engine='streaming')\n",
    "#print(test_result.head(10))\n",
    "\n",
    "if os.path.isdir(test.corpus_path):\n",
    "\tshutil.rmtree(test.corpus_path)\n",
    "\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load(self: Corpus, \n",
    "\t\t corpus_path: str # path to load corpus\n",
    "\t\t ):\n",
    "\t\"\"\" Load corpus from disk and load the corresponding spaCy model. \"\"\"\n",
    "\n",
    "\tlogger.memory_usage('init', init=True)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tif not os.path.isdir(corpus_path):\n",
    "\t\traise FileNotFoundError(f\"Path '{corpus_path}' is not a directory\")\n",
    "\t\n",
    "\texpected_files = ['corpus.json', 'vocab.parquet', 'tokens.parquet', 'puncts.parquet', 'spaces.parquet']\n",
    "\tif not all(os.path.isfile(os.path.join(corpus_path, f)) for f in expected_files):\n",
    "\t\traise FileNotFoundError(f\"Path '{corpus_path}' does not contain all expected files: {expected_files}\")\n",
    "\n",
    "\tself.corpus_path = corpus_path\n",
    "\n",
    "\twith open(f'{self.corpus_path}/corpus.json', 'rb') as f:\n",
    "\t\tdata = msgspec.json.decode(f.read(), type=CorpusMetadata)\n",
    "\n",
    "\tfor k in data.__slots__:\n",
    "\t\tsetattr(self, k, getattr(data, k))\n",
    "\n",
    "\tself._init_spacy_model(self.SPACY_MODEL, version = self.SPACY_MODEL_VERSION)\n",
    "\n",
    "\tself._init_corpus_dataframes()\n",
    "\n",
    "\tlogger.info(f'Load time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# document_count = 6\n",
    "# token_count = 38\n",
    "# word_token_count = 32\n",
    "# unique_tokens = 15\n",
    "# unique_word_tokens = 14\n",
    "\n",
    "# building just in case - then  \n",
    "test = Corpus('test').build_from_csv(source_path = f'{source_path}toy.csv', save_path = save_path, text_column='text', metadata_columns=['source', 'category'], model='en_core_web_sm', spacy_batch_size=1000, build_process_batch_size=5000, build_process_cleanup=True)\n",
    "test = Corpus('test').load(f'{save_path}/test.corpus')\n",
    "\n",
    "assert test.document_count == 6\n",
    "assert test.token_count == 38\n",
    "assert test.word_token_count == 32\n",
    "assert test.unique_tokens == 15\n",
    "assert test.unique_word_tokens == 14\n",
    "\n",
    "assert type(test.metadata) == pl.LazyFrame\n",
    "test.metadata = test.metadata.collect()\n",
    "assert test.metadata.columns == ['source', 'category']\n",
    "\n",
    "#display(test.metadata.head(20))\n",
    "\n",
    "assert os.path.isfile(f'{test.corpus_path}/corpus.json')\n",
    "assert os.path.isfile(f'{test.corpus_path}/README.md')\n",
    "assert os.path.isfile(f'{test.corpus_path}/vocab.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/tokens.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/puncts.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/spaces.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/metadata.parquet')\n",
    "#display(test.vocab.collect().head(20))\n",
    "test_result = test.vocab.filter(pl.col('token') == 'the').collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token_id')).item() == 10\n",
    "assert test_result.select(pl.col('frequency_lower')).item() == 8\n",
    "assert test_result.select(pl.col('frequency_orth')).item() == 2\n",
    "assert test_result.select(pl.col('is_punct')).item() == False\n",
    "assert test_result.select(pl.col('is_space')).item() == False\n",
    "\n",
    "test_result = test.vocab.filter(pl.col('token') == '.').collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token_id')).item() == 15\n",
    "assert test_result.select(pl.col('frequency_lower')).item() == 6\n",
    "assert test_result.select(pl.col('frequency_orth')).item() == 6\n",
    "assert test_result.select(pl.col('is_punct')).item() == True\n",
    "assert test_result.select(pl.col('is_space')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 0).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('lower_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == NOT_DOC_TOKEN\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 99).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 100).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('lower_index')).item() == 10\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == True\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 104).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == 10\n",
    "assert test_result.select(pl.col('lower_index')).item() == 10\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == True\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 105).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 106).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == 15\n",
    "assert test_result.select(pl.col('lower_index')).item() == 15\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 7).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('lower_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == NOT_DOC_TOKEN\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "#test_result = test.tokens.with_row_index('position').filter(pl.col('position') > 99).collect(engine='streaming')\n",
    "#print(test_result.head(10))\n",
    "\n",
    "if os.path.isdir(test.corpus_path):\n",
    "\tshutil.rmtree(test.corpus_path)\n",
    "\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "corpora = {}\n",
    "corpora['brown'] = {'name': 'Brown Corpus', 'slug': 'brown', 'description': 'A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.', 'extension': '.csv.gz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 22:23:18 - INFO - memory_usage - init, memory usage: 4959.6640625 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - Completing build process, memory usage: 4955.40625 MB, difference: -4.2578125 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - init, memory usage: 4955.66015625 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - collected vocab, memory usage: 4955.66015625 MB, difference: 0.0 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - wrote pending tokens to disk, memory usage: 4946.734375 MB, difference: -8.92578125 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - added vocab strings, memory usage: 4941.9140625 MB, difference: -4.8203125 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - got punct tokens, memory usage: 4937.09375 MB, difference: -4.8203125 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - got space tokens, memory usage: 4937.09375 MB, difference: 0.0 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - saved space positions, memory usage: 4932.55078125 MB, difference: -4.54296875 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - wrote final tokens to disk, memory usage: 4927.96484375 MB, difference: -4.5859375 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - saved punct positions, memory usage: 4930.45703125 MB, difference: 2.4921875 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - added frequency to vocab, memory usage: 4930.45703125 MB, difference: 0.0 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - got unique tokens 42930, memory usage: 4928.55859375 MB, difference: -1.8984375 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - added is_punct is_space to vocab, memory usage: 4928.55859375 MB, difference: 0.0 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - wrote vocab to disk, memory usage: 4923.17578125 MB, difference: -5.3828125 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - got doc count 500, memory usage: 4922.8671875 MB, difference: -0.30859375 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - got input length 1139266 (with eof headers), memory usage: 4922.8671875 MB, difference: 0.0 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - got token count 1138566, memory usage: 4922.8671875 MB, difference: 0.0 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - got punct token count (158422), memory usage: 4924.0 MB, difference: 1.1328125 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - got space token count (2339), memory usage: 4925.8125 MB, difference: 1.8125 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - removed build files, memory usage: 4925.8125 MB, difference: 0.0 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - done, memory usage: 4925.8125 MB, difference: 0.0 MB\n",
      "2025-06-27 22:23:20 - INFO - memory_usage - Completed build process, memory usage: 4925.8125 MB, difference: 0.0 MB\n",
      "2025-06-27 22:23:20 - INFO - save_corpus_metadata - Saved corpus metadata time: 0.000 seconds\n",
      "2025-06-27 22:23:20 - INFO - build - Build time: 2.685 seconds\n",
      "2025-06-27 22:23:20 - INFO - build_from_csv - Build from csv time: 2.890 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "set_logger_state('verbose')\n",
    "if os.path.isdir(f'{save_path}/test-build-with-brown.corpus'):\n",
    "\tshutil.rmtree(f'{save_path}/test-build-with-brown.corpus')\n",
    "\n",
    "brown = Corpus(name = 'Test Build with ' + corpora['brown']['name'], description = corpora['brown']['description']).build_from_csv(f'{source_path}/brown.csv.gz', save_path = save_path, text_column='text', metadata_columns=['source'])\n",
    "del brown\n",
    "\n",
    "if os.path.isdir(f'{save_path}/test-build-with-brown.corpus'):\n",
    "\tshutil.rmtree(f'{save_path}/test-build-with-brown.corpus')\n",
    "set_logger_state('quiet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def info(self: Corpus, \n",
    "\t\t include_disk_usage:bool = False, # include information of size on disk in output\n",
    "\t\t formatted:bool = True # return formatted output\n",
    "\t\t ) -> str: # formatted information about the corpus\n",
    "\t\"\"\" Return information about the corpus. \"\"\"\n",
    "\t\n",
    "\tresult = []\n",
    "\tattributes = ['name', 'description', 'date_created', 'conc_version', 'corpus_path', 'document_count', 'token_count', 'word_token_count', 'unique_tokens', 'unique_word_tokens']\n",
    "\tfor attr in attributes:\n",
    "\t\tvalue = getattr(self, attr)\n",
    "\t\tif isinstance(value, bool):\n",
    "\t\t\tresult.append('True' if value else 'False')\n",
    "\t\telif isinstance(value, int):\n",
    "\t\t\tresult.append(f'{value:,}')\n",
    "\t\telse:\n",
    "\t\t\tresult.append(str(value))\n",
    "\n",
    "\tif include_disk_usage:\n",
    "\t\tfiles = {'corpus.json': 'Corpus Metadata', 'metadata.parquet': 'Document Metadata', 'tokens.parquet': 'Tokens', 'vocab.parquet': 'Vocab', 'puncts.parquet': 'Punctuation positions', 'spaces.parquet': 'Space positions'}\n",
    "\t\tfor file, file_descriptor in files.items():\n",
    "\t\t\tsize = os.path.getsize(f'{self.corpus_path}/{file}')\n",
    "\t\t\tattributes.append(file_descriptor + ' (MB)')\n",
    "\t\t\tresult.append(f'{size/1024/1024:.3f}')\n",
    "\n",
    "\t# maybe add in status of these: 'results_cache', 'ngram_index', 'frequency_table'\n",
    "\t# size = sys.getsizeof(getattr(self, attr))\n",
    "\t\n",
    "\tif formatted:\n",
    "\t\tattributes = [attr.replace('_', ' ').title() for attr in attributes]\n",
    "\n",
    "\treturn pl.DataFrame({'Attribute': attributes, 'Value': result})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def report(self: Corpus, \n",
    "\t\t\tinclude_memory_usage:bool = False # include memory usage in output\n",
    "\t\t\t) -> Result: # returns Result object with corpus summary information\n",
    "\t\"\"\" Get information about the corpus as a result object. \"\"\"\n",
    "\treturn Result('summary', self.info(include_memory_usage), 'Corpus Summary', '', {}, [])\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def summary(self: Corpus, \n",
    "\t\t\tinclude_memory_usage:bool = False # include memory usage in output\n",
    "\t\t\t):\n",
    "\t\"\"\" Print information about the corpus in a formatted table. \"\"\"\n",
    "\tresult = Result('summary', self.info(include_memory_usage), 'Corpus Summary', '', {}, [])\n",
    "\tresult.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def __str__(self: Corpus):\n",
    "\t\"\"\" Formatted information about the corpus. \"\"\"\n",
    "\t\n",
    "\treturn str(self.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get summary information on your corpus, including the number of documents, the token count and the number of unique tokens as a dataframe using the `info` method. You can also just print the corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "brown = Corpus().load(f'{save_path}brown.corpus')\n",
    "toy = Corpus().load(f'{save_path}toy.corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Attribute          ‚îÜ Value                                                                                                                                                                                                                                              ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ Name               ‚îÜ Brown Corpus                                                                                                                                                                                                                                       ‚îÇ\n",
      "‚îÇ Description        ‚îÜ A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 ‚îÇ\n",
      "‚îÇ                    ‚îÜ http://www.hit.uib.no/icame/brown/bcm.html. This version ‚Ä¶                                                                                                                                                                                         ‚îÇ\n",
      "‚îÇ Date Created       ‚îÜ 2025-06-25 16:58:33                                                                                                                                                                                                                                ‚îÇ\n",
      "‚îÇ Conc Version       ‚îÜ 0.1.4                                                                                                                                                                                                                                              ‚îÇ\n",
      "‚îÇ Corpus Path        ‚îÜ /home/geoff/data/conc-test-corpora/brown.corpus                                                                                                                                                                                                    ‚îÇ\n",
      "‚îÇ Document Count     ‚îÜ 500                                                                                                                                                                                                                                                ‚îÇ\n",
      "‚îÇ Token Count        ‚îÜ 1,138,566                                                                                                                                                                                                                                          ‚îÇ\n",
      "‚îÇ Word Token Count   ‚îÜ 980,144                                                                                                                                                                                                                                            ‚îÇ\n",
      "‚îÇ Unique Tokens      ‚îÜ 42,930                                                                                                                                                                                                                                             ‚îÇ\n",
      "‚îÇ Unique Word Tokens ‚îÜ 42,907                                                                                                                                                                                                                                             ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "print(brown) # equivalent to print(brown.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info` method can also provide information on the disk usage of the corpus setting the `include_disk_usage` parameter to `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Attribute                  ‚îÜ Value                                                                                                                                                                                                                                              ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ Name                       ‚îÜ Brown Corpus                                                                                                                                                                                                                                       ‚îÇ\n",
      "‚îÇ Description                ‚îÜ A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 ‚îÇ\n",
      "‚îÇ                            ‚îÜ http://www.hit.uib.no/icame/brown/bcm.html. This version ‚Ä¶                                                                                                                                                                                         ‚îÇ\n",
      "‚îÇ Date Created               ‚îÜ 2025-06-25 16:58:33                                                                                                                                                                                                                                ‚îÇ\n",
      "‚îÇ Conc Version               ‚îÜ 0.1.4                                                                                                                                                                                                                                              ‚îÇ\n",
      "‚îÇ Corpus Path                ‚îÜ /home/geoff/data/conc-test-corpora/brown.corpus                                                                                                                                                                                                    ‚îÇ\n",
      "‚îÇ Document Count             ‚îÜ 500                                                                                                                                                                                                                                                ‚îÇ\n",
      "‚îÇ Token Count                ‚îÜ 1,138,566                                                                                                                                                                                                                                          ‚îÇ\n",
      "‚îÇ Word Token Count           ‚îÜ 980,144                                                                                                                                                                                                                                            ‚îÇ\n",
      "‚îÇ Unique Tokens              ‚îÜ 42,930                                                                                                                                                                                                                                             ‚îÇ\n",
      "‚îÇ Unique Word Tokens         ‚îÜ 42,907                                                                                                                                                                                                                                             ‚îÇ\n",
      "‚îÇ Corpus Metadata (Mb)       ‚îÜ 0.001                                                                                                                                                                                                                                              ‚îÇ\n",
      "‚îÇ Document Metadata (Mb)     ‚îÜ 0.001                                                                                                                                                                                                                                              ‚îÇ\n",
      "‚îÇ Tokens (Mb)                ‚îÜ 4.468                                                                                                                                                                                                                                              ‚îÇ\n",
      "‚îÇ Vocab (Mb)                 ‚îÜ 0.678                                                                                                                                                                                                                                              ‚îÇ\n",
      "‚îÇ Punctuation Positions (Mb) ‚îÜ 0.425                                                                                                                                                                                                                                              ‚îÇ\n",
      "‚îÇ Space Positions (Mb)       ‚îÜ 0.012                                                                                                                                                                                                                                              ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "print(brown.info(include_disk_usage=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the same information in a table format by using the `summary` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"bkjnozrooi\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#bkjnozrooi table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#bkjnozrooi thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#bkjnozrooi p { margin: 0; padding: 0; }\n",
       " #bkjnozrooi .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #bkjnozrooi .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #bkjnozrooi .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #bkjnozrooi .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #bkjnozrooi .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #bkjnozrooi .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #bkjnozrooi .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #bkjnozrooi .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #bkjnozrooi .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #bkjnozrooi .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #bkjnozrooi .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #bkjnozrooi .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #bkjnozrooi .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #bkjnozrooi .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #bkjnozrooi .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #bkjnozrooi .gt_from_md> :first-child { margin-top: 0; }\n",
       " #bkjnozrooi .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #bkjnozrooi .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #bkjnozrooi .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #bkjnozrooi .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #bkjnozrooi .gt_row_group_first td { border-top-width: 2px; }\n",
       " #bkjnozrooi .gt_row_group_first th { border-top-width: 2px; }\n",
       " #bkjnozrooi .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #bkjnozrooi .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #bkjnozrooi .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #bkjnozrooi .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #bkjnozrooi .gt_left { text-align: left; }\n",
       " #bkjnozrooi .gt_center { text-align: center; }\n",
       " #bkjnozrooi .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #bkjnozrooi .gt_font_normal { font-weight: normal; }\n",
       " #bkjnozrooi .gt_font_bold { font-weight: bold; }\n",
       " #bkjnozrooi .gt_font_italic { font-style: italic; }\n",
       " #bkjnozrooi .gt_super { font-size: 65%; }\n",
       " #bkjnozrooi .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #bkjnozrooi .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"2\" class=\"gt_heading gt_title gt_font_normal\">Corpus Summary</td>\n",
       "  </tr>\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"2\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\"></td>\n",
       "  </tr>\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Attribute\">Attribute</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Value\">Value</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Name</td>\n",
       "    <td class=\"gt_row gt_left\">Brown Corpus</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Description</td>\n",
       "    <td class=\"gt_row gt_left\">A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Date Created</td>\n",
       "    <td class=\"gt_row gt_left\">2025-06-25 16:58:33</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Conc Version</td>\n",
       "    <td class=\"gt_row gt_left\">0.1.4</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Corpus Path</td>\n",
       "    <td class=\"gt_row gt_left\">/home/geoff/data/conc-test-corpora/brown.corpus</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Document Count</td>\n",
       "    <td class=\"gt_row gt_left\">500</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Token Count</td>\n",
       "    <td class=\"gt_row gt_left\">1,138,566</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Word Token Count</td>\n",
       "    <td class=\"gt_row gt_left\">980,144</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Unique Tokens</td>\n",
       "    <td class=\"gt_row gt_left\">42,930</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Unique Word Tokens</td>\n",
       "    <td class=\"gt_row gt_left\">42,907</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "brown.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, Conc uses Polars and Numpy vector operations where possible to speed up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_token_arrays(self: Corpus):\n",
    "\t\"\"\" Prepare the temporary token arrays for the corpus. \"\"\"\n",
    "\tif 'tokens_array' not in self.results_cache:\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.results_cache['tokens_array'] = self.vocab.sort(by = pl.col('token_id')).select(pl.col('token')).collect(engine='streaming').to_numpy().flatten()\n",
    "\t\tself.results_cache['tokens_array'] = np.insert(self.results_cache['tokens_array'], 0, ERR_TOKEN_STR) # adding a dummy value at the 0 index to align token strings with token_ids\n",
    "\t\tlogger.info(f'Created tokens_array in {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\t\tstart_time = time.time() \n",
    "\t\t# new functionality for disk-based build \n",
    "\t\tself.results_cache['tokens_lookup'] = dict(zip(self.results_cache['tokens_array'], range(len(self.results_cache['tokens_array']))))\n",
    "\t\tlogger.info(f'Created tokens_lookup in {(time.time() - start_time):.3f} seconds')\n",
    "\t\t\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.results_cache['tokens_sort_order'] = self.vocab.sort(by = pl.col('token_id')).select(pl.col('tokens_sort_order')).collect(engine='streaming').to_numpy().flatten()\n",
    "\t\tself.results_cache['tokens_sort_order'] = np.insert(self.results_cache['tokens_sort_order'], 0, 0) # adding a dummy value at the 0 index to align token strings with token_ids\n",
    "\t\tlogger.info(f'Created tokens_sort_order in {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\t\t# start_time = time.time()  # move tokens sort order to build process - takes > 1 second for large corpora, but not needed for all results\n",
    "\t\t# # building tokens_sort_order was implemented in _init_tokens_sort_order - deprecating to simplify as makes sense to build all these in one go\n",
    "\t\t# tokens_array_lower = np.char.lower(self.results_cache['tokens_array'].astype(str))\n",
    "\t\t# self.results_cache['tokens_sort_order'] = np.argsort(np.argsort(tokens_array_lower)) # lowercasing then sorting\t\n",
    "\t\t# logger.info(f'Created tokens_sort_order in {(time.time() - start_time):.3f} seconds')\n",
    "\t\t# del tokens_array_lower\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 22:23:21 - INFO - _init_token_arrays - Created tokens_array in 0.018 seconds\n",
      "2025-06-27 22:23:21 - INFO - _init_token_arrays - Created tokens_lookup in 0.010 seconds\n",
      "2025-06-27 22:23:21 - INFO - _init_token_arrays - Created tokens_sort_order in 0.006 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.8 ms, sys: 20.1 ms, total: 76 ms\n",
      "Wall time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "set_logger_state('verbose')\n",
    "brown.results_cache = {}\n",
    "%time brown._init_token_arrays()\n",
    "set_logger_state('quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_ids_to_tokens(self: Corpus, \n",
    "\t\t\t\t\t\ttoken_ids: np.ndarray|list # token ids to return token strings for \n",
    "\t\t\t\t\t\t) -> np.ndarray: # return token strings for token ids\n",
    "\t\"\"\" Get token strings for a list of token ids. \"\"\" \n",
    "\n",
    "\tself._init_token_arrays()\n",
    "\t\n",
    "\tif isinstance(token_ids, list):\n",
    "\t\ttoken_ids = np.array(token_ids)\n",
    "\tif np.any(token_ids < 0):\n",
    "\t\traise ValueError(\"Token ids must be non-negative integers.\")\n",
    "\t\n",
    "\treturn self.results_cache['tokens_array'][token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def tokens_to_token_ids(self: Corpus, \n",
    "\t\t\t\ttokens: list[str]|np.ndarray[str] # list of tokens to get ids for\n",
    "\t\t\t\t) -> np.ndarray[int]: # array of token ids, 0 for unknown tokens\n",
    "\t\"\"\" Convert a list or np.array of token string to token ids \"\"\"\n",
    "\t\n",
    "\tself._init_token_arrays()\n",
    "\t\n",
    "\tif isinstance(tokens, list):\n",
    "\t\ttokens = np.array(tokens, dtype=str)\n",
    "\t\n",
    "\treturn np.array([self.results_cache['tokens_lookup'].get(token, 0) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_to_id(self: Corpus, \n",
    "\t\t\t\ttoken: str # token to get id for\n",
    "\t\t\t\t) -> int: # return token id (0 if token not found in the corpus)\n",
    "\t\"\"\" Get the token id of a token string. \"\"\"\n",
    "\n",
    "\ttoken_ids = self.tokens_to_token_ids([token])\n",
    "\treturn int(token_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list or numpy array of token strings can be converted to a numpy array of token ids like this using `tokens_to_token_ids` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15682 37698 47121 13458   526 16875 22848 25923 23289]\n"
     ]
    }
   ],
   "source": [
    "tokens = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "token_ids = brown.tokens_to_token_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reverse this use `token_ids_to_tokens` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The' 'quick' 'brown' 'fox' 'jumps' 'over' 'the' 'lazy' 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens = brown.token_ids_to_tokens(token_ids) # token_ids was set above\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "assert np.array_equal(tokens, np.array(['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokens_to_token_ids` method will return a 0 for any tokens not in the corpus vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21572, 28602,     0, 31327])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['some', 'random', 'gazupinfava', 'words']\n",
    "brown.tokens_to_token_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "assert brown.tokens_to_token_ids(['gazupinfava']) == np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If zero is passed to `token_ids_to_tokens` it will return an error token as shown below. A negative value will raise a ValueError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ERROR: not a token'], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.token_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "assert brown.token_ids_to_tokens(np.array([0])) == np.array([ERR_TOKEN_STR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Token ids must be non-negative integers.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "try:\n",
    "    brown.token_ids_to_tokens([1, 2, 0, -1])\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    assert True\n",
    "else:\n",
    "    assert False, \"Expected ValueError for invalid token id 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `token_to_id` method wraps `tokens_to_token_ids`. You can pass a single token string and get the token id back. As with `tokens_to_token_ids`, if the token is not in the vocabulary it will return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47121\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(brown.token_to_id('brown')) # returns token id\n",
    "print(brown.token_to_id('Supercalifragilisticexpialidocious')) # returns 0 if token not in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert type(brown.token_to_id('brown')) == int\n",
    "assert brown.token_to_id('Supercalifragilisticexpialidocious') == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_ids_to_sort_order(self: Corpus, \n",
    "\t\t\t\t\t\t\ttoken_ids: np.ndarray|list # token ids to return token strings for \n",
    "\t\t\t\t\t\t\t) -> np.ndarray: # rank of token ids\n",
    "\t\"\"\" Get the sort order of token strings corresponding to token ids \"\"\"\n",
    "\n",
    "\tself._init_token_arrays()\t\n",
    "\n",
    "\tif isinstance(token_ids, list):\n",
    "\t\ttoken_ids = np.array(token_ids)\n",
    "\tif np.any(token_ids < 0):\n",
    "\t\traise ValueError(\"Token ids must be non-negative integers.\")\n",
    "\t\n",
    "\treturn self.results_cache['tokens_sort_order'][token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The' 'quick' 'brown' 'fox' 'jumps' 'over' 'the' 'lazy' 'dog']\n",
      "[15682 37698 47121 13458   526 16875 22848 25923 23289]\n",
      "[50086 40359  7940 20497 27663 35982 50087 29054 15849]\n",
      "['brown' 'dog' 'fox' 'jumps' 'lazy' 'over' 'quick' 'The' 'the']\n"
     ]
    }
   ],
   "source": [
    "tokens = np.array(['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'])\n",
    "token_ids = brown.tokens_to_token_ids(tokens)\n",
    "sort_order = brown.token_ids_to_sort_order(token_ids)\n",
    "sorted_tokens = tokens[np.argsort(sort_order)]\n",
    "\n",
    "print(tokens)\n",
    "print(token_ids)\n",
    "print(sort_order)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "tokens = np.array(['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'])\n",
    "token_ids = brown.tokens_to_token_ids(tokens)\n",
    "sort_order = brown.token_ids_to_sort_order(token_ids)\n",
    "sorted_tokens = tokens[np.argsort(sort_order)]\n",
    "assert np.array_equal(sorted_tokens, np.array(['brown', 'dog', 'fox', 'jumps', 'lazy', 'over', 'quick', 'The', 'the']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_token_count_text(self: Corpus, \n",
    "\t\t\t\t\texclude_punctuation:bool = False # exclude punctuation tokens from the count\n",
    "\t\t\t\t\t) -> tuple[int, str, str]: # token count with adjustments based on exclusions, token descriptor, total descriptor\n",
    "\t\"\"\" Get the token count for the corpus with adjustments and text for output \"\"\"\n",
    "\n",
    "\tcount_tokens = self.token_count\n",
    "\ttokens_descriptor = 'word and punctuation tokens'\n",
    "\ttotal_descriptor = 'Total word and punctuation tokens'\n",
    "\tif exclude_punctuation:\n",
    "\t\tcount_tokens = self.word_token_count\n",
    "\t\ttokens_descriptor = 'word tokens'\n",
    "\t\ttotal_descriptor = 'Total word tokens'\n",
    "\n",
    "\treturn count_tokens, tokens_descriptor, total_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert toy.get_token_count_text(exclude_punctuation=True) == (32, 'word tokens', 'Total word tokens')\n",
    "assert toy.get_token_count_text(exclude_punctuation=False) == (38, 'word and punctuation tokens', 'Total word and punctuation tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def tokenize(self: Corpus, \n",
    "\t\t\t string:str, # string to tokenize \n",
    "\t\t\t#  return_tokens = False, # return token strings\n",
    "\t\t\t simple_indexing = False # use simple indexing\n",
    "             ): # return tokenized string\n",
    "\t\"\"\" Tokenize a string using the Spacy tokenizer. \"\"\"\n",
    "\t# NOTE: when extending this function - ensure get_token_positions is compatible (e.g. currently assumes fixed sequence length of sequences)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tplaceholder_string = 'zzxxzzplaceholderzzxxzz' # so doesn't split tokens\n",
    "\tis_wildcard_search = False\n",
    "\tif simple_indexing == True:\n",
    "\t\tindex_id = LOWER\n",
    "\t\tstrings_to_tokenize = [string.strip()]\n",
    "\telse:\n",
    "\t\traise('only simple_indexing implemented')\n",
    "\t\t# retained for future rework\n",
    "\t\t# if '*' in string:\n",
    "\t\t# \tis_wildcard_search = True\n",
    "\t\t# \tstring = string.replace('*',placeholder_string)\n",
    "\t\t# if string.islower() == True:\n",
    "\t\t# \tindex_id = LOWER\n",
    "\t\t# else:\n",
    "\t\t# \tindex_id = ORTH\n",
    "\t\t# if '|' in string:\n",
    "\t\t# \tstrings_to_tokenize = string.split('|')\n",
    "\t\t# else:\n",
    "\t\t# \tstrings_to_tokenize = [string.strip()]\n",
    "\ttoken_sequences = []\n",
    "\tfor doc in self._nlp.pipe(strings_to_tokenize): # was tokenizer.pipe(strings_to_tokenize) - retaining for reference\n",
    "\t\t# token_sequences.append(tuple(doc.to_array(index_id))) # not using spacy indexes once corpus created\n",
    "\t\ttoken_sequences.append(list(doc))\n",
    "\t# if is_wildcard_search == True:\n",
    "\t# \ttmp_token_sequence = []\n",
    "\t# \tsequence_count = 1\n",
    "\t# \tfor token in doc:\n",
    "\t# \t\ttmp_token_sequence.append([])\n",
    "\t# \t\tif placeholder_string in token.text:\n",
    "\t# \t\t\tchunked_string = token.text.split(placeholder_string)\n",
    "\t# \t\t\tif len(chunked_string) > 2 or (len(chunked_string) == 2 and chunked_string[0] != '' and chunked_string[1] != ''):\n",
    "\t# \t\t\t\t# use regex\n",
    "\t# \t\t\t\tapproach = 'regex'\n",
    "\t# \t\t\t\tregex = re.compile('.*'.join(chunked_string))\n",
    "\t# \t\t\telif chunked_string[0] == '':\n",
    "\t# \t\t\t\tapproach = 'endswith'\n",
    "\t# \t\t\telse:\n",
    "\t# \t\t\t\tapproach = 'startswith'\n",
    "\t# \t\t\tfor token_id in loaded_corpora[corpus_name]['frequency_lookup']:\n",
    "\t# \t\t\t\tpossible_word = False\n",
    "\t# \t\t\t\tword = loaded_corpora[corpus_name]['vocab'][token_id]\n",
    "\t# \t\t\t\tif approach == 'regex':\n",
    "\t# \t\t\t\t\tif regex.match(word):\n",
    "\t# \t\t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\telif getattr(word,approach)(''.join(chunked_string)):\n",
    "\t# \t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\tif possible_word != False:\n",
    "\t# \t\t\t\t\ttmp_token_sequence[token.i].append(loaded_corpora[corpus_name]['vocab'][possible_word])\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttmp_token_sequence[token.i].append(token.orth)\n",
    "\t# \t\tsequence_count *= len(tmp_token_sequence[token.i])\n",
    "\t# \trotated_token_sequence = []\n",
    "\t# \ttoken_repeat = sequence_count\n",
    "\t# \tfor pos in range(len(tmp_token_sequence)):\n",
    "\t# \t\trotated_token_sequence.append([])\n",
    "\t# \t\tif len(tmp_token_sequence[pos]) == 1:\n",
    "\t# \t\t\trotated_token_sequence[pos] += sequence_count * [tmp_token_sequence[pos][0]]\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttoken_repeat = token_repeat // len(tmp_token_sequence[pos])\n",
    "\t# \t\t\twhile len(rotated_token_sequence[pos]) < sequence_count:\n",
    "\t# \t\t\t\tfor token in tmp_token_sequence[pos]:\n",
    "\t# \t\t\t\t\trotated_token_sequence[pos] += token_repeat * [token]\n",
    "\t# \ttoken_sequences = list(zip(*rotated_token_sequence))\n",
    "\t# \t#for tokens in tmp_token_sequence:\n",
    "\t# \t#    for token in tokens:\n",
    "\t# covert token_sequences to reindexed tokens using original_to_new\n",
    "\t\n",
    "\t# convert sequences to lower case\n",
    "\tif index_id == LOWER:\n",
    "\t\ttoken_sequences = [[token.lower_ for token in sequence] for sequence in token_sequences]\n",
    "\ttoken_sequences = [tuple(self.tokens_to_token_ids(sequence)) for sequence in token_sequences]\n",
    "\t\n",
    "\tlogger.info(f'Tokenization time: {(time.time() - start_time):.5f} seconds')\n",
    "\t# if return_tokens == True:\n",
    "\t\t# return token_sequences, index_id, doc\n",
    "\t# else:\n",
    "\treturn token_sequences, index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.int64(23289),) LOWER\n",
      "(np.int64(23289),) LOWER\n",
      "(np.int64(47121), np.int64(13458)) LOWER\n",
      "(np.int64(22848), np.int64(47121), np.int64(13458)) LOWER\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "token_strs = ['dog', 'Dog', 'Brown Fox', 'the brown fox']\n",
    "\n",
    "for token_str in token_strs:\n",
    "    brown_token_sequence, brown_index_id = brown.tokenize(token_str, simple_indexing=True)\n",
    "    print(brown_token_sequence[0], spacy_attribute_name(brown_index_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with specific texts in the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _get_text(self:Corpus,\n",
    "        doc_id: int, # the id of the document\n",
    "        return_df: bool = True # returns the df with \n",
    "        ):\n",
    "    \"\"\" Get tokens, space definitions and metadata for a text in the corpus \"\"\"\n",
    "    \n",
    "    if doc_id < 1 or doc_id > self.document_count:\n",
    "        raise ValueError(f\"Document ID {doc_id} is out of range. Document ID should be between 1 and the count of documents ({self.document_count}).\")\n",
    "\n",
    "    doc_tokens = self.tokens.with_row_index('position').filter(pl.col('token2doc_index') == doc_id).with_columns(pl.lit(1).alias('not_space'))\n",
    "    doc_space_tokens = self.spaces.filter(pl.col('token2doc_index') == doc_id).with_columns(pl.lit(0).alias('not_space'))\n",
    "    doc_tokens_df = pl.concat([doc_tokens, doc_space_tokens]).sort('position', 'not_space') #.drop('position').drop('not_space') # here - creating as df for return_df\n",
    "\n",
    "    doc_tokens = doc_tokens_df.select(['orth_index', 'has_spaces']).collect()\n",
    "    tokens = self.token_ids_to_tokens(doc_tokens.select(pl.col('orth_index')).to_numpy().flatten())\n",
    "    has_spaces = doc_tokens.select(pl.col('has_spaces')).to_numpy().flatten()\n",
    "    metadata = self.metadata.with_row_index(offset = 1, name = 'document_id').filter(pl.col('document_id') == doc_id).collect()\n",
    "    if return_df == True:\n",
    "        return tokens, has_spaces, metadata, doc_tokens_df\n",
    "    else:\n",
    "        return tokens, has_spaces, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def text(self:Corpus,\n",
    "        doc_id: int # the id of the document\n",
    "        ):\n",
    "    \"\"\" Get a text document \"\"\"\n",
    "\n",
    "    return Text(*self._get_text(doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert str(toy.text(1)) == 'The cat sat on the mat.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find positions of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_tokens_by_index(self: Corpus, \n",
    "\t\t\t   index: str = 'orth_index', # index to get tokens from i.e. 'orth_index' 'lower_index' 'token2doc_index'\n",
    "\t\t\t   exclude_punctuation: bool = False, # exclude punctuation tokens from the result (unused currently)\n",
    "\t\t\t\t) -> np.ndarray:\n",
    "\t\"\"\" Get tokens for a given index. \"\"\"\n",
    "\n",
    "\tlogger.debug(f'Getting tokens for index: {index}')\n",
    "\tstart_time = time.time()\n",
    "\tif index not in ['orth_index', 'lower_index', 'token2doc_index']:\n",
    "\t\traise ValueError(\"Index must be one of 'orth_index', 'lower_index', 'token2doc_index'\")\n",
    "\n",
    "\tcache_key = index\n",
    "\tif exclude_punctuation:\n",
    "\t\tcache_key += '-nopuncts'\n",
    "\tif cache_key in self.results_cache:\n",
    "\t\tlogger.info(f'Tokens for index {index} with exclude_punctuation {exclude_punctuation} already cached, returning cached result in {(time.time() - start_time):.3f} seconds')\n",
    "\t\treturn self.results_cache[cache_key]\n",
    "\telse:\n",
    "\t\tif index not in self.results_cache: # in case build -nopuncts first - get both sorted\n",
    "\t\t\tself.results_cache[index] = self.tokens.select(pl.col(index)).collect(engine='streaming').to_numpy().flatten()\n",
    "\t\tif exclude_punctuation == False:\n",
    "\t\t\tlogger.info(f'Got tokens for index {index} with exclude_punctuation {exclude_punctuation} in {(time.time() - start_time):.3f} seconds')\n",
    "\t\t\treturn self.results_cache[index]\n",
    "\t\telse:\n",
    "\t\t\tif 'puncts' not in self.results_cache:\n",
    "\t\t\t\tself.results_cache['puncts'] = self.puncts.select(pl.col('position')).collect(engine='streaming').to_numpy().flatten()\n",
    "\t\t\tself.results_cache[cache_key] = np.delete(self.results_cache[index], self.results_cache['puncts'])\n",
    "\t\t\tself.results_cache[f'{cache_key}-positions'] = np.delete(np.arange(len(self.results_cache[index])), self.results_cache['puncts'])\n",
    "\t\t\t#self.results_cache[f'{index}-nopuncts'] = self.tokens.with_row_index('position').select(pl.col('position'), pl.col(index)).join(self.puncts.select('position'), on='position', how='anti').drop('position').collect(engine='streaming').to_numpy().flatten()\n",
    "\t\t\tlogger.info(f'Got tokens for index {index} with exclude_punctuation {exclude_punctuation} in {(time.time() - start_time):.3f} seconds')\n",
    "\t\t\treturn self.results_cache[cache_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 22:23:21 - DEBUG - get_tokens_by_index - Getting tokens for index: orth_index\n",
      "2025-06-27 22:23:21 - INFO - get_tokens_by_index - Got tokens for index orth_index with exclude_punctuation False in 0.021 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens array: 1139266\n",
      "[15682  4361 14610 54713 45742 53250  8699 45680 30305  2739]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 22:23:21 - INFO - _init_token_arrays - Created tokens_array in 0.012 seconds\n",
      "2025-06-27 22:23:21 - INFO - _init_token_arrays - Created tokens_lookup in 0.004 seconds\n",
      "2025-06-27 22:23:21 - INFO - _init_token_arrays - Created tokens_sort_order in 0.005 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The' 'Fulton' 'County' 'Grand' 'Jury' 'said' 'Friday' 'an'\n",
      " 'investigation' 'of']\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "set_logger_state('verbose')\n",
    "brown.results_cache = {}\n",
    "tokens = brown.get_tokens_by_index('orth_index')\n",
    "print(f'Length of tokens array: {len(tokens)}')\n",
    "print(tokens[100:110])  # print first 10 tokens\n",
    "print(brown.token_ids_to_tokens(tokens[100:110]))  # print first 10 token strings\n",
    "set_logger_state('quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 22:23:21 - INFO - memory_usage - init, memory usage: 4937.19921875 MB\n",
      "2025-06-27 22:23:21 - INFO - load - Load time: 0.222 seconds\n",
      "2025-06-27 22:23:22 - DEBUG - get_tokens_by_index - Getting tokens for index: orth_index\n",
      "2025-06-27 22:23:23 - INFO - get_tokens_by_index - Got tokens for index orth_index with exclude_punctuation False in 1.462 seconds\n",
      "2025-06-27 22:23:23 - INFO - _init_token_arrays - Created tokens_array in 0.118 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens array size in memory: 383.006 MB\n",
      "Length of tokens array: 100402793\n",
      "[767113 514572 635422 628652 789648 190869 374120 714548 499625 283795]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 22:23:23 - INFO - _init_token_arrays - Created tokens_lookup in 0.243 seconds\n",
      "2025-06-27 22:23:24 - INFO - _init_token_arrays - Created tokens_sort_order in 0.042 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.' 'Speaker' '.' 'with' 'regard' 'to' 'the' 'House' 'investigation'\n",
      " 'on']\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# THIS WILL BREAK CI AS CONGRESS DATA NOT UP - SO COMMENT OUT BEFORE COMMIT\n",
    "# set_logger_state('verbose')\n",
    "# congress = Corpus().load(f'{save_path}/us-congressional-speeches-subset-500k.corpus')\n",
    "# congress.results_cache = {}\n",
    "# tokens = congress.get_tokens_by_index('orth_index', exclude_punctuation=False)\n",
    "# import sys\n",
    "# tokens_size_mb = sys.getsizeof(tokens) / (1024 * 1024)\n",
    "# print(f'Tokens array size in memory: {tokens_size_mb:.3f} MB')\n",
    "# print(f'Length of tokens array: {len(tokens)}')\n",
    "# print(tokens[100:110])  # print first 10 tokens\n",
    "# print(congress.token_ids_to_tokens(tokens[100:110]))  # print first 10 token strings\n",
    "# set_logger_state('quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 22:23:24 - INFO - memory_usage - init, memory usage: 3882.6015625 MB\n",
      "2025-06-27 22:23:24 - INFO - load - Load time: 0.265 seconds\n",
      "2025-06-27 22:23:24 - DEBUG - get_tokens_by_index - Getting tokens for index: orth_index\n",
      "2025-06-27 22:23:25 - INFO - get_tokens_by_index - Got tokens for index orth_index with exclude_punctuation True in 1.415 seconds\n",
      "2025-06-27 22:23:25 - INFO - _init_token_arrays - Created tokens_array in 0.114 seconds\n",
      "2025-06-27 22:23:26 - INFO - _init_token_arrays - Created tokens_lookup in 0.205 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens array size in memory: 346.535 MB\n",
      "Length of tokens array: 90842144\n",
      "[767113 514572 628652 789648 190869 374120 714548 499625 283795 374120]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 22:23:26 - INFO - _init_token_arrays - Created tokens_sort_order in 0.033 seconds\n",
      "2025-06-27 22:23:26 - DEBUG - get_tokens_by_index - Getting tokens for index: orth_index\n",
      "2025-06-27 22:23:26 - INFO - get_tokens_by_index - Tokens for index orth_index with exclude_punctuation True already cached, returning cached result in 0.000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.' 'Speaker' 'with' 'regard' 'to' 'the' 'House' 'investigation' 'on'\n",
      " 'the']\n",
      "[100 101 103 104 105 106 107 108 109 110]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# THIS WILL BREAK CI AS CONGRESS DATA NOT UP - SO COMMENT OUT BEFORE COMMIT\n",
    "# import sys\n",
    "# set_logger_state('verbose')\n",
    "# congress = Corpus().load(f'{save_path}/us-congressional-speeches-subset-500k.corpus')\n",
    "# congress.results_cache = {}\n",
    "# tokens = congress.get_tokens_by_index('orth_index', exclude_punctuation=True)\n",
    "# tokens_size_mb = sys.getsizeof(tokens) / (1024 * 1024)\n",
    "# print(f'Tokens array size in memory: {tokens_size_mb:.3f} MB')\n",
    "# print(f'Length of tokens array: {len(tokens)}')\n",
    "# print(tokens[100:110])  # print first 10 tokens\n",
    "# print(congress.token_ids_to_tokens(tokens[100:110]))  # print first 10 token strings\n",
    "# print(congress.results_cache['orth_index-nopuncts-positions'][100:110])  # print first 10 token strings\n",
    "# # %timeit np.delete(congress.results_cache['orth_index'], congress.results_cache['puncts'])\n",
    "# # %timeit np.setdiff1d(np.arange(len(congress.results_cache['orth_index'])), congress.results_cache['puncts'], assume_unique=True)\n",
    "# # %timeit np.delete(np.arange(len(congress.results_cache['orth_index'])), congress.results_cache['puncts'])\n",
    "# tokens = congress.get_tokens_by_index('orth_index', exclude_punctuation=True)\n",
    "# set_logger_state('quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_ngrams_by_index(self: Corpus, \n",
    "\t\t\t\tngram_length:int, # length of ngrams to get\n",
    "\t\t\t\tindex:str,  # index to get tokens from, e.g. 'orth_index' 'lower_index'\n",
    "\t\t\t\texclude_punctuation: bool = False # exclude punctuation tokens from the result (unused currently)\n",
    "\t\t\t\t) -> np.ndarray:\n",
    "\t\"\"\" Get ngrams for a given index and ngram length. \"\"\"\n",
    "\n",
    "\tif index not in ['orth_index', 'lower_index']:\n",
    "\t\traise ValueError(\"Index must be either 'orth_index' or 'lower_index'\")\n",
    "\n",
    "\tif (index, ngram_length, exclude_punctuation) not in self.ngram_index:\n",
    "\t\tslices = []\n",
    "\t\t[slices.append(np.roll(self.get_tokens_by_index(index, exclude_punctuation), shift)) for shift in -np.arange(ngram_length)]\n",
    "\t\tseq = np.vstack(slices).T\n",
    "\t\tself.ngram_index[(index, ngram_length, exclude_punctuation)] = seq\n",
    "\n",
    "\treturn self.ngram_index[(index, ngram_length, exclude_punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  6],\n",
       "       [ 6, 12],\n",
       "       [12,  8],\n",
       "       [ 8, 10],\n",
       "       [10, 13],\n",
       "       [13, 15],\n",
       "       [15, 17],\n",
       "       [17, 10],\n",
       "       [10, 11],\n",
       "       [11, 12]], dtype=uint32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy.get_ngrams_by_index(ngram_length=2, index='lower_index')[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# congress = Corpus().load(f'{save_path}/us-congressional-speeches-subset-500k.corpus')\n",
    "# sys.getsizeof(congress.get_tokens_by_index('orth_index'))/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_token_positions(self: Corpus, \n",
    "\t\t\t\t\ttoken_sequence: list[np.ndarray], # token sequence to get index for \n",
    "\t\t\t\t\tindex_id: int, # index to search (i.e. ORTH, LOWER)\n",
    "\t\t\t\t\texclude_punctuation: bool = False # exclude punctuation tokens from the result (unused currently)\n",
    "\t\t\t\t\t) -> np.ndarray: # positions of token sequence\n",
    "\t\"\"\" Get the positions of a token sequence in the corpus. \"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tresults = []\n",
    "\n",
    "\tsequence_len = len(token_sequence[0]) # Check when extend tokenization\n",
    "\tvariants_len = len(token_sequence)\n",
    "\n",
    "\tif index_id == ORTH:\n",
    "\t\tindex = 'orth_index'\n",
    "\telse:\n",
    "\t\tindex = 'lower_index'\n",
    "\n",
    "\tif variants_len == 1:\n",
    "\t\tresults.append(np.where(np.all(self.get_ngrams_by_index(ngram_length = sequence_len, index = index, exclude_punctuation = exclude_punctuation) == token_sequence[0], axis=1))[0])\n",
    "\telse:\n",
    "\t\tcondition_list = []\n",
    "\t\tchoice_list = variants_len * [True]\n",
    "\t\tfor seq in token_sequence:\n",
    "\t\t\tcondition_list.append(self.get_ngrams_by_index(ngram_length = sequence_len, index = index, exclude_punctuation = exclude_punctuation) == seq)\n",
    "\t\tresults.append(np.where(np.all(np.select(condition_list, choice_list),axis=1))[0])\n",
    "\n",
    "\tlogger.info(f'Token indexing ({len(results[0])}) time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  18833,   18870,   18880,   18950,   18957,   37578,   88691,\n",
      "        125019,  137037,  137687,  137722,  137731,  137775,  143860,\n",
      "        188374,  248842,  248982,  249204,  249217,  249243,  249311,\n",
      "        249337,  249397,  249425,  249535,  250476,  250495,  250554,\n",
      "        250613,  250645,  250699,  250709,  251033,  252740,  253700,\n",
      "        255256,  255360,  255532,  330282,  359785,  437987,  437991,\n",
      "        438046,  438051,  463456,  463485,  463507,  521175,  648316,\n",
      "        694080,  694129,  694289,  694481,  694760,  695139,  695216,\n",
      "        695313,  861865,  861872,  863503,  863521,  875531,  875573,\n",
      "        875660,  887598,  994901, 1012130, 1028088, 1050598, 1050607,\n",
      "       1052032, 1074911, 1084765, 1086020, 1086052, 1086639, 1104994,\n",
      "       1128317, 1137426])]\n"
     ]
    }
   ],
   "source": [
    "token_str = 'dog'\n",
    "token_sequence, index_id = brown.tokenize(token_str, simple_indexing=True)\n",
    "token_positions = brown.get_token_positions(token_sequence, index_id)\n",
    "print(token_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "token_str = 'dog'\n",
    "token_sequence, index_id = toy.tokenize(token_str, simple_indexing=True)\n",
    "token_positions = toy.get_token_positions(token_sequence, index_id)\n",
    "assert np.array_equal(token_positions[0], np.array([109, 123, 137])) # validated using toy.token_ids_to_tokens(toy.get_tokens_by_index('orth_index')[100:-100])\n",
    "assert np.array_equal(toy.token_ids_to_tokens(toy.get_tokens_by_index('orth_index')[token_positions[0]]), np.array(['dog', 'dog', 'dog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _shift_zeroes_to_end(self:Corpus,\n",
    "\t\t\t\t\t\tarr:np.ndarray # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Move 0 value positions for punctuation and space removal, zeroes get moved to the end of each column. \"\"\"\n",
    "\tresult = np.empty_like(arr)\n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tmask = col_data != 0\n",
    "\t\tresult[:mask.sum(), col] = col_data[mask]\n",
    "\t\tresult[mask.sum():, col] = 0\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _shift_zeroes_to_start(self:Corpus,\n",
    "\t\t\t\t\t\tarr:np.ndarray # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Move 0 value positions for punctuation and space removal to the start of each column \"\"\"\n",
    "\tresult = np.empty_like(arr)\n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tmask = col_data != 0\n",
    "\t\tn_zeros = (~mask).sum()\n",
    "\t\tresult[:n_zeros, col] = 0\n",
    "\t\tresult[n_zeros:, col] = col_data[mask]\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _zero_after_value(self:Corpus,\n",
    "\t\t\t\t\t  arr:np.ndarray, # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t  target: int # Target value to find in the array (e.g., an end-of-file token or a specific collocate frequency)\n",
    "\t\t\t\t\t  ):\n",
    "\t\"\"\" Set values from first occurence of target value to 0 in each column (for processing tokens outside text using eof token) \"\"\"\n",
    "\tarr = arr.copy()  \n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tidx = np.where(col_data == target)[0]\n",
    "\t\tif idx.size > 0:\n",
    "\t\t\tfirst_idx = idx[0]\n",
    "\t\t\tarr[first_idx:, col] = 0\n",
    "\treturn arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_tokens_in_context(self:Corpus,\n",
    "\t\t\t\t\t\t\t   token_positions:np.ndarray, # Numpy array of token positions in the corpus\n",
    "\t\t\t\t\t\t\t   index:str, # Index to use - lower_index, orth_index\n",
    "\t\t\t\t\t\t\t   context_length:int = 5, # Number of context words to consider on each side of the token\n",
    "\t\t\t\t\t\t\t   position_offset:int = 1, # offset to start retrieving context words - negatve is left of node, positive for right - may want to adjust if sequence_len > 1\n",
    "\t\t\t\t\t\t\t   position_offset_step:int = 1, # step to move position offset by, this sets direct, -1 for left, 1 for right\n",
    "\t\t\t\t\t\t\t   exclude_punctuation:bool = True, # ignore punctuation from context retrieved\n",
    "\t\t\t\t\t\t\t   convert_eof:bool = True # if True (for collocation functionality), contexts with end of file tokens will have eof token and tokens after set to zero, otherwise EOF retained (e.g. False used for ngrams)\n",
    "\t\t\t\t\t\t\t   ) -> Result:\n",
    "\t\"\"\" Get tokens in context for given token positions, context length and direction, operates one side at a time. \"\"\"\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tif context_length < 1:\n",
    "\t\t# return empty result\n",
    "\t\treturn np.zeros((0, 0), dtype=np.int32)\n",
    "\n",
    "\ttokens_for_removal = []\n",
    "\tif exclude_punctuation:\n",
    "\t\ttokens_for_removal += self.punct_tokens\n",
    "\tlen_tokens_for_removal = len(tokens_for_removal)\n",
    "\n",
    "\tcollected = False\n",
    "\tcontext_tokens_arr = []\n",
    "\twhile collected == False:\n",
    "\t\tnew_positions = np.array(token_positions[0] + position_offset, dtype = token_positions[0].dtype)\n",
    "\t\tcontext_tokens_arr.append(self.get_tokens_by_index(index)[new_positions])\n",
    "\t\tposition_offset += position_offset_step\n",
    "\t\tif len(context_tokens_arr) >= context_length: \n",
    "\t\t\tcontext_tokens = np.array(context_tokens_arr, dtype = token_positions[0].dtype)\n",
    "\t\t\t# shape = (context_length, len(token_positions[0]))\n",
    "\t\t\tlogger.debug(f\"Context tokens collected: {context_tokens.shape}\")\n",
    "\t\t\tif len_tokens_for_removal > 0: # cleaning punctuation and check if need more iterations\n",
    "\t\t\t\tcontext_tokens = np.where(np.isin(context_tokens, tokens_for_removal), 0, context_tokens)\n",
    "\t\t\tcounts = np.count_nonzero(context_tokens, axis=0)\n",
    "\t\t\tif np.min(counts) < context_length:\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\tcollected = True\n",
    "\n",
    "\tcontext_tokens = self._shift_zeroes_to_end(context_tokens)\n",
    "\tcontext_tokens = context_tokens[:context_length, :]\n",
    "\n",
    "\tif convert_eof: # delete any context that contains self.EOF_TOKEN\n",
    "\t\tif self.EOF_TOKEN in context_tokens:\n",
    "\t\t\tcontext_tokens = self._zero_after_value(context_tokens, self.EOF_TOKEN)\n",
    "\n",
    "\tlogger.info(f\"Context retrieved in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\treturn context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_test_corpora(\n",
    "\t\tsource_path:str, # path to folder with corpora\n",
    "\t\tsave_path:str, # path to save corpora\n",
    "\t\tforce_rebuild:bool = False # force rebuild of corpora, useful for development and testing\n",
    "\t\t):\n",
    "\t\"\"\"(Deprecated - moved to conc.corpora) Build all test corpora from source files.\"\"\"\n",
    "\n",
    "\traise DeprecationWarning(\"Calling build_test_corpora via conc.corpus is deprecated and will be removed by v1.0.0, instead import as 'from conc.corpora import build_test_corpora' and use 'build_sample_corpora'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `build_sample_corpora` was accessible via conc.corpus as `build_test_corpora` up to version 0.1.1. Calling it this way will raise a deprecation warning. It will be removed for version 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
