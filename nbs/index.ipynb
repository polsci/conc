{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from conc.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conc\n",
    "\n",
    "> A Python library for efficient corpus analysis, enabling corpus linguistic analysis in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Conc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conc is a Python library that brings corpus linguistic analysis to Jupyter notebooks. A staple of data science, Jupyter notebooks are a great model for presenting analysis that combines code, reporting and discussion in a way that can be reproduced. Conc aims to allow researchers to analyse large corpora in efficient ways using standard hardware, with the ability to produce clear, publication-ready reports and extend analysis where required using standard Python libraries.\n",
    "\n",
    "Conc uses [spaCy](https://spacy.io/) for tokenising texts. More spaCy functionality will be supported in future releases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conc Principles  \n",
    "\n",
    "* use standard Python libraries for data analysis (i.e. Numpy, Scipy, Jupyterlab)\n",
    "* use vector operations where possible  \n",
    "* use fast code libraries over slow code libraries (i.e. Conc uses [Polars vs Pandas](https://pola.rs/posts/benchmarks/) - you can still output Pandas dataframes if you want to use them)  \n",
    "* provide important information when reporting results  \n",
    "* pre-compute time-intensive and repeatedly used views of the data  \n",
    "* work with smaller slices of the data where possible  \n",
    "* cache specific anaysis during a session to reduce computation for repeated calls  \n",
    "* document corpus representations so that they can be worked with directly  \n",
    "* provide a way to work with access Conc results for further processing with standard Python libraries  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conc is in active development. It is currently [released][pypi] for beta testing. The Github site may be ahead of the Pypi version, so for latest functionality install from Github (see below). The Github code is pre-release and may change. For the latest release, install from Pypi (`pip install conc`). The [documentation][docs] reflects the most recent functionality. See the [CHANGELOG][changelog] for notes on releases and the Roadmap below for upcoming features.  \n",
    "\n",
    "[repo]: https://github.com/polsci/conc\n",
    "[docs]: https://geoffford.nz/conc/\n",
    "[pypi]: https://pypi.org/project/conc/\n",
    "[changelog]: https://github.com/polsci/conc/blob/main/CHANGELOG.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conc is developed by [Dr Geoff Ford](https://geoffford.nz/).\n",
    "\n",
    "Conc originated in my PhD research, which included development of a web-based corpus browser to handle analysis of large corpora. I've been developing Conc through my subsequent research.  \n",
    "\n",
    "Work to create this Python library has been made possible by funding from the Royal Society of New Zealand’s Marsden Fund Grant:  \n",
    "\n",
    "- \"Mapping LAWS: Issue Mapping and Analyzing the Lethal Autonomous Weapons Debate\" (19-UOC-068)  \n",
    "- \"Into the Deep: Analysing the Actors and Controversies Driving the Adoption of the World’s First Deep Sea Mining Governance\" 22-UOC-059 .  \n",
    "\n",
    "Conc is an output of both projects.  \n",
    "\n",
    "Thanks to the Mapping LAWS project team for their support and feedback as first users of ConText (a web-based application built on an earlier version of Conc).  \n",
    "\n",
    "Dr Ford is a researcher with [Te Pokapū Aronui ā-Matihiko | UC Arts Digital Lab (ADL)](https://artsdigitallab.canterbury.ac.nz/). Thanks to the ADL team and the ongoing support of the University of Canterbury's Faculty of Arts who make work like this possible.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install via pip\n",
    "\n",
    "You can install Conc from [pypi][pypi] using this command:   \n",
    "\n",
    "```sh\n",
    "$ pip install conc\n",
    "```\n",
    "\n",
    "To install the latest development version of Conc, which may be ahead of the version on Pypi, you can install from the [repository][repo]:  \n",
    "\n",
    "```sh\n",
    "$ pip install git+https://github.com/polsci/conc.git\n",
    "```\n",
    "\n",
    "### Install a language model\n",
    "\n",
    "The first releases of Conc require a SpaCy language model for tokenization. After installing Conc, install a model. Here's an example of how to install SpaCy's small English model, which is Conc's default language model:  \n",
    "\n",
    "```sh\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "If you are working with a different language or want to use a different 'en' model, check the [SpaCy models documentation](https://spacy.io/models/) for the relevant model name.\n",
    "\n",
    "### Install optional dependencies\n",
    "\n",
    "Conc has some optional dependencies you can install to download source texts to create sample corpora. These are primarily intended for creating corpora for development. To minimize Conc's requirements these are not installed by default. If you want to get sample corpora to test out Conc's functionality you can install these with the following command. \n",
    "\n",
    "```sh\n",
    "$ pip install nltk requests datasets\n",
    "```\n",
    "\n",
    "[repo]: https://github.com/polsci/conc\n",
    "[docs]: https://geoffford.nz/conc/\n",
    "[pypi]: https://pypi.org/project/conc/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-2013 CPU? Install Polars with support for older machines\n",
    "\n",
    "Polars is optimized for modern CPUs with support for AVX2 instructions. If you get kernel crashes running Conc on an older machine (probably pre-2013), this is likely to be an issue with Polars. Polars has an [alternate installation option to support older machines](https://docs.pola.rs/user-guide/installation/), which installs a Polars build compiled without AVX2 support. Replace the standard Polars package with the legacy-support package to use Conc on older machines.\n",
    "\n",
    "```sh\n",
    "$ pip uninstall polars\n",
    "$ pip install polars-lts-cpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Conc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good place to start is TODO, which demonstrates how to build a corpus and output Conc reports.   \n",
    "\n",
    "The [documentation site][docs] provides a reference for Conc functionality and examples of how to create reports for analysis. The current Conc components are listed below. \n",
    "\n",
    "[repo]: https://github.com/polsci/conc\n",
    "[docs]: https://geoffford.nz/conc/\n",
    "[pypi]: https://pypi.org/project/conc/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Class / Function | Module | Functionality | Note |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "| `Corpus` | conc.corpus | Build and load and get information on a corpus, methods to work with a corpus | Required |\n",
    "| `Conc` | conc.conc | Inferface to Conc reports for corpus analysis | Recommended way to access reports for analysis, requires a corpus created by Corpus module |\n",
    "| `Text` | conc.text |Output text from the corpus | Access via Corpus |\n",
    "| `Frequency` | conc.frequency | Frequency reporting | Access via Conc |\n",
    "| `Ngrams` | conc.ngrams | Reporting on `ngram_frequencies` across corpus and `ngrams` containing specific tokens | Access via Conc |\n",
    "| `Concordance` | conc.concordance | Concordancing | Access via Conc |\n",
    "| `Keyness` | conc.keyness | Reporting for keyness analysis | Access via Conc |\n",
    "| `Collocates` | conc.collocates | Reporting for collocation analysis | Access via Conc |\n",
    "| `Result` | conc.result | Handles report results, output result as table or get dataframe | Used by all reports |\n",
    "| `ConcLogger` | conc.core | Logger | Logging implemented in all modules |\n",
    "| `CorpusMetadata` | conc.core | Class to validate Corpus Metadata JSON | Used by Corpus class |\n",
    "\n",
    "The conc.core module implements a number of helpful functions ...\n",
    "\n",
    "| Function | Functionality |\n",
    "| -------- | ------- |\n",
    "| `list_corpora` | Scan a directory for corpora and return a summary |\n",
    "| `get_stop_words` | Get a spaCy stop word list list for a specific model |\n",
    "| Various - see `Get data sources` | Functions to download source texts to create sample corpora. Primarily intended for development/testing. To minimize requirements not all libraries are installed by default. Functions will raise errors with information on installing required libraries. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO before release  \n",
    "\n",
    "- [ ] review ngram exclude punctuation and spaces - not treating same way between functions\n",
    "    - excluding spaces makes sense, but punctuation could be reason to exclude \n",
    "    - are there ever multiple consecutive spaces? if not, could just always get +1 range and cleanup\n",
    "    - i.e. 'economy ! The' could be legit bigram, but probably don't want 'economy The' from this as crosses punct\n",
    "    - approach already in ngram_frequencies may already be sufficient\n",
    "    - but space would just be removed\n",
    "- [ ] ngram frequencies counts - impacted by above\n",
    "- [ ] ngram_frequencies to report\n",
    "- [ ] align ngram api and display with frequencies (e.g. normalized frequency)\n",
    "- [x] work out requirements\n",
    "- [x] Work out polars legacy build and make note about installation\n",
    "- [x] Complete explanation on index page\n",
    "- [x] Complete install (use Textplumber example)\n",
    "- [x] Use textplumber stylesheet\n",
    "- [x] Moved to do items through code/markdown\n",
    "- [x] Use github actions developed for textplumber\n",
    "- [x] add changelog\n",
    "- [x] check license against corpress   \n",
    "- [x] revise tests for build and save and load - write test for corpus build metadata - testing counts, vocab, metadata and token positions\n",
    "- [x] tidy depreciated from Corpus\n",
    "\n",
    "## Final release process ...\n",
    "- [ ] generate README etc\n",
    "- [ ] ensure local tests work via nbdev_prepare\n",
    "- [ ] ensure github CI tests work\n",
    "- [ ] check generated documentation\n",
    "- [ ] Bump version\n",
    "- [ ] Pypi release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "### Short-term\n",
    "\n",
    "- [ ] add tutorial / getting started notebook\n",
    "- [ ] add citation information\n",
    "- [ ] add support for build from datasets library\n",
    "- [ ] anatomy - explain token2doc_index -1 and has_spaces on tokens display and various other fields for vocab.\n",
    "- [ ] Corpus tokenize support for functionality from earlier versions of Conc for wildcards, multiple strings, case insensitive tokenization\n",
    "- [ ] get_ngrams_by_index - adjust for case insensitive\n",
    "- [ ] improve concordance ordering so not fixed options e.g. include 3R1R2R\n",
    "- [ ] concordancing - add in ordering by metadata columns or doc\n",
    "- [ ] annotations support for POS, TAG, SENT_START, LEMMA \n",
    "- [ ] move tokens sort order to build process - takes > 1 second for large corpora, but not needed for all results\n",
    "- [ ] revisit polars streaming - potentially implement a batched write for very large files i.e. splitting vocab/tokens files into smaller chunks to reduce memory usage.\n",
    "\n",
    "### Medium-term\n",
    "\n",
    "- [ ] Support for processing backends other than spaCy (i.e. other tokenizers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developer Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instructions below are only relevant if you want to contribute to Conc. The [nbdev](https://nbdev.fast.ai/) library is being used for development. If you are new to using nbdevc, here are some useful pointers to get you started (or visit the [nbdev website](https://nbdev.fast.ai/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install conc in Development mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "# make sure conc package is installed in development mode\n",
    "$ pip install -e .\n",
    "\n",
    "# make changes under nbs/ directory\n",
    "# ...\n",
    "\n",
    "# compile to have changes apply to conc\n",
    "$ nbdev_prepare\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
