{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocates\n",
    "\n",
    "> Functionality for collocation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp collocates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import time\n",
    "import polars as pl\n",
    "from fastcore.basics import patch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc.corpus import Corpus\n",
    "from conc.result import Result\n",
    "from conc.core import logger, PAGE_SIZE, set_logger_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "np.set_printoptions(suppress=True)\n",
    "set_logger_state('verbose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = f'{os.environ.get(\"HOME\")}/data/'\n",
    "save_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'\n",
    "\n",
    "path_to_toy_corpus = f'{save_path}toy.corpus'\n",
    "path_to_brown_corpus = f'{save_path}brown.corpus'\n",
    "path_to_reuters_corpus = f'{save_path}reuters.corpus'\n",
    "path_to_gutenberg_corpus = f'{save_path}gutenberg.corpus'\n",
    "path_to_gardenparty_corpus = f'{save_path}garden-party.corpus'\n",
    "path_to_congress_corpus = f'{save_path}us-congressional-speeches-subset-100k.corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 14:04:12 - INFO - memory_usage - init, memory usage: 1775.4140625 MB\n",
      "2025-06-06 14:04:13 - INFO - load - Load time: 0.207 seconds\n",
      "2025-06-06 14:04:13 - INFO - memory_usage - init, memory usage: 1775.4140625 MB\n",
      "2025-06-06 14:04:13 - INFO - load - Load time: 0.207 seconds\n",
      "2025-06-06 14:04:13 - INFO - memory_usage - init, memory usage: 1775.4140625 MB\n",
      "2025-06-06 14:04:13 - INFO - load - Load time: 0.211 seconds\n",
      "2025-06-06 14:04:13 - INFO - memory_usage - init, memory usage: 1775.4140625 MB\n",
      "2025-06-06 14:04:13 - INFO - load - Load time: 0.316 seconds\n",
      "2025-06-06 14:04:13 - INFO - memory_usage - init, memory usage: 1773.4140625 MB\n",
      "2025-06-06 14:04:14 - INFO - load - Load time: 0.209 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "reuters = Corpus().load(path_to_reuters_corpus)\n",
    "brown = Corpus().load(path_to_brown_corpus)\n",
    "gardenparty = Corpus().load(path_to_gardenparty_corpus)\n",
    "gutenberg = Corpus().load(path_to_gutenberg_corpus)\n",
    "congress = Corpus().load(path_to_congress_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Collocates:\n",
    "\t\"\"\" Class for collocation analysis reporting. \"\"\"\n",
    "\tdef __init__(self,\n",
    "\t\t\t  corpus:Corpus # Corpus instance\n",
    "\t\t\t  ): \n",
    "\t\tself.corpus = corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _shift_zeroes_to_end(self:Collocates,\n",
    "\t\t\t\t\t\tarr:np.ndarray # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Move 0 value positions for punctuation and space removal \"\"\"\n",
    "\tresult = np.empty_like(arr)\n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tmask = col_data != 0\n",
    "\t\tresult[:mask.sum(), col] = col_data[mask]\n",
    "\t\tresult[mask.sum():, col] = 0\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _zero_after_value(self:Collocates,\n",
    "\t\t\t\t\t  arr:np.ndarray, # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t  target: int # Target value to find in the array (e.g., an end-of-file token or a specific collocate frequency)\n",
    "\t\t\t\t\t  ):\n",
    "\t\"\"\" Set values from first occurence of target value to 0 in each column (for processing tokens outside text using eof token) \"\"\"\n",
    "\tarr = arr.copy()  \n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tidx = np.where(col_data == target)[0]\n",
    "\t\tif idx.size > 0:\n",
    "\t\t\tfirst_idx = idx[0]\n",
    "\t\t\tarr[first_idx:, col] = 0\n",
    "\treturn arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning journal = notebook pick up collocation dominoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _get_collocates_in_context(self:Collocates,\n",
    "\t\t\t\t\t\t\t   token_positions:np.ndarray, # Numpy array of token positions in the corpus\n",
    "\t\t\t\t\t\t\t   index:str, # Index to use - lower_index, orth_index\n",
    "\t\t\t\t\t\t\t   context_length:int = 5, # Number of context words to consider on each side of the token\n",
    "\t\t\t\t\t\t\t   position_offset:int = 1 # offset to start retrieving context words - -1 for left, positive for right (may be adjusted by sequence_len)\n",
    "\t\t\t\t\t\t\t   ) -> Result:\n",
    "\t\"\"\" Get collocates in context for a given token index, operates one side at a time. \"\"\"\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tif context_length < 1:\n",
    "\t\t# return empty result\n",
    "\t\treturn np.zeros((0, 0), dtype=np.int32)\n",
    "\n",
    "\tif position_offset < 0:\n",
    "\t\tposition_offset_step = -1\n",
    "\telse:\n",
    "\t\tposition_offset_step = 1\n",
    "\t\n",
    "\tcollected = False\n",
    "\tcontext_tokens_arr = []\n",
    "\twhile collected == False:\n",
    "\t\tnew_positions = np.array(token_positions[0] + position_offset, dtype = token_positions[0].dtype)\n",
    "\t\tcontext_tokens_arr.append(self.corpus.get_tokens_by_index(index)[new_positions])\n",
    "\t\tposition_offset += position_offset_step\n",
    "\t\tif len(context_tokens_arr) >= context_length: # cleaning spaces and punctuation and check if need more iterations\n",
    "\t\t\tcontext_tokens = np.array(context_tokens_arr, dtype = token_positions[0].dtype)\n",
    "\t\t\tcontext_tokens = np.where(np.isin(context_tokens, self.corpus.punct_tokens + self.corpus.space_tokens), 0, context_tokens)\n",
    "\t\t\tcounts = np.count_nonzero(context_tokens, axis=0)\n",
    "\t\t\tif np.min(counts) < context_length:\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\tcollected = True\n",
    "\n",
    "\tcontext_tokens = self._shift_zeroes_to_end(context_tokens)\n",
    "\tcontext_tokens = context_tokens[:context_length, :]\n",
    "\n",
    "\tif self.corpus.EOF_TOKEN in context_tokens:\n",
    "\t\tcontext_tokens = self._zero_after_value(context_tokens, self.corpus.EOF_TOKEN)\n",
    "\n",
    "\tlogger.info(f\"Collocates retrieved in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\treturn context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def collocates(self:Collocates, \n",
    "\t\t\t\ttoken_str:str, # Token to search for\n",
    "\t\t\t\tcollocation_measure:str = 'logdice', # statistical measure to use for collocation calculation: logdice, mutual_information\n",
    "\t\t\t\tcontext_length:int|None=5, # Window size per side in tokens - use this for setting context lengths on left and right to same value\n",
    "\t\t\t\tcontext_left:int|None=None, # If context_left or context_right > 0 sets context lengths independently\n",
    "\t\t\t\tcontext_right:int|None=None, # see context_left\n",
    "\t\t\t\tmin_collocate_frequency:int=5, # Minimum count of collocates\n",
    "\t\t\t\tpage_size:int=PAGE_SIZE, # number of rows to return, if 0 returns all\n",
    "\t\t\t\tpage_current:int=1, # current page, ignored if page_size is 0\n",
    "\t\t\t\texclude_punctuation:bool=True, # exclude punctuation tokens\n",
    "\t\t\t\texclude_spaces:bool=True # exclude space tokens\n",
    "\t\t\t\t) -> Result:\n",
    "\t\"\"\" Report collocates for a given token string. \"\"\"\n",
    "\n",
    "\ttoken_sequence, index_id = self.corpus.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "\tindex_column = 'lower_index'\n",
    "\tfrequency_column = 'frequency_lower'\n",
    "\tcolumns = ['rank', 'token', 'collocate_frequency', 'frequency']\n",
    "\n",
    "\tif collocation_measure not in ['logdice', 'mutual_information']:\n",
    "\t\traise ValueError(f'Collocation measure must be one of \"logdice\" or \"mutual_information\"')\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tdebug = False\n",
    "\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\ttoken_positions = self.corpus.get_token_positions(token_sequence, index_id)\n",
    "\n",
    "\tcount_tokens = self.corpus.token_count\n",
    "\ttokens_descriptor = 'all tokens'\n",
    "\ttotal_descriptor = 'Total tokens'\n",
    "\tif exclude_punctuation and exclude_spaces:\n",
    "\t\tcount_tokens = self.corpus.word_token_count\n",
    "\t\ttokens_descriptor = 'word tokens'\n",
    "\t\ttotal_descriptor = 'Total word tokens'\n",
    "\telif exclude_punctuation:\n",
    "\t\tspace_tokens_count = self.corpus.spaces.select(pl.len()).collect(engine='streaming').item()\n",
    "\t\tcount_tokens = self.corpus.word_token_count + space_tokens_count\n",
    "\t\ttokens_descriptor = 'word and space tokens'\n",
    "\t\ttotal_descriptor = 'Total word and space tokens'\n",
    "\telif exclude_spaces:\n",
    "\t\tpunct_tokens_count = self.corpus.puncts.select(pl.len()).collect(engine='streaming').item()\n",
    "\t\tcount_tokens = self.corpus.word_token_count + punct_tokens_count\n",
    "\t\ttokens_descriptor = 'word and punctuation tokens'\n",
    "\t\ttotal_descriptor = 'Total word and punctuation tokens'\n",
    "\n",
    "\tformatted_data = []\n",
    "\tformatted_data.append(f'Report based on {tokens_descriptor}')\n",
    "\n",
    "\t# if any of context_length, context_left, context_right are None - set them to 0\n",
    "\tif context_length is None:\n",
    "\t\tcontext_length = 0\n",
    "\tif context_left is None:\n",
    "\t\tcontext_left = 0\n",
    "\tif context_right is None:\n",
    "\t\tcontext_right = 0\n",
    "\n",
    "\tif context_left == 0 and context_right == 0:\n",
    "\t\tcontext_left = context_length\n",
    "\t\tcontext_right = context_length\n",
    "\telif (context_left > 0 or context_right > 0) and context_length > 0:\n",
    "\t\tlogger.warning('Context length is ignored if either context_left or context_right is set to a value greater than 0. To remove this warning, set context_length to None or 0.')\n",
    "\n",
    "\tformatted_data.append(f'Context tokens left: {context_left}, context tokens right: {context_right}')\n",
    "\n",
    "\t# getting context tokens\n",
    "\tleft_tokens = self._get_collocates_in_context(token_positions=token_positions, index=index_column, context_length=context_left, position_offset=-1)\n",
    "\tright_tokens = self._get_collocates_in_context(token_positions=token_positions, index=index_column, context_length=context_right, position_offset=sequence_len)\n",
    "\tcombined_tokens = np.concatenate([left_tokens.flatten(), right_tokens.flatten()])\n",
    "\tcombined_tokens = combined_tokens[combined_tokens != 0] # removes punctuation and space placeholder\n",
    "\ttoken_count_in_context_window = combined_tokens.shape[0]\n",
    "\n",
    "\t# getting frequencies of collocates\n",
    "\tunique_token_ids, counts = np.unique(combined_tokens, return_counts=True)\n",
    "\t#unique_token_ids = unique_token_ids.astype(np.int32)\n",
    "\n",
    "\tdf = pl.DataFrame({\n",
    "\t\t'token_id': unique_token_ids,\n",
    "\t\t'collocate_frequency': counts\n",
    "\t})\n",
    "\n",
    "\t# adding frequency of collocates in corpus\n",
    "\tdf = df.join(self.corpus.vocab.collect().select(['token_id', 'token', frequency_column]), on='token_id', how='left', maintain_order='left')\n",
    "\tdf = df.rename({frequency_column: 'frequency'})\n",
    "\n",
    "\tfiltering_descriptors = []\n",
    "\tif min_collocate_frequency > 1: # applying min_frequency filter\n",
    "\t\tdf = df.filter(pl.col('collocate_frequency') >= min_collocate_frequency)\n",
    "\t\tfiltering_descriptors.append(f'minimum collocation frequency ({min_collocate_frequency})')\n",
    "\tif len(filtering_descriptors) > 0:\n",
    "\t\tformatted_data.append(f'Filtered tokens by {(\", \".join(filtering_descriptors))}')\n",
    "\n",
    "\tif collocation_measure == 'logdice':\n",
    "\t\t# calculating collocation measure\n",
    "\t\t# from old code: logdice = 14 + math.log2((2 * collocate_count) / (node_frequency + loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "\t\tdf = df.with_columns(\n",
    "\t\t\t(pl.lit(14) + ((2 * pl.col('collocate_frequency')) / (pl.lit(token_positions[0].shape[0]) + pl.col('frequency'))).log(2))\n",
    "\t\t\t.alias('logdice')\n",
    "\t\t)\n",
    "\t\tcolumns.append('logdice')\n",
    "\n",
    "\tif collocation_measure == 'mutual_information':\n",
    "\t\t# from old code: mi = math.log2((loaded_corpora[corpus_name]['token_count'] * collocate_count) / (node_frequency * loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "\t\tdf = df.with_columns(\n",
    "\t\t\t(pl.lit(count_tokens) * pl.col('collocate_frequency') / (pl.lit(token_positions[0].shape[0]) * pl.col('frequency'))).log(2)\n",
    "\t\t\t.alias('mutual_information')\n",
    "\t\t)\n",
    "\t\tcolumns.append('mutual_information')\n",
    "\n",
    "\n",
    "\t# based on calculation for keyness: https://ucrel.lancs.ac.uk/llwizard.html\n",
    "\t# a = collocate frequency in context, is collocate_frequency\n",
    "\t# b = collocate frequency outside context ...\n",
    "\t# TODO - add when syntax to handle case for individual tokens constituting the node, adjust collocate_frequency_outside_context by token_positions[0].shape[0]\n",
    "\tdf = df.with_columns(\n",
    "\t\t(pl.col('frequency') - pl.col('collocate_frequency')).alias('collocate_frequency_outside_context')\n",
    "\t)\n",
    "\n",
    "\t# c = total tokens in context windows, is token_count_in_context_window calculated above\n",
    "\t# d = total tokens outside context windows\n",
    "\ttotal_tokens_outside_context_window = count_tokens - token_count_in_context_window - (token_positions[0].shape[0] * sequence_len)\n",
    "\n",
    "\t# E1 = c*(a+b) / (c+d) \n",
    "\t# E2 = d*(a+b) / (c+d)\n",
    "\t# E1 and E2\n",
    "\tdf = df.with_columns(\n",
    "\t\t((pl.lit(token_count_in_context_window) * (pl.col('collocate_frequency') + pl.col('collocate_frequency_outside_context'))) / (pl.lit(token_count_in_context_window) + pl.lit(total_tokens_outside_context_window))).alias('expected_frequency1'),\n",
    "\t\t((pl.lit(total_tokens_outside_context_window) * (pl.col('collocate_frequency') + pl.col('collocate_frequency_outside_context'))) / (pl.lit(token_count_in_context_window) + pl.lit(total_tokens_outside_context_window))).alias('expected_frequency2'), \n",
    "\t)\n",
    "\n",
    "\t# G2 = 2*((a*ln (a/E1)) + (b*ln (b/E2))) \n",
    "\t# components of G2 as term1 and term 2 - (a*ln (a/E1)) (b*ln (b/E2))\n",
    "\tdf = df.with_columns([\n",
    "\t\tpl.when(pl.col('collocate_frequency') > 0)\n",
    "\t\t.then(pl.col('collocate_frequency') * (pl.col('collocate_frequency') / pl.col('expected_frequency1')).log())\n",
    "\t\t.otherwise(0)\n",
    "\t\t.alias('term1'),\n",
    "\t\tpl.when(pl.col('collocate_frequency_outside_context') > 0)\n",
    "\t\t.then(pl.col('collocate_frequency_outside_context') * (pl.col('collocate_frequency_outside_context') / pl.col('expected_frequency2')).log())\n",
    "\t\t.otherwise(0)\n",
    "\t\t.alias('term2') # 0 if no reference frequency\n",
    "\t])\n",
    "\n",
    "\t# G2\n",
    "\tdf = df.with_columns(\n",
    "\t\t(2 * (pl.col('term1') + pl.col('term2'))).alias('log_likelihood')\n",
    "\t)\n",
    "\tcolumns.append('log_likelihood')\n",
    "\n",
    "\t# prepare report information and paging ...\n",
    "\tdf = df.sort(collocation_measure, descending = True).with_row_index('rank', offset=1)\n",
    "\tunique_collocates = df.select(pl.len()).item()\n",
    "\tformatted_data.append(f'Unique collocates: {unique_collocates:,.0f}')\n",
    "\n",
    "\tif page_size > 0:\n",
    "\t\tstart = (page_current - 1) * page_size\n",
    "\t\tdf = df.slice(start, page_size)\n",
    "\n",
    "\tif page_size != 0 and unique_collocates > page_size:\n",
    "\t\tformatted_data.append(f'Showing {page_size} rows')\n",
    "\t\tformatted_data.append(f'Page {page_current} of {unique_collocates // page_size + 1}')\n",
    "\n",
    "\t#formatted_data.append(f'{total_descriptor}: {count_tokens:,.0f}')\n",
    "\n",
    "\tlogger.info(f\"Collocates calculated in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\treturn Result(type = 'collocates', df = df.select(columns), title=f'Collocates of \"{token_str}\"', description=f'{self.corpus.name}', summary_data={}, formatted_data=formatted_data)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocates = Collocates(reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 15:29:19 - INFO - tokenize - Tokenization time: 0.00011 seconds\n",
      "2025-06-06 15:29:19 - INFO - get_token_positions - Token indexing (31) time: 0.00165 seconds\n",
      "2025-06-06 15:29:19 - INFO - _get_collocates_in_context - Collocates retrieved in 0.00 seconds.\n",
      "2025-06-06 15:29:19 - INFO - _get_collocates_in_context - Collocates retrieved in 0.00 seconds.\n",
      "2025-06-06 15:29:19 - INFO - collocates - Collocates calculated in 0.02 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.int64(33020),)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"iphwjnilhx\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#iphwjnilhx table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#iphwjnilhx thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#iphwjnilhx p { margin: 0; padding: 0; }\n",
       " #iphwjnilhx .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #iphwjnilhx .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #iphwjnilhx .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #iphwjnilhx .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #iphwjnilhx .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #iphwjnilhx .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #iphwjnilhx .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #iphwjnilhx .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #iphwjnilhx .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #iphwjnilhx .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #iphwjnilhx .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #iphwjnilhx .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #iphwjnilhx .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #iphwjnilhx .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #iphwjnilhx .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #iphwjnilhx .gt_from_md> :first-child { margin-top: 0; }\n",
       " #iphwjnilhx .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #iphwjnilhx .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #iphwjnilhx .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #iphwjnilhx .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #iphwjnilhx .gt_row_group_first td { border-top-width: 2px; }\n",
       " #iphwjnilhx .gt_row_group_first th { border-top-width: 2px; }\n",
       " #iphwjnilhx .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #iphwjnilhx .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #iphwjnilhx .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #iphwjnilhx .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #iphwjnilhx .gt_left { text-align: left; }\n",
       " #iphwjnilhx .gt_center { text-align: center; }\n",
       " #iphwjnilhx .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #iphwjnilhx .gt_font_normal { font-weight: normal; }\n",
       " #iphwjnilhx .gt_font_bold { font-weight: bold; }\n",
       " #iphwjnilhx .gt_font_italic { font-style: italic; }\n",
       " #iphwjnilhx .gt_super { font-size: 65%; }\n",
       " #iphwjnilhx .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #iphwjnilhx .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"6\" class=\"gt_heading gt_title gt_font_normal\">Collocates of &quot;fish&quot;</td>\n",
       "  </tr>\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"6\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\">Reuters Corpus</td>\n",
       "  </tr>\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Rank\">Rank</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Token\">Token</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Collocate Frequency\">Collocate Frequency</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Frequency\">Frequency</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Logdice\">Logdice</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Log Likelihood\">Log Likelihood</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">1</td>\n",
       "    <td class=\"gt_row gt_left\">meal</td>\n",
       "    <td class=\"gt_row gt_right\">7</td>\n",
       "    <td class=\"gt_row gt_right\">68</td>\n",
       "    <td class=\"gt_row gt_right\">11.18</td>\n",
       "    <td class=\"gt_row gt_right\">73.11</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">2</td>\n",
       "    <td class=\"gt_row gt_left\">production</td>\n",
       "    <td class=\"gt_row gt_right\">5</td>\n",
       "    <td class=\"gt_row gt_right\">1,468</td>\n",
       "    <td class=\"gt_row gt_right\">6.77</td>\n",
       "    <td class=\"gt_row gt_right\">18.23</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">3</td>\n",
       "    <td class=\"gt_row gt_left\">and</td>\n",
       "    <td class=\"gt_row gt_right\">17</td>\n",
       "    <td class=\"gt_row gt_right\">25,645</td>\n",
       "    <td class=\"gt_row gt_right\">4.44</td>\n",
       "    <td class=\"gt_row gt_right\">15.22</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">4</td>\n",
       "    <td class=\"gt_row gt_left\">said</td>\n",
       "    <td class=\"gt_row gt_right\">14</td>\n",
       "    <td class=\"gt_row gt_right\">25,379</td>\n",
       "    <td class=\"gt_row gt_right\">4.17</td>\n",
       "    <td class=\"gt_row gt_right\">9.23</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">5</td>\n",
       "    <td class=\"gt_row gt_left\">the</td>\n",
       "    <td class=\"gt_row gt_right\">19</td>\n",
       "    <td class=\"gt_row gt_right\">69,263</td>\n",
       "    <td class=\"gt_row gt_right\">3.17</td>\n",
       "    <td class=\"gt_row gt_right\">1.01</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">6</td>\n",
       "    <td class=\"gt_row gt_left\">of</td>\n",
       "    <td class=\"gt_row gt_right\">10</td>\n",
       "    <td class=\"gt_row gt_right\">36,779</td>\n",
       "    <td class=\"gt_row gt_right\">3.15</td>\n",
       "    <td class=\"gt_row gt_right\">0.49</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">7</td>\n",
       "    <td class=\"gt_row gt_left\">to</td>\n",
       "    <td class=\"gt_row gt_right\">9</td>\n",
       "    <td class=\"gt_row gt_right\">36,328</td>\n",
       "    <td class=\"gt_row gt_right\">3.02</td>\n",
       "    <td class=\"gt_row gt_right\">0.16</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">8</td>\n",
       "    <td class=\"gt_row gt_left\">in</td>\n",
       "    <td class=\"gt_row gt_right\">7</td>\n",
       "    <td class=\"gt_row gt_right\">29,252</td>\n",
       "    <td class=\"gt_row gt_right\">2.97</td>\n",
       "    <td class=\"gt_row gt_right\">0.07</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "  <tfoot class=\"gt_sourcenotes\">\n",
       "  \n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Report based on word tokens</td>\n",
       "  </tr>\n",
       "\n",
       "\n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Context tokens left: 5, context tokens right: 5</td>\n",
       "  </tr>\n",
       "\n",
       "\n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Filtered tokens by minimum collocation frequency (5)</td>\n",
       "  </tr>\n",
       "\n",
       "\n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Unique collocates: 8</td>\n",
       "  </tr>\n",
       "\n",
       "</tfoot>\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.1 ms, sys: 3.95 ms, total: 60.1 ms\n",
      "Wall time: 25.5 ms\n"
     ]
    }
   ],
   "source": [
    "for word in [\"fish\"]: # brown used 'i went in', 'any of us',  for testing \"economy\"\n",
    "    %time collocates.collocates(word, collocation_measure='logdice', context_length = 5, min_collocate_frequency = 5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "## Collocation methods\n",
    "# \n",
    "# # @patch\n",
    "# def collocates(self: Corpus, word, nice_word, constrain_to = False, context_length = 5, limit = 50, cutoff=5, stat = 'ld', output = False):\n",
    "#     elements = {}\n",
    "#     nodes = []\n",
    "#     edges = []\n",
    "\n",
    "#     coll_token_sequence, coll_index_id = tokenize_string(corpus_name, word)\n",
    "#     token_id = coll_token_sequence[0][0]\n",
    "    \n",
    "#     #print(token_id, nice_word)\n",
    "    \n",
    "#     if constrain_to == False:\n",
    "#         nodes.append((token_id, {'label': nice_word, 'size': 1}))\n",
    "    \n",
    "#     #print(coll_token_sequence, coll_index_id)\n",
    "#     coll_token_index = profile_get_token_index(corpus_name, coll_token_sequence, coll_index_id) # get_token_positions\n",
    "#     #print(coll_token_index)\n",
    "#     positional_columns, concordance = profile_get_concordance(corpus_name, coll_token_sequence, coll_token_index, context_words = context_length, index_id = LOWER)\n",
    "#     #print(concordance)\n",
    "#     collocates = []\n",
    "#     for row in concordance:\n",
    "#         if eof_token in row:\n",
    "#             indexes = np.where(np.array(row) == eof_token)[0]\n",
    "#             #print(indexes)\n",
    "#             slice_min = -1\n",
    "#             slice_max = context_length * 2 + 1\n",
    "#             for i in indexes:\n",
    "#                 if i < context_length and i > slice_min:\n",
    "#                     slice_min = i\n",
    "#                 elif i > context_length and i < slice_max:\n",
    "#                     #print('***')\n",
    "#                     slice_max = i\n",
    "#             slice_min += 1\n",
    "#             #slice_max -= 1\n",
    "#             #print(slice_min,slice_max)\n",
    "#             #print(row)\n",
    "#             #print(row[slice_min:slice_max])\n",
    "#             collocates.append(row[slice_min:slice_max])\n",
    "#         else:\n",
    "#             collocates.append(row)\n",
    "\n",
    "#     node_frequency = len(collocates)\n",
    "            \n",
    "#     if len(collocates) < 1:\n",
    "#         print('no collocates')\n",
    "#     else:\n",
    "#         #print(collocates)\n",
    "#         collocates = np.concatenate(collocates)\n",
    "#         collocates = np.unique(collocates, axis=0, return_counts=True)\n",
    "#         #collocates = collocates[collocates[:,1].argsort()]\n",
    "        \n",
    "#         #print(collocates)\n",
    "#         logdices = []\n",
    "\n",
    "#         for row in range(len(collocates[0])):\n",
    "#             collocate = collocates[0][row]\n",
    "#             collocate_count = collocates[1][row]\n",
    "#             if constrain_to == False:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 if loaded_corpora[corpus_name]['vocab'][collocate] in constrain_to:\n",
    "#                     pass\n",
    "#                     #print(corpus['vocab'][collocate],' in constrain_to', constrain_to)\n",
    "#                 else:\n",
    "#                     continue\n",
    "#             if collocate in coll_token_sequence:\n",
    "#                 #print('match china')\n",
    "#                 pass\n",
    "#             elif collocate_count > 1:\n",
    "#                 #14 + log2D=14+log2(2fxy/(fx+fy))\n",
    "\n",
    "#                 if re.search(_RE_PUNCT,loaded_corpora[corpus_name]['vocab'][collocate]) is None: # FIX - HAVE OPION TO REMOVE PUNC\n",
    "#                     if collocate_count >= cutoff:\n",
    "#                         logdice = 14 + math.log2((2 * collocate_count) / (node_frequency + loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "#                         mi = math.log2((loaded_corpora[corpus_name]['token_count'] * collocate_count) / (node_frequency * loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "#                         logdices.append([logdice, mi, collocate, loaded_corpora[corpus_name]['vocab'][collocate], collocate_count, loaded_corpora[corpus_name]['frequency_lookup'][collocate]])\n",
    "#                         #if logdice > 8:\n",
    "#                         #print(collocate, node_frequency, loaded_corpora[corpus_name]['vocab'][collocate], collocates[1][row],logdice, mi)\n",
    "#                         matches = np.where(collocates == collocate)[0]\n",
    "#                         collocate_count = len(matches)\n",
    "#         #        if (collocate_count > 5):\n",
    "    \n",
    "#     top_collocates = []\n",
    "#     if stat == 'mi':\n",
    "#         sorted_collocates = sorted(logdices, reverse=True, key=lambda x: x[1])[0:limit]\n",
    "#         for row in sorted_collocates:\n",
    "#             nodes.append((row[2], {'label': row[3], 'size': 1}))\n",
    "#             edges.append((token_id, row[2], {'weight': 1}))\n",
    "#             #print(row)\n",
    "#             top_collocates.append(row[3])\n",
    "#     else:\n",
    "#         sorted_collocates = sorted(logdices, reverse=True, key=lambda x: x[0])[0:limit]\n",
    "#         for row in sorted_collocates:\n",
    "#             nodes.append((row[2], {'label': row[3], 'size': 1}))\n",
    "#             edges.append((token_id, row[2], {'weight': row[0]}))\n",
    "#             top_collocates.append(row[3])\n",
    "    \n",
    "#     if constrain_to == False:\n",
    "#         for top_collocate in top_collocates:\n",
    "#             top_collocate_elements, top_collocate_sorted, df = profile_prepare_collocates(corpus_name, top_collocate, top_collocate, constrain_to = top_collocates, context_length=context_length, limit = limit, stat=stat)\n",
    "#             for edge in top_collocate_elements['edges']:\n",
    "#                 edges.append(edge) \n",
    "    \n",
    "    \n",
    "#     #display(sorted(logdices, reverse=True, key=lambda x: x[0])[0:limit])\n",
    "    \n",
    "#     elements['nodes'] = nodes\n",
    "#     elements['edges'] = edges\n",
    "#     df = pd.DataFrame(sorted_collocates, columns = ['logdice', 'mi', 'collocate_token_id', 'collocate', 'collocate_count', 'collocate_token_frequency'])\n",
    "#     # if output != False:\n",
    "#     #     if output == 'file':\n",
    "#     #         with open(output_dir + corpus_name + '/collocates-' + stat + '-' + nice_word + '.html', 'w', encoding='utf8') as f:\n",
    "#     #             f.write(df.to_html(classes='table table-stripped'))\n",
    "#     return elements, sorted_collocates, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
