{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocates\n",
    "\n",
    "> Functionality for collocation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp collocates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import time\n",
    "import polars as pl\n",
    "from fastcore.basics import patch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc.corpus import Corpus\n",
    "from conc.result import Result\n",
    "from conc.core import logger, PAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Collocates:\n",
    "\t\"\"\" Class for collocation analysis reporting. \"\"\"\n",
    "\tdef __init__(self,\n",
    "\t\t\t  corpus:Corpus # Corpus instance\n",
    "\t\t\t  ): \n",
    "\t\tself.corpus = corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "## Collocation methods\n",
    "# \n",
    "# # @patch\n",
    "# def collocates(self: Corpus, word, nice_word, constrain_to = False, context_length = 5, limit = 50, cutoff=5, stat = 'ld', output = False):\n",
    "#     elements = {}\n",
    "#     nodes = []\n",
    "#     edges = []\n",
    "\n",
    "#     coll_token_sequence, coll_index_id = tokenize_string(corpus_name, word)\n",
    "#     token_id = coll_token_sequence[0][0]\n",
    "    \n",
    "#     #print(token_id, nice_word)\n",
    "    \n",
    "#     if constrain_to == False:\n",
    "#         nodes.append((token_id, {'label': nice_word, 'size': 1}))\n",
    "    \n",
    "#     #print(coll_token_sequence, coll_index_id)\n",
    "#     coll_token_index = profile_get_token_index(corpus_name, coll_token_sequence, coll_index_id) # get_token_positions\n",
    "#     #print(coll_token_index)\n",
    "#     positional_columns, concordance = profile_get_concordance(corpus_name, coll_token_sequence, coll_token_index, context_words = context_length, index_id = LOWER)\n",
    "#     #print(concordance)\n",
    "#     collocates = []\n",
    "#     for row in concordance:\n",
    "#         if eof_token in row:\n",
    "#             indexes = np.where(np.array(row) == eof_token)[0]\n",
    "#             #print(indexes)\n",
    "#             slice_min = -1\n",
    "#             slice_max = context_length * 2 + 1\n",
    "#             for i in indexes:\n",
    "#                 if i < context_length and i > slice_min:\n",
    "#                     slice_min = i\n",
    "#                 elif i > context_length and i < slice_max:\n",
    "#                     #print('***')\n",
    "#                     slice_max = i\n",
    "#             slice_min += 1\n",
    "#             #slice_max -= 1\n",
    "#             #print(slice_min,slice_max)\n",
    "#             #print(row)\n",
    "#             #print(row[slice_min:slice_max])\n",
    "#             collocates.append(row[slice_min:slice_max])\n",
    "#         else:\n",
    "#             collocates.append(row)\n",
    "\n",
    "#     node_frequency = len(collocates)\n",
    "            \n",
    "#     if len(collocates) < 1:\n",
    "#         print('no collocates')\n",
    "#     else:\n",
    "#         #print(collocates)\n",
    "#         collocates = np.concatenate(collocates)\n",
    "#         collocates = np.unique(collocates, axis=0, return_counts=True)\n",
    "#         #collocates = collocates[collocates[:,1].argsort()]\n",
    "        \n",
    "#         #print(collocates)\n",
    "#         logdices = []\n",
    "\n",
    "#         for row in range(len(collocates[0])):\n",
    "#             collocate = collocates[0][row]\n",
    "#             collocate_count = collocates[1][row]\n",
    "#             if constrain_to == False:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 if loaded_corpora[corpus_name]['vocab'][collocate] in constrain_to:\n",
    "#                     pass\n",
    "#                     #print(corpus['vocab'][collocate],' in constrain_to', constrain_to)\n",
    "#                 else:\n",
    "#                     continue\n",
    "#             if collocate in coll_token_sequence:\n",
    "#                 #print('match china')\n",
    "#                 pass\n",
    "#             elif collocate_count > 1:\n",
    "#                 #14 + log2D=14+log2(2fxy/(fx+fy))\n",
    "\n",
    "#                 if re.search(_RE_PUNCT,loaded_corpora[corpus_name]['vocab'][collocate]) is None: # FIX - HAVE OPION TO REMOVE PUNC\n",
    "#                     if collocate_count >= cutoff:\n",
    "#                         logdice = 14 + math.log2((2 * collocate_count) / (node_frequency + loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "#                         mi = math.log2((loaded_corpora[corpus_name]['token_count'] * collocate_count) / (node_frequency * loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "#                         logdices.append([logdice, mi, collocate, loaded_corpora[corpus_name]['vocab'][collocate], collocate_count, loaded_corpora[corpus_name]['frequency_lookup'][collocate]])\n",
    "#                         #if logdice > 8:\n",
    "#                         #print(collocate, node_frequency, loaded_corpora[corpus_name]['vocab'][collocate], collocates[1][row],logdice, mi)\n",
    "#                         matches = np.where(collocates == collocate)[0]\n",
    "#                         collocate_count = len(matches)\n",
    "#         #        if (collocate_count > 5):\n",
    "    \n",
    "#     top_collocates = []\n",
    "#     if stat == 'mi':\n",
    "#         sorted_collocates = sorted(logdices, reverse=True, key=lambda x: x[1])[0:limit]\n",
    "#         for row in sorted_collocates:\n",
    "#             nodes.append((row[2], {'label': row[3], 'size': 1}))\n",
    "#             edges.append((token_id, row[2], {'weight': 1}))\n",
    "#             #print(row)\n",
    "#             top_collocates.append(row[3])\n",
    "#     else:\n",
    "#         sorted_collocates = sorted(logdices, reverse=True, key=lambda x: x[0])[0:limit]\n",
    "#         for row in sorted_collocates:\n",
    "#             nodes.append((row[2], {'label': row[3], 'size': 1}))\n",
    "#             edges.append((token_id, row[2], {'weight': row[0]}))\n",
    "#             top_collocates.append(row[3])\n",
    "    \n",
    "#     if constrain_to == False:\n",
    "#         for top_collocate in top_collocates:\n",
    "#             top_collocate_elements, top_collocate_sorted, df = profile_prepare_collocates(corpus_name, top_collocate, top_collocate, constrain_to = top_collocates, context_length=context_length, limit = limit, stat=stat)\n",
    "#             for edge in top_collocate_elements['edges']:\n",
    "#                 edges.append(edge) \n",
    "    \n",
    "    \n",
    "#     #display(sorted(logdices, reverse=True, key=lambda x: x[0])[0:limit])\n",
    "    \n",
    "#     elements['nodes'] = nodes\n",
    "#     elements['edges'] = edges\n",
    "#     df = pd.DataFrame(sorted_collocates, columns = ['logdice', 'mi', 'collocate_token_id', 'collocate', 'collocate_count', 'collocate_token_frequency'])\n",
    "#     # if output != False:\n",
    "#     #     if output == 'file':\n",
    "#     #         with open(output_dir + corpus_name + '/collocates-' + stat + '-' + nice_word + '.html', 'w', encoding='utf8') as f:\n",
    "#     #             f.write(df.to_html(classes='table table-stripped'))\n",
    "#     return elements, sorted_collocates, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
