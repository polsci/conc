{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocates\n",
    "\n",
    "> Functionality for collocation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp collocates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import time\n",
    "import polars as pl\n",
    "from fastcore.basics import patch\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc.corpus import Corpus\n",
    "from conc.result import Result\n",
    "from conc.core import logger, PAGE_SIZE, set_logger_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "np.set_printoptions(suppress=True)\n",
    "set_logger_state('verbose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = f'{os.environ.get(\"HOME\")}/data/'\n",
    "save_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'\n",
    "\n",
    "path_to_toy_corpus = f'{save_path}toy.corpus'\n",
    "path_to_brown_corpus = f'{save_path}brown.corpus'\n",
    "path_to_reuters_corpus = f'{save_path}reuters.corpus'\n",
    "path_to_gutenberg_corpus = f'{save_path}gutenberg.corpus'\n",
    "path_to_gardenparty_corpus = f'{save_path}garden-party.corpus'\n",
    "path_to_congress_corpus = f'{save_path}us-congressional-speeches-subset-100k.corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 12:18:28 - INFO - memory_usage - init, memory usage: 973.8203125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 12:18:28 - INFO - load - Load time: 0.769 seconds\n",
      "2025-06-07 12:18:28 - INFO - memory_usage - init, memory usage: 987.546875 MB\n",
      "2025-06-07 12:18:29 - INFO - load - Load time: 0.844 seconds\n",
      "2025-06-07 12:18:29 - INFO - memory_usage - init, memory usage: 991.4765625 MB\n",
      "2025-06-07 12:18:30 - INFO - load - Load time: 0.653 seconds\n",
      "2025-06-07 12:18:30 - INFO - memory_usage - init, memory usage: 991.4765625 MB\n",
      "2025-06-07 12:18:31 - INFO - load - Load time: 0.622 seconds\n",
      "2025-06-07 12:18:31 - INFO - memory_usage - init, memory usage: 991.4765625 MB\n",
      "2025-06-07 12:18:31 - INFO - load - Load time: 0.632 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "reuters = Corpus().load(path_to_reuters_corpus)\n",
    "brown = Corpus().load(path_to_brown_corpus)\n",
    "gardenparty = Corpus().load(path_to_gardenparty_corpus)\n",
    "gutenberg = Corpus().load(path_to_gutenberg_corpus)\n",
    "congress = Corpus().load(path_to_congress_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Collocates:\n",
    "\t\"\"\" Class for collocation analysis reporting. \"\"\"\n",
    "\tdef __init__(self,\n",
    "\t\t\t  corpus:Corpus # Corpus instance\n",
    "\t\t\t  ): \n",
    "\t\tself.corpus = corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _shift_zeroes_to_end(self:Collocates,\n",
    "\t\t\t\t\t\tarr:np.ndarray # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Move 0 value positions for punctuation and space removal \"\"\"\n",
    "\tresult = np.empty_like(arr)\n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tmask = col_data != 0\n",
    "\t\tresult[:mask.sum(), col] = col_data[mask]\n",
    "\t\tresult[mask.sum():, col] = 0\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _zero_after_value(self:Collocates,\n",
    "\t\t\t\t\t  arr:np.ndarray, # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t  target: int # Target value to find in the array (e.g., an end-of-file token or a specific collocate frequency)\n",
    "\t\t\t\t\t  ):\n",
    "\t\"\"\" Set values from first occurence of target value to 0 in each column (for processing tokens outside text using eof token) \"\"\"\n",
    "\tarr = arr.copy()  \n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tidx = np.where(col_data == target)[0]\n",
    "\t\tif idx.size > 0:\n",
    "\t\t\tfirst_idx = idx[0]\n",
    "\t\t\tarr[first_idx:, col] = 0\n",
    "\treturn arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning journal = notebook pick up collocation dominoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _get_tokens_in_context(self:Collocates,\n",
    "\t\t\t\t\t\t\t   token_positions:np.ndarray, # Numpy array of token positions in the corpus\n",
    "\t\t\t\t\t\t\t   index:str, # Index to use - lower_index, orth_index\n",
    "\t\t\t\t\t\t\t   context_length:int = 5, # Number of context words to consider on each side of the token\n",
    "\t\t\t\t\t\t\t   position_offset:int = 1, # offset to start retrieving context words - -1 for left, positive for right (may be adjusted by sequence_len)\n",
    "\t\t\t\t\t\t\t   exclude_punctuation:bool = True, # exclude punctuation from collocate retrieval\n",
    "\t\t\t\t\t\t\t   exclude_spaces:bool = True\n",
    "\t\t\t\t\t\t\t   ) -> Result:\n",
    "\t\"\"\" Get collocates in context for a given token index, operates one side at a time. \"\"\"\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tif context_length < 1:\n",
    "\t\t# return empty result\n",
    "\t\treturn np.zeros((0, 0), dtype=np.int32)\n",
    "\n",
    "\tif position_offset < 0:\n",
    "\t\tposition_offset_step = -1\n",
    "\telse:\n",
    "\t\tposition_offset_step = 1\n",
    "\t\n",
    "\ttokens_for_removal = []\n",
    "\tif exclude_punctuation:\n",
    "\t\ttokens_for_removal += self.corpus.punct_tokens\n",
    "\tif exclude_spaces:\n",
    "\t\ttokens_for_removal += self.corpus.space_tokens\n",
    "\tlen_tokens_for_removal = len(tokens_for_removal)\n",
    "\n",
    "\tcollected = False\n",
    "\tcontext_tokens_arr = []\n",
    "\twhile collected == False:\n",
    "\t\tnew_positions = np.array(token_positions[0] + position_offset, dtype = token_positions[0].dtype)\n",
    "\t\tcontext_tokens_arr.append(self.corpus.get_tokens_by_index(index)[new_positions])\n",
    "\t\tposition_offset += position_offset_step\n",
    "\t\tif len(context_tokens_arr) >= context_length: \n",
    "\t\t\tcontext_tokens = np.array(context_tokens_arr, dtype = token_positions[0].dtype)\n",
    "\t\t\tif len_tokens_for_removal > 0: # cleaning spaces and punctuation and check if need more iterations\n",
    "\t\t\t\tcontext_tokens = np.where(np.isin(context_tokens, self.corpus.punct_tokens + self.corpus.space_tokens), 0, context_tokens)\n",
    "\t\t\tcounts = np.count_nonzero(context_tokens, axis=0)\n",
    "\t\t\tif np.min(counts) < context_length:\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\tcollected = True\n",
    "\n",
    "\tcontext_tokens = self._shift_zeroes_to_end(context_tokens)\n",
    "\tcontext_tokens = context_tokens[:context_length, :]\n",
    "\n",
    "\tif self.corpus.EOF_TOKEN in context_tokens:\n",
    "\t\tcontext_tokens = self._zero_after_value(context_tokens, self.corpus.EOF_TOKEN)\n",
    "\n",
    "\tlogger.info(f\"Collocates retrieved in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\treturn context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def collocates(self:Collocates, \n",
    "\t\t\t\ttoken_str:str, # Token to search for\n",
    "\t\t\t\teffect_size_measure:str = 'logdice', # statistical measure to use for collocation calculation: logdice, mutual_information\n",
    "\t\t\t\tstatistical_significance_measure:str = 'log_likelihood', # statistical significance measure to use, currently only 'log_likelihood' is supported\n",
    "\t\t\t\torder:str|None = None, # default of None orders by collocation measure, results can also be ordered by: collocate_frequency, frequency, log_likelihood\n",
    "\t\t\t\torder_descending:bool = True, # order is descending or ascending\n",
    "\t\t\t\tstatistical_significance_cut: float|None = None, # statistical significance p-value to filter results, e.g. 0.05 or 0.01 or 0.001 - ignored if None or 0\n",
    "\t\t\t\tapply_bonferroni:bool = False, # apply Bonferroni correction to the statistical significance cut-off\n",
    "\t\t\t\tcontext_length:int|None=5, # Window size per side in tokens - use this for setting context lengths on left and right to same value\n",
    "\t\t\t\tcontext_left:int|None=None, # If context_left or context_right > 0 sets context lengths independently\n",
    "\t\t\t\tcontext_right:int|None=None, # see context_left\n",
    "\t\t\t\tmin_collocate_frequency:int=5, # Minimum count of collocates\n",
    "\t\t\t\tpage_size:int=PAGE_SIZE, # number of rows to return, if 0 returns all\n",
    "\t\t\t\tpage_current:int=1, # current page, ignored if page_size is 0\n",
    "\t\t\t\texclude_punctuation:bool=True, # exclude punctuation tokens\n",
    "\t\t\t\texclude_spaces:bool=True # exclude space tokens\n",
    "\t\t\t\t) -> Result:\n",
    "\t\"\"\" Report collocates for a given token string. \"\"\"\n",
    "\n",
    "\tif effect_size_measure not in ['logdice', 'mutual_information']:\n",
    "\t\traise ValueError(f'Collocation measure must be one of \"logdice\" or \"mutual_information\".')\n",
    "\t\n",
    "\tif statistical_significance_measure not in ['log_likelihood']:\n",
    "\t\traise ValueError(f'Statistical significance measure must be \"log_likelihood\".')\n",
    "\n",
    "\tif order not in [None, effect_size_measure, 'collocate_frequency', 'frequency', statistical_significance_measure]:\n",
    "\t\traise ValueError(f'The order parameter must be None (default) or one of: {effect_size_measure}, collocate_frequency, frequency, {statistical_significance_measure}')\n",
    "\n",
    "\ttoken_sequence, index_id = self.corpus.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "\tindex_column = 'lower_index'\n",
    "\tfrequency_column = 'frequency_lower'\n",
    "\tcolumns = ['rank', 'token', 'collocate_frequency', 'frequency']\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tdebug = False\n",
    "\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\ttoken_positions = self.corpus.get_token_positions(token_sequence, index_id)\n",
    "\n",
    "\tif token_positions is None or token_positions[0].shape[0] == 0:\n",
    "\t\tlogger.warning(f'Token \"{token_str}\" not found in the corpus.')\n",
    "\t\treturn Result(type='collocates', df=pl.DataFrame(), title=f'No matches for \"{token_str}\"', description=f'{self.corpus.name}', summary_data={}, formatted_data=[])\n",
    "\n",
    "\tcount_tokens = self.corpus.token_count\n",
    "\ttokens_descriptor = 'all tokens'\n",
    "\ttotal_descriptor = 'Total tokens'\n",
    "\tif exclude_punctuation and exclude_spaces:\n",
    "\t\tcount_tokens = self.corpus.word_token_count\n",
    "\t\ttokens_descriptor = 'word tokens'\n",
    "\t\ttotal_descriptor = 'Total word tokens'\n",
    "\telif exclude_punctuation:\n",
    "\t\tspace_tokens_count = self.corpus.spaces.select(pl.len()).collect(engine='streaming').item()\n",
    "\t\tcount_tokens = self.corpus.word_token_count + space_tokens_count\n",
    "\t\ttokens_descriptor = 'word and space tokens'\n",
    "\t\ttotal_descriptor = 'Total word and space tokens'\n",
    "\telif exclude_spaces:\n",
    "\t\tpunct_tokens_count = self.corpus.puncts.select(pl.len()).collect(engine='streaming').item()\n",
    "\t\tcount_tokens = self.corpus.word_token_count + punct_tokens_count\n",
    "\t\ttokens_descriptor = 'word and punctuation tokens'\n",
    "\t\ttotal_descriptor = 'Total word and punctuation tokens'\n",
    "\n",
    "\tformatted_data = []\n",
    "\tformatted_data.append(f'Report based on {tokens_descriptor}')\n",
    "\n",
    "\t# if any of context_length, context_left, context_right are None - set them to 0\n",
    "\tif context_length is None:\n",
    "\t\tcontext_length = 0\n",
    "\tif context_left is None:\n",
    "\t\tcontext_left = 0\n",
    "\tif context_right is None:\n",
    "\t\tcontext_right = 0\n",
    "\n",
    "\tif context_left == 0 and context_right == 0:\n",
    "\t\tcontext_left = context_length\n",
    "\t\tcontext_right = context_length\n",
    "\telif (context_left > 0 or context_right > 0) and context_length > 0:\n",
    "\t\tlogger.warning('Context length is ignored if either context_left or context_right is set to a value greater than 0. To remove this warning, set context_length to None or 0.')\n",
    "\n",
    "\tformatted_data.append(f'Context tokens left: {context_left}, context tokens right: {context_right}')\n",
    "\n",
    "\t# getting context tokens\n",
    "\tleft_tokens = self._get_tokens_in_context(token_positions=token_positions, index=index_column, context_length=context_left, position_offset=-1, exclude_punctuation=exclude_punctuation, exclude_spaces=exclude_spaces)\n",
    "\tright_tokens = self._get_tokens_in_context(token_positions=token_positions, index=index_column, context_length=context_right, position_offset=sequence_len, exclude_punctuation=exclude_punctuation, exclude_spaces=exclude_spaces)\n",
    "\tcombined_tokens = np.concatenate([left_tokens.flatten(), right_tokens.flatten()])\n",
    "\tdel left_tokens, right_tokens\n",
    "\tcombined_tokens = combined_tokens[combined_tokens != 0] # removes punctuation and space placeholder\n",
    "\t# getting frequencies of collocates\n",
    "\tunique_token_ids, counts = np.unique(combined_tokens, return_counts=True)\n",
    "\ttoken_count_in_context_window = combined_tokens.shape[0]\n",
    "\n",
    "\tnode_tokens = self._get_tokens_in_context(token_positions=token_positions, index=index_column, context_length=sequence_len, position_offset=0, exclude_punctuation=exclude_punctuation, exclude_spaces=exclude_spaces)\n",
    "\tunique_node_token_ids, node_counts = np.unique(node_tokens, return_counts=True)\n",
    "\n",
    "\tdf = pl.DataFrame({\n",
    "\t\t'token_id': unique_token_ids,\n",
    "\t\t'collocate_frequency': counts\n",
    "\t})\n",
    "\n",
    "\t# for log liklihood calculation - need to have counts of node tokens to adjust collocate_frequency_outside_context\n",
    "\tnode_frequency = np.zeros_like(unique_token_ids, dtype=np.int32)\n",
    "\t# use where to set node_frequency for node tokens\n",
    "\tfor i, token_id in enumerate(unique_node_token_ids):\n",
    "\t\tnode_frequency[unique_token_ids == token_id] = node_counts[i]\n",
    "\tdf = df.with_columns(\n",
    "\t\tpl.lit(node_frequency).alias('node_frequency')\n",
    "\t)\n",
    "\n",
    "\t# adding frequency of collocates in corpus\n",
    "\tdf = df.join(self.corpus.vocab.collect().select(['token_id', 'token', frequency_column]), on='token_id', how='left', maintain_order='left')\n",
    "\tdf = df.rename({frequency_column: 'frequency'})\n",
    "\n",
    "\tfiltering_descriptors = []\n",
    "\tif min_collocate_frequency > 1: # applying min_frequency filter\n",
    "\t\tdf = df.filter(pl.col('collocate_frequency') >= min_collocate_frequency)\n",
    "\t\tfiltering_descriptors.append(f'minimum collocation frequency ({min_collocate_frequency})')\n",
    "\tif len(filtering_descriptors) > 0:\n",
    "\t\tformatted_data.append(f'Filtered tokens by {(\", \".join(filtering_descriptors))}')\n",
    "\n",
    "\tif effect_size_measure == 'logdice':\n",
    "\t\t# calculating collocation measure\n",
    "\t\t# from old code: logdice = 14 + math.log2((2 * collocate_count) / (node_frequency + loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "\t\tdf = df.with_columns(\n",
    "\t\t\t(pl.lit(14) + ((2 * pl.col('collocate_frequency')) / (pl.lit(token_positions[0].shape[0]) + pl.col('frequency'))).log(2))\n",
    "\t\t\t.alias('logdice')\n",
    "\t\t)\n",
    "\t\tcolumns.append('logdice')\n",
    "\telif effect_size_measure == 'mutual_information':\n",
    "\t\t# from old code: mi = math.log2((loaded_corpora[corpus_name]['token_count'] * collocate_count) / (node_frequency * loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "\t\tdf = df.with_columns(\n",
    "\t\t\t(pl.lit(count_tokens) * pl.col('collocate_frequency') / (pl.lit(token_positions[0].shape[0]) * pl.col('frequency'))).log(2)\n",
    "\t\t\t.alias('mutual_information')\n",
    "\t\t)\n",
    "\t\tcolumns.append('mutual_information')\n",
    "\n",
    "\tif statistical_significance_measure == 'log_likelihood':\n",
    "\t\t# based on calculation for keyness: https://ucrel.lancs.ac.uk/llwizard.html\n",
    "\t\t# a = collocate frequency in context, is collocate_frequency\n",
    "\t\t# b = collocate frequency outside context ...\n",
    "\t\t# reminder: for individual tokens constituting the node, collocate_frequency_outside_context should exclude frequency of tokens in node\n",
    "\t\tdf = df.with_columns(\n",
    "\t\t\tpl.max_horizontal([(pl.col('frequency') - pl.col('collocate_frequency') - pl.col('node_frequency')), pl.lit(0)]).alias('collocate_frequency_outside_context')\n",
    "\t\t)\n",
    "\n",
    "\t\t# c = total tokens in context windows, is token_count_in_context_window calculated above\n",
    "\t\t# d = total tokens outside context windows\n",
    "\t\ttotal_tokens_outside_context_window = count_tokens - token_count_in_context_window - (token_positions[0].shape[0] * sequence_len)\n",
    "\n",
    "\t\t# E1 = c*(a+b) / (c+d) \n",
    "\t\t# E2 = d*(a+b) / (c+d)\n",
    "\t\t# E1 and E2\n",
    "\t\tdf = df.with_columns(\n",
    "\t\t\t((pl.lit(token_count_in_context_window) * (pl.col('collocate_frequency') + pl.col('collocate_frequency_outside_context'))) / (pl.lit(token_count_in_context_window) + pl.lit(total_tokens_outside_context_window))).alias('expected_frequency1'),\n",
    "\t\t\t((pl.lit(total_tokens_outside_context_window) * (pl.col('collocate_frequency') + pl.col('collocate_frequency_outside_context'))) / (pl.lit(token_count_in_context_window) + pl.lit(total_tokens_outside_context_window))).alias('expected_frequency2'), \n",
    "\t\t)\n",
    "\n",
    "\t\t# G2 = 2*((a*ln (a/E1)) + (b*ln (b/E2))) \n",
    "\t\t# components of G2 as term1 and term 2 - (a*ln (a/E1)) (b*ln (b/E2))\n",
    "\t\tdf = df.with_columns([\n",
    "\t\t\tpl.when(pl.col('collocate_frequency') > 0)\n",
    "\t\t\t.then(pl.col('collocate_frequency') * (pl.col('collocate_frequency') / pl.col('expected_frequency1')).log())\n",
    "\t\t\t.otherwise(0)\n",
    "\t\t\t.alias('term1'),\n",
    "\t\t\tpl.when(pl.col('collocate_frequency_outside_context') > 0)\n",
    "\t\t\t.then(pl.col('collocate_frequency_outside_context') * (pl.col('collocate_frequency_outside_context') / pl.col('expected_frequency2')).log())\n",
    "\t\t\t.otherwise(0)\n",
    "\t\t\t.alias('term2') # 0 if no reference frequency\n",
    "\t\t])\n",
    "\n",
    "\t\t# G2\n",
    "\t\tdf = df.with_columns(\n",
    "\t\t\t(2 * (pl.col('term1') + pl.col('term2'))).alias('log_likelihood')\n",
    "\t\t)\n",
    "\t\tcolumns.append('log_likelihood')\n",
    "\n",
    "\tunique_collocates = df.select(pl.len()).item()\n",
    "\n",
    "\tif statistical_significance_cut is not None and statistical_significance_cut > 0:\n",
    "\t\tp = statistical_significance_cut\n",
    "\t\t# bonferroni correction\n",
    "\t\tif apply_bonferroni:\n",
    "\t\t\tp_value_descriptor = f'Keywords filtered based on p-value {p} with Bonferroni correction (based on {unique_collocates} tests)'\n",
    "\t\t\tp = p / unique_collocates # adjust by criteria\n",
    "\t\telse:\n",
    "\t\t\tp_value_descriptor = f'Keywords filtered based on p-value: {p}'\n",
    "\t\tcut = chi2.ppf(1 - p, df=1)\t\t\n",
    "\t\tdf = df.filter(pl.col('log_likelihood') > cut)\n",
    "\t\tformatted_data.append(p_value_descriptor)\n",
    "\t\tunique_collocates = df.select(pl.len()).item()\n",
    "\n",
    "\tformatted_data.append(f'Unique collocates: {unique_collocates:,.0f}')\n",
    "\n",
    "\t# prepare report information and paging ...\n",
    "\tif order is None:\n",
    "\t\torder = effect_size_measure\n",
    "\tdf = df.sort(order, descending = order_descending).with_row_index('rank', offset=1)\n",
    "\n",
    "\tif page_size > 0:\n",
    "\t\tstart = (page_current - 1) * page_size\n",
    "\t\tdf = df.slice(start, page_size)\n",
    "\n",
    "\tif page_size != 0 and unique_collocates > page_size:\n",
    "\t\tformatted_data.append(f'Showing {page_size} rows')\n",
    "\t\tformatted_data.append(f'Page {page_current} of {unique_collocates // page_size + 1}')\n",
    "\n",
    "\t#formatted_data.append(f'{total_descriptor}: {count_tokens:,.0f}')\n",
    "\tif debug:\n",
    "\t\tcolumns = df.columns\n",
    "\n",
    "\tlogger.info(f\"Collocates calculated in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\treturn Result(type = 'collocates', df = df.select(columns), title=f'Collocates of \"{token_str}\"', description=f'{self.corpus.name}', summary_data={}, formatted_data=formatted_data)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocates = Collocates(congress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 12:18:32 - INFO - _init_token_arrays - Created tokens_array in 0.081 seconds\n",
      "2025-06-07 12:18:32 - INFO - _init_token_arrays - Created tokens_lookup in 0.086 seconds\n",
      "2025-06-07 12:18:33 - INFO - _init_token_arrays - Created tokens_sort_order in 1.406 seconds\n",
      "2025-06-07 12:18:33 - INFO - tokenize - Tokenization time: 1.59777 seconds\n",
      "2025-06-07 12:18:34 - INFO - get_token_positions - Token indexing (3758) time: 0.55651 seconds\n",
      "2025-06-07 12:18:34 - INFO - _get_tokens_in_context - Collocates retrieved in 0.10 seconds.\n",
      "2025-06-07 12:18:34 - INFO - _get_tokens_in_context - Collocates retrieved in 0.11 seconds.\n",
      "2025-06-07 12:18:34 - INFO - _get_tokens_in_context - Collocates retrieved in 0.07 seconds.\n",
      "2025-06-07 12:18:34 - INFO - collocates - Collocates calculated in 0.95 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"pomoekdmkr\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#pomoekdmkr table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#pomoekdmkr thead, tbody, tfoot, tr, td, th { border-style: none !important; }\n",
       " tr { background-color: transparent !important; }\n",
       "#pomoekdmkr p { margin: 0 !important; padding: 0 !important; }\n",
       " #pomoekdmkr .gt_table { display: table !important; border-collapse: collapse !important; line-height: normal !important; margin-left: 0 !important; margin-right: auto !important; color: #333333 !important; font-size: 16px !important; font-weight: normal !important; font-style: normal !important; background-color: #FFFFFF !important; width: auto !important; border-top-style: solid !important; border-top-width: 2px !important; border-top-color: #A8A8A8 !important; border-right-style: none !important; border-right-width: 2px !important; border-right-color: #D3D3D3 !important; border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #A8A8A8 !important; border-left-style: none !important; border-left-width: 2px !important; border-left-color: #D3D3D3 !important; }\n",
       " #pomoekdmkr .gt_caption { padding-top: 4px !important; padding-bottom: 4px !important; }\n",
       " #pomoekdmkr .gt_title { color: #333333 !important; font-size: 125% !important; font-weight: initial !important; padding-top: 4px !important; padding-bottom: 4px !important; padding-left: 5px !important; padding-right: 5px !important; border-bottom-color: #FFFFFF !important; border-bottom-width: 0 !important; }\n",
       " #pomoekdmkr .gt_subtitle { color: #333333 !important; font-size: 85% !important; font-weight: initial !important; padding-top: 3px !important; padding-bottom: 5px !important; padding-left: 5px !important; padding-right: 5px !important; border-top-color: #FFFFFF !important; border-top-width: 0 !important; }\n",
       " #pomoekdmkr .gt_heading { background-color: #FFFFFF !important; text-align: center !important; border-bottom-color: #FFFFFF !important; border-left-style: none !important; border-left-width: 1px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 1px !important; border-right-color: #D3D3D3 !important; }\n",
       " #pomoekdmkr .gt_bottom_border { border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; }\n",
       " #pomoekdmkr .gt_col_headings { border-top-style: solid !important; border-top-width: 2px !important; border-top-color: #D3D3D3 !important; border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; border-left-style: none !important; border-left-width: 1px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 1px !important; border-right-color: #D3D3D3 !important; }\n",
       " #pomoekdmkr .gt_col_heading { color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: normal !important; text-transform: inherit !important; border-left-style: none !important; border-left-width: 1px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 1px !important; border-right-color: #D3D3D3 !important; vertical-align: bottom !important; padding-top: 5px !important; padding-bottom: 5px !important; padding-left: 5px !important; padding-right: 5px !important; overflow-x: hidden !important; }\n",
       " #pomoekdmkr .gt_column_spanner_outer { color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: normal !important; text-transform: inherit !important; padding-top: 0 !important; padding-bottom: 0 !important; padding-left: 4px !important; padding-right: 4px !important; }\n",
       " #pomoekdmkr .gt_column_spanner_outer:first-child { padding-left: 0 !important; }\n",
       " #pomoekdmkr .gt_column_spanner_outer:last-child { padding-right: 0 !important; }\n",
       " #pomoekdmkr .gt_column_spanner { border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; vertical-align: bottom !important; padding-top: 5px !important; padding-bottom: 5px !important; overflow-x: hidden !important; display: inline-block !important; width: 100% !important; }\n",
       " #pomoekdmkr .gt_spanner_row { border-bottom-style: hidden !important; }\n",
       " #pomoekdmkr .gt_group_heading { padding-top: 8px !important; padding-bottom: 8px !important; padding-left: 5px !important; padding-right: 5px !important; color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: initial !important; text-transform: inherit !important; border-top-style: solid !important; border-top-width: 2px !important; border-top-color: #D3D3D3 !important; border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; border-left-style: none !important; border-left-width: 1px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 1px !important; border-right-color: #D3D3D3 !important; vertical-align: middle !important; text-align: left !important; }\n",
       " #pomoekdmkr .gt_empty_group_heading { padding: 0.5px !important; color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: initial !important; border-top-style: solid !important; border-top-width: 2px !important; border-top-color: #D3D3D3 !important; border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; vertical-align: middle !important; }\n",
       " #pomoekdmkr .gt_from_md> :first-child { margin-top: 0 !important; }\n",
       " #pomoekdmkr .gt_from_md> :last-child { margin-bottom: 0 !important; }\n",
       " #pomoekdmkr .gt_row { padding-top: 8px !important; padding-bottom: 8px !important; padding-left: 5px !important; padding-right: 5px !important; margin: 10px !important; border-top-style: solid !important; border-top-width: 1px !important; border-top-color: #D3D3D3 !important; border-left-style: none !important; border-left-width: 1px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 1px !important; border-right-color: #D3D3D3 !important; vertical-align: middle !important; overflow-x: hidden !important; }\n",
       " #pomoekdmkr .gt_stub { color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: initial !important; text-transform: inherit !important; border-right-style: solid !important; border-right-width: 2px !important; border-right-color: #D3D3D3 !important; padding-left: 5px !important; padding-right: 5px !important; }\n",
       " #pomoekdmkr .gt_stub_row_group { color: #333333 !important; background-color: #FFFFFF !important; font-size: 100% !important; font-weight: initial !important; text-transform: inherit !important; border-right-style: solid !important; border-right-width: 2px !important; border-right-color: #D3D3D3 !important; padding-left: 5px !important; padding-right: 5px !important; vertical-align: top !important; }\n",
       " #pomoekdmkr .gt_row_group_first td { border-top-width: 2px !important; }\n",
       " #pomoekdmkr .gt_row_group_first th { border-top-width: 2px !important; }\n",
       " #pomoekdmkr .gt_striped { background-color: rgba(128,128,128,0.05) !important; }\n",
       " #pomoekdmkr .gt_table_body { border-top-style: solid !important; border-top-width: 2px !important; border-top-color: #D3D3D3 !important; border-bottom-style: solid !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; }\n",
       " #pomoekdmkr .gt_sourcenotes { color: #333333 !important; background-color: #FFFFFF !important; border-bottom-style: none !important; border-bottom-width: 2px !important; border-bottom-color: #D3D3D3 !important; border-left-style: none !important; border-left-width: 2px !important; border-left-color: #D3D3D3 !important; border-right-style: none !important; border-right-width: 2px !important; border-right-color: #D3D3D3 !important; }\n",
       " #pomoekdmkr .gt_sourcenote { font-size: 90% !important; padding-top: 4px !important; padding-bottom: 4px !important; padding-left: 5px !important; padding-right: 5px !important; text-align: left !important; }\n",
       " #pomoekdmkr .gt_left { text-align: left !important; }\n",
       " #pomoekdmkr .gt_center { text-align: center !important; }\n",
       " #pomoekdmkr .gt_right { text-align: right !important; font-variant-numeric: tabular-nums !important; }\n",
       " #pomoekdmkr .gt_font_normal { font-weight: normal !important; }\n",
       " #pomoekdmkr .gt_font_bold { font-weight: bold !important; }\n",
       " #pomoekdmkr .gt_font_italic { font-style: italic !important; }\n",
       " #pomoekdmkr .gt_super { font-size: 65% !important; }\n",
       " #pomoekdmkr .gt_footnote_marks { font-size: 75% !important; vertical-align: 0.4em !important; position: initial !important; }\n",
       " #pomoekdmkr .gt_asterisk { font-size: 100% !important; vertical-align: 0 !important; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"6\" class=\"gt_heading gt_title gt_font_normal\">Collocates of &quot;economy&quot;</td>\n",
       "  </tr>\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"6\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\">US Congressional Speeches Subset 100k</td>\n",
       "  </tr>\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Rank\">Rank</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Token\">Token</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Collocate-Frequency\">Collocate Frequency</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Frequency\">Frequency</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Logdice\">Logdice</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Log-Likelihood\">Log Likelihood</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">1</td>\n",
       "    <td class=\"gt_row gt_left\">our</td>\n",
       "    <td class=\"gt_row gt_right\">1,084</td>\n",
       "    <td class=\"gt_row gt_right\">60,051</td>\n",
       "    <td class=\"gt_row gt_right\">9.12</td>\n",
       "    <td class=\"gt_row gt_right\">2,801.70</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">2</td>\n",
       "    <td class=\"gt_row gt_left\">efficiency</td>\n",
       "    <td class=\"gt_row gt_right\">60</td>\n",
       "    <td class=\"gt_row gt_right\">732</td>\n",
       "    <td class=\"gt_row gt_right\">8.77</td>\n",
       "    <td class=\"gt_row gt_right\">329.93</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">3</td>\n",
       "    <td class=\"gt_row gt_left\">stimulate</td>\n",
       "    <td class=\"gt_row gt_right\">51</td>\n",
       "    <td class=\"gt_row gt_right\">299</td>\n",
       "    <td class=\"gt_row gt_right\">8.69</td>\n",
       "    <td class=\"gt_row gt_right\">358.80</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">4</td>\n",
       "    <td class=\"gt_row gt_left\">global</td>\n",
       "    <td class=\"gt_row gt_right\">55</td>\n",
       "    <td class=\"gt_row gt_right\">618</td>\n",
       "    <td class=\"gt_row gt_right\">8.69</td>\n",
       "    <td class=\"gt_row gt_right\">311.68</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">5</td>\n",
       "    <td class=\"gt_row gt_left\">jobs</td>\n",
       "    <td class=\"gt_row gt_right\">83</td>\n",
       "    <td class=\"gt_row gt_right\">3,100</td>\n",
       "    <td class=\"gt_row gt_right\">8.63</td>\n",
       "    <td class=\"gt_row gt_right\">274.52</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">6</td>\n",
       "    <td class=\"gt_row gt_left\">sector</td>\n",
       "    <td class=\"gt_row gt_right\">50</td>\n",
       "    <td class=\"gt_row gt_right\">866</td>\n",
       "    <td class=\"gt_row gt_right\">8.47</td>\n",
       "    <td class=\"gt_row gt_right\">239.69</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">7</td>\n",
       "    <td class=\"gt_row gt_left\">economy</td>\n",
       "    <td class=\"gt_row gt_right\">70</td>\n",
       "    <td class=\"gt_row gt_right\">3,758</td>\n",
       "    <td class=\"gt_row gt_right\">8.25</td>\n",
       "    <td class=\"gt_row gt_right\">865.99</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">8</td>\n",
       "    <td class=\"gt_row gt_left\">growth</td>\n",
       "    <td class=\"gt_row gt_right\">51</td>\n",
       "    <td class=\"gt_row gt_right\">1,762</td>\n",
       "    <td class=\"gt_row gt_right\">8.24</td>\n",
       "    <td class=\"gt_row gt_right\">176.16</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">9</td>\n",
       "    <td class=\"gt_row gt_left\">segment</td>\n",
       "    <td class=\"gt_row gt_right\">34</td>\n",
       "    <td class=\"gt_row gt_right\">235</td>\n",
       "    <td class=\"gt_row gt_right\">8.12</td>\n",
       "    <td class=\"gt_row gt_right\">227.17</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">10</td>\n",
       "    <td class=\"gt_row gt_left\">vital</td>\n",
       "    <td class=\"gt_row gt_right\">45</td>\n",
       "    <td class=\"gt_row gt_right\">1,707</td>\n",
       "    <td class=\"gt_row gt_right\">8.08</td>\n",
       "    <td class=\"gt_row gt_right\">147.53</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">11</td>\n",
       "    <td class=\"gt_row gt_left\">american</td>\n",
       "    <td class=\"gt_row gt_right\">165</td>\n",
       "    <td class=\"gt_row gt_right\">18,173</td>\n",
       "    <td class=\"gt_row gt_right\">7.95</td>\n",
       "    <td class=\"gt_row gt_right\">235.41</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">12</td>\n",
       "    <td class=\"gt_row gt_left\">competitive</td>\n",
       "    <td class=\"gt_row gt_right\">34</td>\n",
       "    <td class=\"gt_row gt_right\">842</td>\n",
       "    <td class=\"gt_row gt_right\">7.92</td>\n",
       "    <td class=\"gt_row gt_right\">139.11</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">13</td>\n",
       "    <td class=\"gt_row gt_left\">nations</td>\n",
       "    <td class=\"gt_row gt_right\">82</td>\n",
       "    <td class=\"gt_row gt_right\">7,412</td>\n",
       "    <td class=\"gt_row gt_right\">7.91</td>\n",
       "    <td class=\"gt_row gt_right\">142.89</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">14</td>\n",
       "    <td class=\"gt_row gt_left\">growing</td>\n",
       "    <td class=\"gt_row gt_right\">38</td>\n",
       "    <td class=\"gt_row gt_right\">1,432</td>\n",
       "    <td class=\"gt_row gt_right\">7.91</td>\n",
       "    <td class=\"gt_row gt_right\">125.05</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">15</td>\n",
       "    <td class=\"gt_row gt_left\">sectors</td>\n",
       "    <td class=\"gt_row gt_right\">28</td>\n",
       "    <td class=\"gt_row gt_right\">131</td>\n",
       "    <td class=\"gt_row gt_right\">7.88</td>\n",
       "    <td class=\"gt_row gt_right\">210.88</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">16</td>\n",
       "    <td class=\"gt_row gt_left\">help</td>\n",
       "    <td class=\"gt_row gt_right\">76</td>\n",
       "    <td class=\"gt_row gt_right\">6,905</td>\n",
       "    <td class=\"gt_row gt_right\">7.87</td>\n",
       "    <td class=\"gt_row gt_right\">131.80</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">17</td>\n",
       "    <td class=\"gt_row gt_left\">entire</td>\n",
       "    <td class=\"gt_row gt_right\">47</td>\n",
       "    <td class=\"gt_row gt_right\">3,482</td>\n",
       "    <td class=\"gt_row gt_right\">7.73</td>\n",
       "    <td class=\"gt_row gt_right\">97.56</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">18</td>\n",
       "    <td class=\"gt_row gt_left\">agricultural</td>\n",
       "    <td class=\"gt_row gt_right\">41</td>\n",
       "    <td class=\"gt_row gt_right\">2,598</td>\n",
       "    <td class=\"gt_row gt_right\">7.72</td>\n",
       "    <td class=\"gt_row gt_right\">96.20</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">19</td>\n",
       "    <td class=\"gt_row gt_left\">impact</td>\n",
       "    <td class=\"gt_row gt_right\">36</td>\n",
       "    <td class=\"gt_row gt_right\">1,878</td>\n",
       "    <td class=\"gt_row gt_right\">7.71</td>\n",
       "    <td class=\"gt_row gt_right\">96.93</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">20</td>\n",
       "    <td class=\"gt_row gt_left\">domestic</td>\n",
       "    <td class=\"gt_row gt_right\">38</td>\n",
       "    <td class=\"gt_row gt_right\">2,193</td>\n",
       "    <td class=\"gt_row gt_right\">7.71</td>\n",
       "    <td class=\"gt_row gt_right\">95.44</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "  <tfoot class=\"gt_sourcenotes\">\n",
       "  \n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Report based on word tokens</td>\n",
       "  </tr>\n",
       "\n",
       "\n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Context tokens left: 5, context tokens right: 5</td>\n",
       "  </tr>\n",
       "\n",
       "\n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Filtered tokens by minimum collocation frequency (5)</td>\n",
       "  </tr>\n",
       "\n",
       "\n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Keywords filtered based on p-value 0.0001 with Bonferroni correction (based on 864 tests)</td>\n",
       "  </tr>\n",
       "\n",
       "\n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Unique collocates: 123</td>\n",
       "  </tr>\n",
       "\n",
       "\n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Showing 20 rows</td>\n",
       "  </tr>\n",
       "\n",
       "\n",
       "  <tr>\n",
       "    <td class=\"gt_sourcenote\" colspan=\"6\">Page 1 of 7</td>\n",
       "  </tr>\n",
       "\n",
       "</tfoot>\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.83 s, sys: 947 ms, total: 2.77 s\n",
      "Wall time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "for word in [\"economy\"]: # brown used 'i went in', 'any of us',  for testing \"economy\"\n",
    "    %time collocates.collocates(word, order = None, order_descending = True, statistical_significance_cut = 0.0001, apply_bonferroni=True, effect_size_measure='logdice', context_length = 5, min_collocate_frequency = 5, page_current = 1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
