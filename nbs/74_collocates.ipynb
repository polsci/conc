{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocates\n",
    "\n",
    "> Functionality for collocation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp collocates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import time\n",
    "import polars as pl\n",
    "from fastcore.basics import patch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc.corpus import Corpus\n",
    "from conc.result import Result\n",
    "from conc.core import logger, PAGE_SIZE, set_logger_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = f'{os.environ.get(\"HOME\")}/data/'\n",
    "save_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'\n",
    "\n",
    "path_to_toy_corpus = f'{save_path}toy.corpus'\n",
    "path_to_brown_corpus = f'{save_path}brown.corpus'\n",
    "path_to_reuters_corpus = f'{save_path}reuters.corpus'\n",
    "path_to_gutenberg_corpus = f'{save_path}gutenberg.corpus'\n",
    "path_to_gardenparty_corpus = f'{save_path}garden-party.corpus'\n",
    "path_to_congress_corpus = f'{save_path}us-congressional-speeches-subset-100k.corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Collocates:\n",
    "\t\"\"\" Class for collocation analysis reporting. \"\"\"\n",
    "\tdef __init__(self,\n",
    "\t\t\t  corpus:Corpus # Corpus instance\n",
    "\t\t\t  ): \n",
    "\t\tself.corpus = corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _shift_zeroes_to_end(self:Collocates,\n",
    "\t\t\t\t\t\tarr:np.ndarray # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Move 0 value positions for punctuation and space removal \"\"\"\n",
    "\tresult = np.empty_like(arr)\n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tmask = col_data != 0\n",
    "\t\tresult[:mask.sum(), col] = col_data[mask]\n",
    "\t\tresult[mask.sum():, col] = 0\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _zero_after_value(self:Collocates,\n",
    "\t\t\t\t\t  arr:np.ndarray, # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t  target: int # Target value to find in the array (e.g., an end-of-file token or a specific collocate frequency)\n",
    "\t\t\t\t\t  ):\n",
    "\t\"\"\" Set values from first occurence of target value to 0 in each column (for processing tokens outside text using eof token) \"\"\"\n",
    "\tarr = arr.copy()  \n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tidx = np.where(col_data == target)[0]\n",
    "\t\tif idx.size > 0:\n",
    "\t\t\tfirst_idx = idx[0]\n",
    "\t\t\tarr[first_idx:, col] = 0\n",
    "\treturn arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning journal = notebook pick up collocation dominoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _get_collocates_in_context(self:Collocates,\n",
    "\t\t\t\t\t\t\t   token_positions:np.ndarray, # Numpy array of token positions in the corpus\n",
    "\t\t\t\t\t\t\t   index:str, # Index to use - lower_index, orth_index\n",
    "\t\t\t\t\t\t\t   context_words:int = 5, # Number of context words to consider on each side of the token\n",
    "\t\t\t\t\t\t\t   position_offset:int = 1 # offset to start retrieving context words - -1 for left, positive for right (may be adjusted by sequence_len)\n",
    "\t\t\t\t\t\t\t   ) -> Result:\n",
    "\t\"\"\" Get collocates in context for a given token index, operates one side at a time. \"\"\"\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tif position_offset < 0:\n",
    "\t\tposition_offset_step = -1\n",
    "\telse:\n",
    "\t\tposition_offset_step = 1\n",
    "\t\n",
    "\tcollected = False\n",
    "\tcontext_tokens_arr = []\n",
    "\twhile collected == False:\n",
    "\t\tnew_positions = np.array(token_positions[0] + position_offset, dtype = token_positions[0].dtype)\n",
    "\t\tcontext_tokens_arr.append(self.corpus.get_tokens_by_index(index)[new_positions])\n",
    "\t\tposition_offset += position_offset_step\n",
    "\t\tif len(context_tokens_arr) >= context_words: # cleaning spaces and punctuation and check if need more iterations\n",
    "\t\t\tcontext_tokens = np.array(context_tokens_arr, dtype = token_positions[0].dtype)\n",
    "\t\t\tcontext_tokens = np.where(np.isin(context_tokens, self.corpus.punct_tokens + self.corpus.space_tokens), 0, context_tokens)\n",
    "\t\t\tcounts = np.count_nonzero(context_tokens, axis=0)\n",
    "\t\t\tif np.min(counts) < context_words:\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\tcollected = True\n",
    "\n",
    "\tcontext_tokens = self._shift_zeroes_to_end(context_tokens)\n",
    "\tcontext_tokens = context_tokens[:context_words, :]\n",
    "\n",
    "\tif self.corpus.EOF_TOKEN in context_tokens:\n",
    "\t\tcontext_tokens = self._zero_after_value(context_tokens, self.corpus.EOF_TOKEN)\n",
    "\n",
    "\tlogger.info(f\"Collocates retrieved in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\treturn context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def collocates(self:Collocates, \n",
    "\t\t\t\ttoken_str:str, # Token to search for\n",
    "\t\t\t\tcollocation_measure:str = 'logdice', # statistical measure to use for collocation calculation: logdice, mutual_information \n",
    "\t\t\t\tcontext_words:int=5, # Window size\n",
    "\t\t\t\tmin_frequency:int=5, # Minimum count of collocates\n",
    "\t\t\t\tpage_size:int=PAGE_SIZE, # number of rows to return, if 0 returns all\n",
    "\t\t\t\tpage_current:int=1, # current page, ignored if page_size is 0\n",
    "\t\t\t\t) -> Result:\n",
    "\t\"\"\" Report collocates for a given token string. \"\"\"\n",
    "\n",
    "\ttoken_sequence, index_id = self.corpus.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "\tindex = 'lower_index'\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\n",
    "\ttoken_positions = self.corpus.get_token_positions(token_sequence, index_id)\n",
    "\n",
    "\t# getting context tokens\n",
    "\tleft_tokens = self._get_collocates_in_context(token_positions=token_positions, index=index, context_words=context_words, position_offset=-1)\n",
    "\tright_tokens = self._get_collocates_in_context(token_positions=token_positions, index=index, context_words=context_words, position_offset=sequence_len)\n",
    "\tcombined_tokens = np.concatenate([left_tokens.flatten(), right_tokens.flatten()])\n",
    "\tcombined_tokens = combined_tokens[combined_tokens != 0] # removes punctuation and space placeholder\n",
    "\n",
    "\t# getting frequencies of collocates\n",
    "\tunique_token_ids, counts = np.unique(combined_tokens, return_counts=True)\n",
    "\t#unique_token_ids = unique_token_ids.astype(np.int32)\n",
    "\n",
    "\tdf = pl.DataFrame({\n",
    "\t\t'token_id': unique_token_ids,\n",
    "\t\t'collocation_frequency': counts\n",
    "\t})\n",
    "\n",
    "\t# adding frequency of collocates in corpus\n",
    "\tdf = df.join(self.corpus.vocab.collect().select(['token_id', 'token', 'frequency_lower']), on='token_id', how='left', maintain_order='left')\n",
    "\n",
    "\t# applying min_frequency filter\n",
    "\tif min_frequency > 1:\n",
    "\t\tdf = df.filter(pl.col('collocation_frequency') >= min_frequency)\n",
    "\n",
    "\t# calculating collocation measure\n",
    "\t# from old code: logdice = 14 + math.log2((2 * collocate_count) / (node_frequency + loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "\tdf = df.with_columns(\n",
    "\t\t(pl.lit(14) + ((2 * pl.col('collocation_frequency')) / (pl.lit(token_positions[0].shape[0]) + pl.col('frequency_lower'))).log(2))\n",
    "\t\t.alias('logdice')\n",
    "\t)\n",
    "\n",
    "\t# from old code: mi = math.log2((loaded_corpora[corpus_name]['token_count'] * collocate_count) / (node_frequency * loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "\tdf = df.with_columns(\n",
    "\t\t(pl.lit(self.corpus.word_token_count) * pl.col('collocation_frequency') / (pl.lit(token_positions[0].shape[0]) * pl.col('frequency_lower'))).log(2)\n",
    "\t\t.alias('mutual_information')\n",
    "\t)\n",
    "\n",
    "\t# TODO slice result for page\n",
    "\n",
    "\t# prepare report information e.g. for filters etc - see frequencies or keyness as reference\n",
    "\n",
    "\tlogger.info(f\"Collocates calculated in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\treturn Result(type = 'collocates', df = df.sort(collocation_measure, descending = True).head(10), title='Collocates', description=f'Context words: {context_words}, Corpus: {self.corpus.name}', summary_data={}, formatted_data=[])\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:56:42 - INFO - memory_usage - init, memory usage: 2042.06640625 MB\n",
      "2025-06-06 11:56:42 - INFO - load - Load time: 0.201 seconds\n",
      "2025-06-06 11:56:42 - INFO - memory_usage - init, memory usage: 2042.06640625 MB\n",
      "2025-06-06 11:56:42 - INFO - load - Load time: 0.199 seconds\n",
      "2025-06-06 11:56:42 - INFO - memory_usage - init, memory usage: 2042.06640625 MB\n",
      "2025-06-06 11:56:43 - INFO - load - Load time: 0.199 seconds\n",
      "2025-06-06 11:56:43 - INFO - memory_usage - init, memory usage: 2042.06640625 MB\n",
      "2025-06-06 11:56:43 - INFO - load - Load time: 0.200 seconds\n",
      "2025-06-06 11:56:43 - INFO - memory_usage - init, memory usage: 2042.06640625 MB\n",
      "2025-06-06 11:56:43 - INFO - load - Load time: 0.198 seconds\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "set_logger_state('verbose')\n",
    "reuters = Corpus().load(path_to_reuters_corpus)\n",
    "brown = Corpus().load(path_to_brown_corpus)\n",
    "gardenparty = Corpus().load(path_to_gardenparty_corpus)\n",
    "gutenberg = Corpus().load(path_to_gutenberg_corpus)\n",
    "congress = Corpus().load(path_to_congress_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocates = Collocates(reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:57:57 - INFO - tokenize - Tokenization time: 0.00011 seconds\n",
      "2025-06-06 11:57:57 - INFO - get_token_positions - Token indexing (621) time: 0.00166 seconds\n",
      "2025-06-06 11:57:57 - INFO - _get_collocates_in_context - Collocates retrieved in 0.00 seconds.\n",
      "2025-06-06 11:57:57 - INFO - _get_collocates_in_context - Collocates retrieved in 0.00 seconds.\n",
      "2025-06-06 11:57:57 - INFO - collocates - Collocates calculated in 0.02 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"shqcosvqff\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#shqcosvqff table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#shqcosvqff thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#shqcosvqff p { margin: 0; padding: 0; }\n",
       " #shqcosvqff .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #shqcosvqff .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #shqcosvqff .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #shqcosvqff .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #shqcosvqff .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #shqcosvqff .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #shqcosvqff .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #shqcosvqff .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #shqcosvqff .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #shqcosvqff .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #shqcosvqff .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #shqcosvqff .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #shqcosvqff .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #shqcosvqff .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #shqcosvqff .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #shqcosvqff .gt_from_md> :first-child { margin-top: 0; }\n",
       " #shqcosvqff .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #shqcosvqff .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #shqcosvqff .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #shqcosvqff .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #shqcosvqff .gt_row_group_first td { border-top-width: 2px; }\n",
       " #shqcosvqff .gt_row_group_first th { border-top-width: 2px; }\n",
       " #shqcosvqff .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #shqcosvqff .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #shqcosvqff .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #shqcosvqff .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #shqcosvqff .gt_left { text-align: left; }\n",
       " #shqcosvqff .gt_center { text-align: center; }\n",
       " #shqcosvqff .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #shqcosvqff .gt_font_normal { font-weight: normal; }\n",
       " #shqcosvqff .gt_font_bold { font-weight: bold; }\n",
       " #shqcosvqff .gt_font_italic { font-style: italic; }\n",
       " #shqcosvqff .gt_super { font-size: 65%; }\n",
       " #shqcosvqff .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #shqcosvqff .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"6\" class=\"gt_heading gt_title gt_font_normal\">Collocates</td>\n",
       "  </tr>\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"6\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\">Context words: 5, Corpus: Reuters Corpus</td>\n",
       "  </tr>\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Token Id\">Token Id</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Collocation Frequency\">Collocation Frequency</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Token\">Token</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Frequency Lower\">Frequency Lower</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Logdice\">Logdice</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Mutual Information\">Mutual Information</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">71765</td>\n",
       "    <td class=\"gt_row gt_right\">29</td>\n",
       "    <td class=\"gt_row gt_left\">stimulate</td>\n",
       "    <td class=\"gt_row gt_right\">85</td>\n",
       "    <td class=\"gt_row gt_right\">10.39</td>\n",
       "    <td class=\"gt_row gt_right\">9.59</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">39297</td>\n",
       "    <td class=\"gt_row gt_right\">20</td>\n",
       "    <td class=\"gt_row gt_left\">boost</td>\n",
       "    <td class=\"gt_row gt_right\">222</td>\n",
       "    <td class=\"gt_row gt_right\">9.60</td>\n",
       "    <td class=\"gt_row gt_right\">7.66</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">36515</td>\n",
       "    <td class=\"gt_row gt_right\">35</td>\n",
       "    <td class=\"gt_row gt_left\">japanese</td>\n",
       "    <td class=\"gt_row gt_right\">944</td>\n",
       "    <td class=\"gt_row gt_right\">9.52</td>\n",
       "    <td class=\"gt_row gt_right\">6.38</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">55412</td>\n",
       "    <td class=\"gt_row gt_right\">27</td>\n",
       "    <td class=\"gt_row gt_left\">domestic</td>\n",
       "    <td class=\"gt_row gt_right\">700</td>\n",
       "    <td class=\"gt_row gt_right\">9.39</td>\n",
       "    <td class=\"gt_row gt_right\">6.44</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">21138</td>\n",
       "    <td class=\"gt_row gt_right\">23</td>\n",
       "    <td class=\"gt_row gt_left\">german</td>\n",
       "    <td class=\"gt_row gt_right\">537</td>\n",
       "    <td class=\"gt_row gt_right\">9.35</td>\n",
       "    <td class=\"gt_row gt_right\">6.59</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">7135</td>\n",
       "    <td class=\"gt_row gt_right\">35</td>\n",
       "    <td class=\"gt_row gt_left\">world</td>\n",
       "    <td class=\"gt_row gt_right\">1,173</td>\n",
       "    <td class=\"gt_row gt_right\">9.32</td>\n",
       "    <td class=\"gt_row gt_right\">6.07</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">62950</td>\n",
       "    <td class=\"gt_row gt_right\">12</td>\n",
       "    <td class=\"gt_row gt_left\">grew</td>\n",
       "    <td class=\"gt_row gt_right\">103</td>\n",
       "    <td class=\"gt_row gt_right\">9.09</td>\n",
       "    <td class=\"gt_row gt_right\">8.04</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">63903</td>\n",
       "    <td class=\"gt_row gt_right\">10</td>\n",
       "    <td class=\"gt_row gt_left\">sluggish</td>\n",
       "    <td class=\"gt_row gt_right\">44</td>\n",
       "    <td class=\"gt_row gt_right\">8.94</td>\n",
       "    <td class=\"gt_row gt_right\">9.00</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">61316</td>\n",
       "    <td class=\"gt_row gt_right\">18</td>\n",
       "    <td class=\"gt_row gt_left\">economy</td>\n",
       "    <td class=\"gt_row gt_right\">621</td>\n",
       "    <td class=\"gt_row gt_right\">8.89</td>\n",
       "    <td class=\"gt_row gt_right\">6.03</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_right\">66052</td>\n",
       "    <td class=\"gt_row gt_right\">13</td>\n",
       "    <td class=\"gt_row gt_left\">measures</td>\n",
       "    <td class=\"gt_row gt_right\">288</td>\n",
       "    <td class=\"gt_row gt_right\">8.87</td>\n",
       "    <td class=\"gt_row gt_right\">6.67</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.2 ms, sys: 12.7 ms, total: 59.9 ms\n",
      "Wall time: 27.4 ms\n"
     ]
    }
   ],
   "source": [
    "for word in [\"economy\"]: # brown used 'i went in', 'any of us',  for testing \"economy\"\n",
    "    %time collocates.collocates(word, collocation_measure='logdice', context_words=5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "## Collocation methods\n",
    "# \n",
    "# # @patch\n",
    "# def collocates(self: Corpus, word, nice_word, constrain_to = False, context_length = 5, limit = 50, cutoff=5, stat = 'ld', output = False):\n",
    "#     elements = {}\n",
    "#     nodes = []\n",
    "#     edges = []\n",
    "\n",
    "#     coll_token_sequence, coll_index_id = tokenize_string(corpus_name, word)\n",
    "#     token_id = coll_token_sequence[0][0]\n",
    "    \n",
    "#     #print(token_id, nice_word)\n",
    "    \n",
    "#     if constrain_to == False:\n",
    "#         nodes.append((token_id, {'label': nice_word, 'size': 1}))\n",
    "    \n",
    "#     #print(coll_token_sequence, coll_index_id)\n",
    "#     coll_token_index = profile_get_token_index(corpus_name, coll_token_sequence, coll_index_id) # get_token_positions\n",
    "#     #print(coll_token_index)\n",
    "#     positional_columns, concordance = profile_get_concordance(corpus_name, coll_token_sequence, coll_token_index, context_words = context_length, index_id = LOWER)\n",
    "#     #print(concordance)\n",
    "#     collocates = []\n",
    "#     for row in concordance:\n",
    "#         if eof_token in row:\n",
    "#             indexes = np.where(np.array(row) == eof_token)[0]\n",
    "#             #print(indexes)\n",
    "#             slice_min = -1\n",
    "#             slice_max = context_length * 2 + 1\n",
    "#             for i in indexes:\n",
    "#                 if i < context_length and i > slice_min:\n",
    "#                     slice_min = i\n",
    "#                 elif i > context_length and i < slice_max:\n",
    "#                     #print('***')\n",
    "#                     slice_max = i\n",
    "#             slice_min += 1\n",
    "#             #slice_max -= 1\n",
    "#             #print(slice_min,slice_max)\n",
    "#             #print(row)\n",
    "#             #print(row[slice_min:slice_max])\n",
    "#             collocates.append(row[slice_min:slice_max])\n",
    "#         else:\n",
    "#             collocates.append(row)\n",
    "\n",
    "#     node_frequency = len(collocates)\n",
    "            \n",
    "#     if len(collocates) < 1:\n",
    "#         print('no collocates')\n",
    "#     else:\n",
    "#         #print(collocates)\n",
    "#         collocates = np.concatenate(collocates)\n",
    "#         collocates = np.unique(collocates, axis=0, return_counts=True)\n",
    "#         #collocates = collocates[collocates[:,1].argsort()]\n",
    "        \n",
    "#         #print(collocates)\n",
    "#         logdices = []\n",
    "\n",
    "#         for row in range(len(collocates[0])):\n",
    "#             collocate = collocates[0][row]\n",
    "#             collocate_count = collocates[1][row]\n",
    "#             if constrain_to == False:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 if loaded_corpora[corpus_name]['vocab'][collocate] in constrain_to:\n",
    "#                     pass\n",
    "#                     #print(corpus['vocab'][collocate],' in constrain_to', constrain_to)\n",
    "#                 else:\n",
    "#                     continue\n",
    "#             if collocate in coll_token_sequence:\n",
    "#                 #print('match china')\n",
    "#                 pass\n",
    "#             elif collocate_count > 1:\n",
    "#                 #14 + log2D=14+log2(2fxy/(fx+fy))\n",
    "\n",
    "#                 if re.search(_RE_PUNCT,loaded_corpora[corpus_name]['vocab'][collocate]) is None: # FIX - HAVE OPION TO REMOVE PUNC\n",
    "#                     if collocate_count >= cutoff:\n",
    "#                         logdice = 14 + math.log2((2 * collocate_count) / (node_frequency + loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "#                         mi = math.log2((loaded_corpora[corpus_name]['token_count'] * collocate_count) / (node_frequency * loaded_corpora[corpus_name]['frequency_lookup'][collocate]))\n",
    "#                         logdices.append([logdice, mi, collocate, loaded_corpora[corpus_name]['vocab'][collocate], collocate_count, loaded_corpora[corpus_name]['frequency_lookup'][collocate]])\n",
    "#                         #if logdice > 8:\n",
    "#                         #print(collocate, node_frequency, loaded_corpora[corpus_name]['vocab'][collocate], collocates[1][row],logdice, mi)\n",
    "#                         matches = np.where(collocates == collocate)[0]\n",
    "#                         collocate_count = len(matches)\n",
    "#         #        if (collocate_count > 5):\n",
    "    \n",
    "#     top_collocates = []\n",
    "#     if stat == 'mi':\n",
    "#         sorted_collocates = sorted(logdices, reverse=True, key=lambda x: x[1])[0:limit]\n",
    "#         for row in sorted_collocates:\n",
    "#             nodes.append((row[2], {'label': row[3], 'size': 1}))\n",
    "#             edges.append((token_id, row[2], {'weight': 1}))\n",
    "#             #print(row)\n",
    "#             top_collocates.append(row[3])\n",
    "#     else:\n",
    "#         sorted_collocates = sorted(logdices, reverse=True, key=lambda x: x[0])[0:limit]\n",
    "#         for row in sorted_collocates:\n",
    "#             nodes.append((row[2], {'label': row[3], 'size': 1}))\n",
    "#             edges.append((token_id, row[2], {'weight': row[0]}))\n",
    "#             top_collocates.append(row[3])\n",
    "    \n",
    "#     if constrain_to == False:\n",
    "#         for top_collocate in top_collocates:\n",
    "#             top_collocate_elements, top_collocate_sorted, df = profile_prepare_collocates(corpus_name, top_collocate, top_collocate, constrain_to = top_collocates, context_length=context_length, limit = limit, stat=stat)\n",
    "#             for edge in top_collocate_elements['edges']:\n",
    "#                 edges.append(edge) \n",
    "    \n",
    "    \n",
    "#     #display(sorted(logdices, reverse=True, key=lambda x: x[0])[0:limit])\n",
    "    \n",
    "#     elements['nodes'] = nodes\n",
    "#     elements['edges'] = edges\n",
    "#     df = pd.DataFrame(sorted_collocates, columns = ['logdice', 'mi', 'collocate_token_id', 'collocate', 'collocate_count', 'collocate_token_frequency'])\n",
    "#     # if output != False:\n",
    "#     #     if output == 'file':\n",
    "#     #         with open(output_dir + corpus_name + '/collocates-' + stat + '-' + nice_word + '.html', 'w', encoding='utf8') as f:\n",
    "#     #             f.write(df.to_html(classes='table table-stripped'))\n",
    "#     return elements, sorted_collocates, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
