{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6713975",
   "metadata": {},
   "source": [
    "# Anatomy of a corpus\n",
    "\n",
    "> Information on Conc corpus format if you want to access the data directly.\n",
    "- toc: false\n",
    "- page-layout: full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cdbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import polars as pl\n",
    "import os\n",
    "import msgspec\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = f'{os.environ.get(\"HOME\")}/data/'\n",
    "save_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'\n",
    "\n",
    "path_to_toy_corpus = f'{save_path}toy.corpus'\n",
    "path_to_brown_corpus = f'{save_path}brown.corpus'\n",
    "path_to_reuters_corpus = f'{save_path}reuters.corpus'\n",
    "path_to_gardenparty_corpus = f'{save_path}garden-party.corpus'\n",
    "path_to_congress_corpus = f'{save_path}us-congressional-speeches-subset-10k.corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad510588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from conc.corpus import Corpus, CorpusMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734e6c6",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301dc260",
   "metadata": {},
   "source": [
    "A Conc corpus is a directory containing files with specific names and formats to represent the data. This document provides an overview of the various files and what they contain. Here is the directory structure of an example Conc corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac48f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── garden-party.corpus\n",
      "│   ├── tokens.parquet\n",
      "│   ├── spaces.parquet\n",
      "│   ├── README.md\n",
      "│   ├── puncts.parquet\n",
      "│   ├── corpus.json\n",
      "│   ├── vocab.parquet\n",
      "│   └── metadata.parquet\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "def print_directory_tree(path, prefix=\"\", restrict_to=None):\n",
    "\tpath = Path(path)\n",
    "\tcontents = list(path.iterdir())\n",
    "\tpointers = ['├── '] * (len(contents) - 1) + ['└── ']\n",
    "\tfor pointer, child in zip(pointers, contents):\n",
    "\t\tif restrict_to is not None and restrict_to not in child.name:\n",
    "\t\t\tcontinue\n",
    "\t\tprint(prefix + pointer + child.name)\n",
    "\t\tif child.is_dir():\n",
    "\t\t\textension = '│   ' if pointer == '├── ' else '    '\n",
    "\t\t\tprint_directory_tree(child, prefix + extension)\n",
    "\n",
    "# Example usage: show current directory\n",
    "print_directory_tree(f'{save_path}', '', restrict_to='garden-party.corpus')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45e7d1",
   "metadata": {},
   "source": [
    "Note: by default the library creates a directory with the `.corpus` suffix. The directory name is created automatically on build based on a slugified version of the corpus name you assigned.  \n",
    "\n",
    "For example, if you passed in the name:  \n",
    "\n",
    "\tGarden Party Corpus\n",
    "\n",
    "The directory will be:  \n",
    "\n",
    "\tgarden-party.corpus\n",
    "\n",
    "The directory can be renamed and still loaded. The `.corpus` extension is intended to make corpora on your filesystem easier to find or identify.\n",
    "\n",
    "To distribute a corpus, send a zip of the directory for others to extract or just share the directory as-is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3a11e",
   "metadata": {},
   "source": [
    "Below is an overview of the files in a Conc corpus directory. The data can be accessed via Conc or accessed directly from the files. \n",
    "\n",
    "| File | Access via Conc | Description |\n",
    "| -------- | ------- | ------- |\n",
    "| README.md | - | Human readable information about the corpus to aide distribution |\n",
    "| corpus.json | specific properties e.g. conc.token_count | Machine readable information about the corpus, including name, description, various summary statistics, and models used to build the corpus |\n",
    "| vocab.parquet | corpus.vocab | A table mapping token strings to token IDs and frequency information |\n",
    "| tokens.parquet | corpus.tokens | A table with indices based on token positions used to query the corpus with tokens represented by numeric IDs |\n",
    "| metadata.parquet | corpus.metadata | A table with metadata for each document (if there is any) |\n",
    "| spaces.parquet | corpus.spaces | A table to allow recovery of document spacing without the original texts |\n",
    "| puncts.parquet | corpus.puncts | A table with punctuation positions |\n",
    "\n",
    "Below is more information about each file. You can obviously work with a corpus using Conc, but you can work with the processed corpus [parquet](https://parquet.apache.org/docs/file-format/) and JSON files directly. Conc works with parquet files using the [Polars library](https://pola.rs/), but there are other libraries that support the format. Python provides native support for JSON, but there are more efficient libraries. Conc uses the [msgspec library](https://github.com/jcrist/msgspec) to read and write JSON. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a17c22",
   "metadata": {},
   "source": [
    "## Notes on specific Conc corpus files and data formats\n",
    "\n",
    "The following information will help you if you want to work with the corpus data/files directly.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6219163c",
   "metadata": {},
   "source": [
    "### README.md\n",
    "\n",
    "Below is an example of the README.md file generated by the Conc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4f1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div class=\"alert alert-block alert-success\">\n",
       "\n",
       "## Brown Corpus\n",
       "\n",
       "### About\n",
       "\n",
       "This directory contains a corpus created using the [Conc](https://github.com/polsci/conc) Python library. \n",
       "\n",
       "### Corpus Information\n",
       "\n",
       "A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.\n",
       "\n",
       "Date created: 2025-07-23 22:27:11  \n",
       "Document count: 500  \n",
       "Token count: 1138566  \n",
       "Word token count: 980144  \n",
       "Unique tokens: 42930  \n",
       "Unique word tokens: 42907  \n",
       "Conc Version Number: 0.1.10  \n",
       "spaCy model: en_core_web_sm, version 3.8.0  \n",
       "\n",
       "### Using this corpus\n",
       " \n",
       "Conc can be installed [via pip](https://pypi.org/project/conc/). The [Conc documentation site](https://geoffford.nz/conc) \n",
       "has tutorials and detailed information to get you started with Conc or to work with the corpus \n",
       "data directly.  \n",
       "\n",
       "### Cite Conc\n",
       "\n",
       "If you use Conc in your work, please cite it as follows: Ford, G. (2025). Conc: a Python library for efficient corpus analysis (Version 0.1.10) [Computer software]. https://doi.org/10.5281/zenodo.16358752\n",
       "\n",
       "\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "with open(f'{path_to_brown_corpus}/README.md', 'rb') as f:\n",
    "    markdown = '<div class=\"alert alert-block alert-success\">\\n\\n' + f.read().decode('utf-8') + '\\n'\n",
    "    markdown = markdown.replace('\\n#', '\\n##') # making headings smaller for display\n",
    "    markdown += '</div>'\n",
    "    display(Markdown(markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee41ffa",
   "metadata": {},
   "source": [
    "### corpus.json file\n",
    "\n",
    "Below is the schema of the `corpus.json` file showing metadata saved with a corpus. These are loaded by Conc as attributes using `Corpus.load` or are created when you build a corpus using `Corpus.build_from_files` or `Corpus.build_from_csv`. The schema used to validate the JSON data represents the names and types of the attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bcca47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': {'type': 'string'},\n",
       " 'description': {'type': 'string'},\n",
       " 'slug': {'type': 'string'},\n",
       " 'conc_version': {'type': 'string'},\n",
       " 'document_count': {'type': 'integer'},\n",
       " 'token_count': {'type': 'integer'},\n",
       " 'word_token_count': {'type': 'integer'},\n",
       " 'punct_token_count': {'type': 'integer'},\n",
       " 'space_token_count': {'type': 'integer'},\n",
       " 'unique_tokens': {'type': 'integer'},\n",
       " 'unique_word_tokens': {'type': 'integer'},\n",
       " 'date_created': {'type': 'string'},\n",
       " 'EOF_TOKEN': {'type': 'integer'},\n",
       " 'SPACY_EOF_TOKEN': {'type': 'integer'},\n",
       " 'SPACY_MODEL': {'type': 'string'},\n",
       " 'SPACY_MODEL_VERSION': {'type': 'string'},\n",
       " 'punct_tokens': {'type': 'array', 'items': {'type': 'integer'}},\n",
       " 'space_tokens': {'type': 'array', 'items': {'type': 'integer'}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "properties = msgspec.json.schema(CorpusMetadata)['$defs']['CorpusMetadata']['properties']\n",
    "display(properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84ccce",
   "metadata": {},
   "source": [
    "Once you have [built or loaded a corpus](https://geoffford.nz/conc/tutorials/recipes.html) you can access the attributes. For example ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8462f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown Corpus\n",
      "Word token count:  980144\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus().load(path_to_brown_corpus) # loading the Brown corpus\n",
    "print(corpus.name) # accessing the name of the corpus\n",
    "print('Word token count: ', corpus.word_token_count) # access word_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac9e2a",
   "metadata": {},
   "source": [
    "Some of these attributes are exposed by `Corpus` methods. For example ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de59ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>Attribute</th><th>Value</th></tr></thead><tbody><tr><td>&quot;Name&quot;</td><td>&quot;Brown Corpus&quot;</td></tr><tr><td>&quot;Description&quot;</td><td>&quot;A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version …</td></tr><tr><td>&quot;Date Created&quot;</td><td>&quot;2025-07-23 22:27:11&quot;</td></tr><tr><td>&quot;Conc Version&quot;</td><td>&quot;0.1.10&quot;</td></tr><tr><td>&quot;Corpus Path&quot;</td><td>&quot;/home/geoff/data/conc-test-corpora/brown.corpus&quot;</td></tr><tr><td>&quot;Document Count&quot;</td><td>&quot;500&quot;</td></tr><tr><td>&quot;Token Count&quot;</td><td>&quot;1,138,566&quot;</td></tr><tr><td>&quot;Word Token Count&quot;</td><td>&quot;980,144&quot;</td></tr><tr><td>&quot;Unique Tokens&quot;</td><td>&quot;42,930&quot;</td></tr><tr><td>&quot;Unique Word Tokens&quot;</td><td>&quot;42,907&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌────────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│ Attribute          ┆ Value                                                                                                                                                                                                                                              │\n",
       "╞════════════════════╪════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╡\n",
       "│ Name               ┆ Brown Corpus                                                                                                                                                                                                                                       │\n",
       "│ Description        ┆ A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 │\n",
       "│                    ┆ http://www.hit.uib.no/icame/brown/bcm.html. This version …                                                                                                                                                                                         │\n",
       "│ Date Created       ┆ 2025-07-23 22:27:11                                                                                                                                                                                                                                │\n",
       "│ Conc Version       ┆ 0.1.10                                                                                                                                                                                                                                             │\n",
       "│ Corpus Path        ┆ /home/geoff/data/conc-test-corpora/brown.corpus                                                                                                                                                                                                    │\n",
       "│ Document Count     ┆ 500                                                                                                                                                                                                                                                │\n",
       "│ Token Count        ┆ 1,138,566                                                                                                                                                                                                                                          │\n",
       "│ Word Token Count   ┆ 980,144                                                                                                                                                                                                                                            │\n",
       "│ Unique Tokens      ┆ 42,930                                                                                                                                                                                                                                             │\n",
       "│ Unique Word Tokens ┆ 42,907                                                                                                                                                                                                                                             │\n",
       "└────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "corpus.info() # Polars dataframe with summary metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c199320",
   "metadata": {},
   "source": [
    "### vocab.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6df38",
   "metadata": {},
   "source": [
    "The vocab parquet file contains ...\n",
    "\n",
    "1. A lookup between token_id, token (string representation), and tokens_sort_order. The sort order allows sorting tokens alphabetically directly from token ids.\n",
    "2. A frequency table, with counts for lower cased tokens and orthographic realisation of tokens as they appeared in the text.\n",
    "3. Information on the type of token (i.e. whether punctuation or space - or if neither of those, a \"word\" token)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb6c4d",
   "metadata": {},
   "source": [
    "If you have loaded a corpus in Conc, you can access the vocab parquet data as a Polars dataframe like this ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db0721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>rank</th><th>tokens_sort_order</th><th>token_id</th><th>token</th><th>frequency_lower</th><th>frequency_orth</th><th>is_punct</th><th>is_space</th></tr></thead><tbody><tr><td>1</td><td>50087</td><td>22848</td><td>&quot;the&quot;</td><td>63516</td><td>62473</td><td>false</td><td>false</td></tr><tr><td>2</td><td>28</td><td>8128</td><td>&quot;,&quot;</td><td>58331</td><td>58331</td><td>true</td><td>false</td></tr><tr><td>3</td><td>41</td><td>38309</td><td>&quot;.&quot;</td><td>49907</td><td>49907</td><td>true</td><td>false</td></tr><tr><td>4</td><td>35232</td><td>2739</td><td>&quot;of&quot;</td><td>36321</td><td>36122</td><td>false</td><td>false</td></tr><tr><td>5</td><td>3351</td><td>7126</td><td>&quot;and&quot;</td><td>27787</td><td>27633</td><td>false</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────┬───────────────────┬──────────┬───────┬─────────────────┬────────────────┬──────────┬──────────┐\n",
       "│ rank ┆ tokens_sort_order ┆ token_id ┆ token ┆ frequency_lower ┆ frequency_orth ┆ is_punct ┆ is_space │\n",
       "╞══════╪═══════════════════╪══════════╪═══════╪═════════════════╪════════════════╪══════════╪══════════╡\n",
       "│ 1    ┆ 50087             ┆ 22848    ┆ the   ┆ 63516           ┆ 62473          ┆ false    ┆ false    │\n",
       "│ 2    ┆ 28                ┆ 8128     ┆ ,     ┆ 58331           ┆ 58331          ┆ true     ┆ false    │\n",
       "│ 3    ┆ 41                ┆ 38309    ┆ .     ┆ 49907           ┆ 49907          ┆ true     ┆ false    │\n",
       "│ 4    ┆ 35232             ┆ 2739     ┆ of    ┆ 36321           ┆ 36122          ┆ false    ┆ false    │\n",
       "│ 5    ┆ 3351              ┆ 7126     ┆ and   ┆ 27787           ┆ 27633          ┆ false    ┆ false    │\n",
       "└──────┴───────────────────┴──────────┴───────┴─────────────────┴────────────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "# corpus.vocab is a Polars dataframe\n",
    "corpus.vocab.head(5).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf1dd7",
   "metadata": {},
   "source": [
    "You can also access the vocab data directly from the parquet file using Polars (or other libraries that support parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb32cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>rank</th><th>tokens_sort_order</th><th>token_id</th><th>token</th><th>frequency_lower</th><th>frequency_orth</th><th>is_punct</th><th>is_space</th></tr></thead><tbody><tr><td>1</td><td>50087</td><td>22848</td><td>&quot;the&quot;</td><td>63516</td><td>62473</td><td>false</td><td>false</td></tr><tr><td>2</td><td>28</td><td>8128</td><td>&quot;,&quot;</td><td>58331</td><td>58331</td><td>true</td><td>false</td></tr><tr><td>3</td><td>41</td><td>38309</td><td>&quot;.&quot;</td><td>49907</td><td>49907</td><td>true</td><td>false</td></tr><tr><td>4</td><td>35232</td><td>2739</td><td>&quot;of&quot;</td><td>36321</td><td>36122</td><td>false</td><td>false</td></tr><tr><td>5</td><td>3351</td><td>7126</td><td>&quot;and&quot;</td><td>27787</td><td>27633</td><td>false</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────┬───────────────────┬──────────┬───────┬─────────────────┬────────────────┬──────────┬──────────┐\n",
       "│ rank ┆ tokens_sort_order ┆ token_id ┆ token ┆ frequency_lower ┆ frequency_orth ┆ is_punct ┆ is_space │\n",
       "╞══════╪═══════════════════╪══════════╪═══════╪═════════════════╪════════════════╪══════════╪══════════╡\n",
       "│ 1    ┆ 50087             ┆ 22848    ┆ the   ┆ 63516           ┆ 62473          ┆ false    ┆ false    │\n",
       "│ 2    ┆ 28                ┆ 8128     ┆ ,     ┆ 58331           ┆ 58331          ┆ true     ┆ false    │\n",
       "│ 3    ┆ 41                ┆ 38309    ┆ .     ┆ 49907           ┆ 49907          ┆ true     ┆ false    │\n",
       "│ 4    ┆ 35232             ┆ 2739     ┆ of    ┆ 36321           ┆ 36122          ┆ false    ┆ false    │\n",
       "│ 5    ┆ 3351              ┆ 7126     ┆ and   ┆ 27787           ┆ 27633          ┆ false    ┆ false    │\n",
       "└──────┴───────────────────┴──────────┴───────┴─────────────────┴────────────────┴──────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "display(pl.scan_parquet(f'{path_to_brown_corpus}/vocab.parquet').head(5).collect(engine='streaming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b388d",
   "metadata": {},
   "source": [
    "To illustrate how frequencies are stored, see the instances for 'the'. The counts for the form as it appeared in the text are stored in frequency_orth. 'The' appears 1043 times and 'the' as lowercase appears 62,473 times. The frequency_lower column provides a count of the total number of mentions of 'the' regardless of case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6df9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>rank</th><th>tokens_sort_order</th><th>token_id</th><th>token</th><th>frequency_lower</th><th>frequency_orth</th><th>is_punct</th><th>is_space</th></tr></thead><tbody><tr><td>1</td><td>50087</td><td>22848</td><td>&quot;the&quot;</td><td>63516</td><td>62473</td><td>false</td><td>false</td></tr><tr><td>99</td><td>50086</td><td>15682</td><td>&quot;The&quot;</td><td>null</td><td>1043</td><td>false</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────┬───────────────────┬──────────┬───────┬─────────────────┬────────────────┬──────────┬──────────┐\n",
       "│ rank ┆ tokens_sort_order ┆ token_id ┆ token ┆ frequency_lower ┆ frequency_orth ┆ is_punct ┆ is_space │\n",
       "╞══════╪═══════════════════╪══════════╪═══════╪═════════════════╪════════════════╪══════════╪══════════╡\n",
       "│ 1    ┆ 50087             ┆ 22848    ┆ the   ┆ 63516           ┆ 62473          ┆ false    ┆ false    │\n",
       "│ 99   ┆ 50086             ┆ 15682    ┆ The   ┆ null            ┆ 1043           ┆ false    ┆ false    │\n",
       "└──────┴───────────────────┴──────────┴───────┴─────────────────┴────────────────┴──────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "display(pl.scan_parquet(f'{path_to_brown_corpus}/vocab.parquet').filter(pl.col('token').str.to_lowercase() == 'the').head(5).collect(engine='streaming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d978152b",
   "metadata": {},
   "source": [
    "Punctuation is included in the token table, but these tokens can be filtered in Conc reports. If you are working with the table directly you can use `is_punct` to access or remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eacaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>rank</th><th>tokens_sort_order</th><th>token_id</th><th>token</th><th>frequency_lower</th><th>frequency_orth</th><th>is_punct</th><th>is_space</th></tr></thead><tbody><tr><td>2</td><td>28</td><td>8128</td><td>&quot;,&quot;</td><td>58331</td><td>58331</td><td>true</td><td>false</td></tr><tr><td>3</td><td>41</td><td>38309</td><td>&quot;.&quot;</td><td>49907</td><td>49907</td><td>true</td><td>false</td></tr><tr><td>12</td><td>1577</td><td>1601</td><td>&quot;`&quot;</td><td>9788</td><td>9788</td><td>true</td><td>false</td></tr><tr><td>14</td><td>14</td><td>42833</td><td>&quot;&#x27;&#x27;&quot;</td><td>8762</td><td>8762</td><td>true</td><td>false</td></tr><tr><td>15</td><td>29</td><td>27963</td><td>&quot;-&quot;</td><td>8131</td><td>8131</td><td>true</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────┬───────────────────┬──────────┬───────┬─────────────────┬────────────────┬──────────┬──────────┐\n",
       "│ rank ┆ tokens_sort_order ┆ token_id ┆ token ┆ frequency_lower ┆ frequency_orth ┆ is_punct ┆ is_space │\n",
       "╞══════╪═══════════════════╪══════════╪═══════╪═════════════════╪════════════════╪══════════╪══════════╡\n",
       "│ 2    ┆ 28                ┆ 8128     ┆ ,     ┆ 58331           ┆ 58331          ┆ true     ┆ false    │\n",
       "│ 3    ┆ 41                ┆ 38309    ┆ .     ┆ 49907           ┆ 49907          ┆ true     ┆ false    │\n",
       "│ 12   ┆ 1577              ┆ 1601     ┆ `     ┆ 9788            ┆ 9788           ┆ true     ┆ false    │\n",
       "│ 14   ┆ 14                ┆ 42833    ┆ ''    ┆ 8762            ┆ 8762           ┆ true     ┆ false    │\n",
       "│ 15   ┆ 29                ┆ 27963    ┆ -     ┆ 8131            ┆ 8131           ┆ true     ┆ false    │\n",
       "└──────┴───────────────────┴──────────┴───────┴─────────────────┴────────────────┴──────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "display(pl.scan_parquet(f'{path_to_brown_corpus}/vocab.parquet').filter(pl.col('is_punct') == True).head(5).collect(engine='streaming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b9348",
   "metadata": {},
   "source": [
    "Conc also stores space tokens from spaCy's tokenisation process, which are sequences of whitespace characters. Space tokens are included (without counts). Space tokens are explained in more detail below the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3009bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>rank</th><th>tokens_sort_order</th><th>token_id</th><th>token</th><th>frequency_lower</th><th>frequency_orth</th><th>is_punct</th><th>is_space</th></tr></thead><tbody><tr><td>47165</td><td>1</td><td>2956</td><td>&quot;\n",
       "&quot;</td><td>null</td><td>null</td><td>false</td><td>true</td></tr><tr><td>47166</td><td>2</td><td>2799</td><td>&quot;\n",
       "\n",
       "&quot;</td><td>null</td><td>null</td><td>false</td><td>true</td></tr><tr><td>47167</td><td>3</td><td>27276</td><td>&quot;\n",
       "\n",
       "\t&quot;</td><td>null</td><td>null</td><td>false</td><td>true</td></tr><tr><td>47168</td><td>4</td><td>22812</td><td>&quot;\n",
       "\n",
       "\n",
       "&quot;</td><td>null</td><td>null</td><td>false</td><td>true</td></tr><tr><td>47169</td><td>5</td><td>4112</td><td>&quot;\n",
       "\n",
       "\n",
       "\t&quot;</td><td>null</td><td>null</td><td>false</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌───────┬───────────────────┬──────────┬───────┬─────────────────┬────────────────┬──────────┬──────────┐\n",
       "│ rank  ┆ tokens_sort_order ┆ token_id ┆ token ┆ frequency_lower ┆ frequency_orth ┆ is_punct ┆ is_space │\n",
       "╞═══════╪═══════════════════╪══════════╪═══════╪═════════════════╪════════════════╪══════════╪══════════╡\n",
       "│ 47165 ┆ 1                 ┆ 2956     ┆       ┆ null            ┆ null           ┆ false    ┆ true     │\n",
       "│       ┆                   ┆          ┆       ┆                 ┆                ┆          ┆          │\n",
       "│ 47166 ┆ 2                 ┆ 2799     ┆       ┆ null            ┆ null           ┆ false    ┆ true     │\n",
       "│       ┆                   ┆          ┆       ┆                 ┆                ┆          ┆          │\n",
       "│       ┆                   ┆          ┆       ┆                 ┆                ┆          ┆          │\n",
       "│ 47167 ┆ 3                 ┆ 27276    ┆       ┆ null            ┆ null           ┆ false    ┆ true     │\n",
       "│       ┆                   ┆          ┆       ┆                 ┆                ┆          ┆          │\n",
       "│       ┆                   ┆          ┆ \t     ┆                 ┆                ┆          ┆          │\n",
       "│ 47168 ┆ 4                 ┆ 22812    ┆       ┆ null            ┆ null           ┆ false    ┆ true     │\n",
       "│       ┆                   ┆          ┆       ┆                 ┆                ┆          ┆          │\n",
       "│       ┆                   ┆          ┆       ┆                 ┆                ┆          ┆          │\n",
       "│       ┆                   ┆          ┆       ┆                 ┆                ┆          ┆          │\n",
       "│ 47169 ┆ 5                 ┆ 4112     ┆       ┆ null            ┆ null           ┆ false    ┆ true     │\n",
       "│       ┆                   ┆          ┆       ┆                 ┆                ┆          ┆          │\n",
       "│       ┆                   ┆          ┆       ┆                 ┆                ┆          ┆          │\n",
       "│       ┆                   ┆          ┆ \t     ┆                 ┆                ┆          ┆          │\n",
       "└───────┴───────────────────┴──────────┴───────┴─────────────────┴────────────────┴──────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "display(pl.scan_parquet(f'{path_to_brown_corpus}/vocab.parquet').filter(pl.col('is_space') == True).head(5).collect(engine='streaming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff0366",
   "metadata": {},
   "source": [
    "#### A note about space tokens\n",
    "\n",
    "When SpaCy tokenises text it outputs whether each token is followed by a standard space character or not. Conc stores this information during build in the has_spaces column in the tokens table (see below). SpaCy also creates tokens for other whitespace sequences. Conc documentation refers to these as space tokens. Space tokens may be useful for some sequence classification problems, but more importantly for Conc - it allows re-representation of source document in their original form with newlines, tabs and sequences of whitespace preserved. \n",
    "\n",
    "Space tokens are not included in overall token counts stored in the corpus.json file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776487e",
   "metadata": {},
   "source": [
    "### tokens.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffce4d5",
   "metadata": {},
   "source": [
    "The `tokens.parquet` file contains a table representing tokens in the corpus. Whitespace tokens have been removed (see discussion of space tokens above). The tokens data can be directly accessed in Conc as a Polars dataframe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e53e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>position</th><th>orth_index</th><th>lower_index</th><th>token2doc_index</th><th>has_spaces</th></tr></thead><tbody><tr><td>99</td><td>46333</td><td>46333</td><td>-1</td><td>false</td></tr><tr><td>100</td><td>15682</td><td>22848</td><td>1</td><td>true</td></tr><tr><td>101</td><td>4361</td><td>41672</td><td>1</td><td>true</td></tr><tr><td>102</td><td>14610</td><td>29725</td><td>1</td><td>true</td></tr><tr><td>103</td><td>54713</td><td>49998</td><td>1</td><td>true</td></tr><tr><td>104</td><td>45742</td><td>19078</td><td>1</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────────┬────────────┬─────────────┬─────────────────┬────────────┐\n",
       "│ position ┆ orth_index ┆ lower_index ┆ token2doc_index ┆ has_spaces │\n",
       "╞══════════╪════════════╪═════════════╪═════════════════╪════════════╡\n",
       "│ 99       ┆ 46333      ┆ 46333       ┆ -1              ┆ false      │\n",
       "│ 100      ┆ 15682      ┆ 22848       ┆ 1               ┆ true       │\n",
       "│ 101      ┆ 4361       ┆ 41672       ┆ 1               ┆ true       │\n",
       "│ 102      ┆ 14610      ┆ 29725       ┆ 1               ┆ true       │\n",
       "│ 103      ┆ 54713      ┆ 49998       ┆ 1               ┆ true       │\n",
       "│ 104      ┆ 45742      ┆ 19078       ┆ 1               ┆ true       │\n",
       "└──────────┴────────────┴─────────────┴─────────────────┴────────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "# corpus.tokens is a Polars dataframe\n",
    "corpus.tokens.with_row_index('position').filter(pl.col('position').is_between(99, 104)).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a6ae7",
   "metadata": {},
   "source": [
    "You can also access the tokens data directly from the parquet file using Polars (or other libraries that support parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cd68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>position</th><th>orth_index</th><th>lower_index</th><th>token2doc_index</th><th>has_spaces</th></tr></thead><tbody><tr><td>99</td><td>46333</td><td>46333</td><td>-1</td><td>false</td></tr><tr><td>100</td><td>15682</td><td>22848</td><td>1</td><td>true</td></tr><tr><td>101</td><td>4361</td><td>41672</td><td>1</td><td>true</td></tr><tr><td>102</td><td>14610</td><td>29725</td><td>1</td><td>true</td></tr><tr><td>103</td><td>54713</td><td>49998</td><td>1</td><td>true</td></tr><tr><td>104</td><td>45742</td><td>19078</td><td>1</td><td>true</td></tr><tr><td>105</td><td>53250</td><td>53250</td><td>1</td><td>true</td></tr><tr><td>106</td><td>8699</td><td>35796</td><td>1</td><td>true</td></tr><tr><td>107</td><td>45680</td><td>45680</td><td>1</td><td>true</td></tr><tr><td>108</td><td>30305</td><td>30305</td><td>1</td><td>true</td></tr><tr><td>109</td><td>2739</td><td>2739</td><td>1</td><td>true</td></tr><tr><td>110</td><td>38486</td><td>35571</td><td>1</td><td>false</td></tr><tr><td>111</td><td>49732</td><td>49732</td><td>1</td><td>true</td></tr><tr><td>112</td><td>42720</td><td>42720</td><td>1</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────────┬────────────┬─────────────┬─────────────────┬────────────┐\n",
       "│ position ┆ orth_index ┆ lower_index ┆ token2doc_index ┆ has_spaces │\n",
       "╞══════════╪════════════╪═════════════╪═════════════════╪════════════╡\n",
       "│ 99       ┆ 46333      ┆ 46333       ┆ -1              ┆ false      │\n",
       "│ 100      ┆ 15682      ┆ 22848       ┆ 1               ┆ true       │\n",
       "│ 101      ┆ 4361       ┆ 41672       ┆ 1               ┆ true       │\n",
       "│ 102      ┆ 14610      ┆ 29725       ┆ 1               ┆ true       │\n",
       "│ 103      ┆ 54713      ┆ 49998       ┆ 1               ┆ true       │\n",
       "│ 104      ┆ 45742      ┆ 19078       ┆ 1               ┆ true       │\n",
       "│ 105      ┆ 53250      ┆ 53250       ┆ 1               ┆ true       │\n",
       "│ 106      ┆ 8699       ┆ 35796       ┆ 1               ┆ true       │\n",
       "│ 107      ┆ 45680      ┆ 45680       ┆ 1               ┆ true       │\n",
       "│ 108      ┆ 30305      ┆ 30305       ┆ 1               ┆ true       │\n",
       "│ 109      ┆ 2739       ┆ 2739        ┆ 1               ┆ true       │\n",
       "│ 110      ┆ 38486      ┆ 35571       ┆ 1               ┆ false      │\n",
       "│ 111      ┆ 49732      ┆ 49732       ┆ 1               ┆ true       │\n",
       "│ 112      ┆ 42720      ┆ 42720       ┆ 1               ┆ true       │\n",
       "└──────────┴────────────┴─────────────┴─────────────────┴────────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "pl.scan_parquet(f'{path_to_brown_corpus}/tokens.parquet').with_row_index('position').filter(pl.col('position').is_between(99, 112)).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e34eb",
   "metadata": {},
   "source": [
    "The columns are as follows:\n",
    "\n",
    "- the `orth_index` column stores the token_id of the original form of the token\n",
    "- the `lower_index` column stores the token_id of the lowercased form of the token  \n",
    "- the `token2doc_index` column stores the document id assigned by Conc in the order the texts where processed (starting from index position 1) - this is the same order as the metadata in the `metadata.parquet` file.\n",
    "- the `has_spaces` column stores a boolean value indicating if the token is followed by a standard space character or not.\n",
    "\n",
    "To demarcate the start and end of documents, Conc uses an end of file token (EOF_TOKEN) in the orth_index and lower_index columns. The EOF_TOKEN is stored in `corpus.json` and is accessible as an attribute of a `Corpus` object.\n",
    "\n",
    "The token2doc_index represents token positions outside texts in the corpus as -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5140a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# tmp_tokens_df = pl.scan_parquet(f'{path_to_brown_corpus}/tokens.parquet').with_row_index('position').collect()\n",
    "# display(tmp_tokens_df.filter(pl.col('position').is_between(99, 121)).head(5))\n",
    "# display(tmp_tokens_df.filter(pl.col('position').is_between(343, 348)).head(5))\n",
    "\n",
    "# space_tokens_df = tmp_tokens_df.filter((pl.col('orth_index').is_in(brown.space_tokens))).with_row_index('adjust_by').with_columns((pl.col('position') - pl.col('adjust_by')).alias('corrected'))\n",
    "# # replace position with corrected \n",
    "# space_tokens_df = space_tokens_df.with_columns(pl.col('corrected').alias('position')).drop('adjust_by').drop('corrected')\n",
    "# # remove space_tokens_df from tmp_tokens_df\n",
    "# tmp_tokens_df = tmp_tokens_df.filter(~pl.col('orth_index').is_in(brown.space_tokens)).drop('position').with_row_index('position')\n",
    "\n",
    "# tmp_tokens_df = tmp_tokens_df.with_columns(pl.lit(1).alias('not_space'))\n",
    "# space_tokens_df = space_tokens_df.with_columns(pl.lit(0).alias('not_space'))\n",
    "\n",
    "# display(tmp_tokens_df.filter(pl.col('position').is_between(99, 121)).head(5))\n",
    "# display(tmp_tokens_df.filter(pl.col('position').is_between(343, 348)).head(5))\n",
    "\n",
    "# display(space_tokens_df.head(5))\n",
    "\n",
    "# reconstructed_df = pl.concat([tmp_tokens_df, space_tokens_df]).sort('position', 'not_space').drop('position').drop('not_space').with_row_index('position')\n",
    "# display(reconstructed_df.filter(pl.col('position').is_between(99, 121)).head(5))\n",
    "# display(reconstructed_df.filter(pl.col('position').is_between(343, 348)).head(5))\n",
    "\n",
    "# # is resconstructed identical to original? programmatically check\n",
    "# assert pl.scan_parquet(f'{path_to_brown_corpus}/tokens.parquet').collect(engine='streaming').with_row_index('position').equals(reconstructed_df)\n",
    "\n",
    "# # test for just one doc ...\n",
    "# doc_id = 300\n",
    "# tmp_doc_df = tmp_tokens_df.filter(pl.col('token2doc_index') == doc_id)\n",
    "# display(tmp_doc_df.head(5))\n",
    "# reconstructed_df = pl.concat([tmp_doc_df, space_tokens_df.filter(pl.col('token2doc_index') == doc_id)]).sort('position', 'not_space').drop('position').drop('not_space')\n",
    "# display(reconstructed_df.head(5))\n",
    "# assert pl.scan_parquet(f'{path_to_brown_corpus}/tokens.parquet').filter(pl.col('token2doc_index') == doc_id).collect(engine='streaming').equals(reconstructed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb1b1a",
   "metadata": {},
   "source": [
    "The view below shows tokens joined with vocab, so you can see the token string and the attributes of the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d4576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>position</th><th>orth_index</th><th>lower_index</th><th>token2doc_index</th><th>has_spaces</th><th>token</th><th>is_punct</th><th>is_space</th></tr></thead><tbody><tr><td>99</td><td>46333</td><td>46333</td><td>-1</td><td>false</td><td>&quot; conc-end-of-file-token&quot;</td><td>false</td><td>false</td></tr><tr><td>100</td><td>15682</td><td>22848</td><td>1</td><td>true</td><td>&quot;The&quot;</td><td>false</td><td>false</td></tr><tr><td>101</td><td>4361</td><td>41672</td><td>1</td><td>true</td><td>&quot;Fulton&quot;</td><td>false</td><td>false</td></tr><tr><td>102</td><td>14610</td><td>29725</td><td>1</td><td>true</td><td>&quot;County&quot;</td><td>false</td><td>false</td></tr><tr><td>103</td><td>54713</td><td>49998</td><td>1</td><td>true</td><td>&quot;Grand&quot;</td><td>false</td><td>false</td></tr><tr><td>104</td><td>45742</td><td>19078</td><td>1</td><td>true</td><td>&quot;Jury&quot;</td><td>false</td><td>false</td></tr><tr><td>105</td><td>53250</td><td>53250</td><td>1</td><td>true</td><td>&quot;said&quot;</td><td>false</td><td>false</td></tr><tr><td>106</td><td>8699</td><td>35796</td><td>1</td><td>true</td><td>&quot;Friday&quot;</td><td>false</td><td>false</td></tr><tr><td>107</td><td>45680</td><td>45680</td><td>1</td><td>true</td><td>&quot;an&quot;</td><td>false</td><td>false</td></tr><tr><td>108</td><td>30305</td><td>30305</td><td>1</td><td>true</td><td>&quot;investigation&quot;</td><td>false</td><td>false</td></tr><tr><td>109</td><td>2739</td><td>2739</td><td>1</td><td>true</td><td>&quot;of&quot;</td><td>false</td><td>false</td></tr><tr><td>110</td><td>38486</td><td>35571</td><td>1</td><td>false</td><td>&quot;Atlanta&quot;</td><td>false</td><td>false</td></tr><tr><td>111</td><td>49732</td><td>49732</td><td>1</td><td>true</td><td>&quot;&#x27;s&quot;</td><td>false</td><td>false</td></tr><tr><td>112</td><td>42720</td><td>42720</td><td>1</td><td>true</td><td>&quot;recent&quot;</td><td>false</td><td>false</td></tr><tr><td>113</td><td>12294</td><td>12294</td><td>1</td><td>true</td><td>&quot;primary&quot;</td><td>false</td><td>false</td></tr><tr><td>114</td><td>29461</td><td>29461</td><td>1</td><td>true</td><td>&quot;election&quot;</td><td>false</td><td>false</td></tr><tr><td>115</td><td>42473</td><td>42473</td><td>1</td><td>true</td><td>&quot;produced&quot;</td><td>false</td><td>false</td></tr><tr><td>116</td><td>1601</td><td>1601</td><td>1</td><td>false</td><td>&quot;`&quot;</td><td>true</td><td>false</td></tr><tr><td>117</td><td>1601</td><td>1601</td><td>1</td><td>true</td><td>&quot;`&quot;</td><td>true</td><td>false</td></tr><tr><td>118</td><td>39507</td><td>39507</td><td>1</td><td>true</td><td>&quot;no&quot;</td><td>false</td><td>false</td></tr><tr><td>119</td><td>9335</td><td>9335</td><td>1</td><td>true</td><td>&quot;evidence&quot;</td><td>false</td><td>false</td></tr><tr><td>120</td><td>42833</td><td>42833</td><td>1</td><td>true</td><td>&quot;&#x27;&#x27;&quot;</td><td>true</td><td>false</td></tr><tr><td>121</td><td>13607</td><td>13607</td><td>1</td><td>true</td><td>&quot;that&quot;</td><td>false</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────────┬────────────┬─────────────┬─────────────────┬────────────┬─────────────────────────┬──────────┬──────────┐\n",
       "│ position ┆ orth_index ┆ lower_index ┆ token2doc_index ┆ has_spaces ┆ token                   ┆ is_punct ┆ is_space │\n",
       "╞══════════╪════════════╪═════════════╪═════════════════╪════════════╪═════════════════════════╪══════════╪══════════╡\n",
       "│ 99       ┆ 46333      ┆ 46333       ┆ -1              ┆ false      ┆  conc-end-of-file-token ┆ false    ┆ false    │\n",
       "│ 100      ┆ 15682      ┆ 22848       ┆ 1               ┆ true       ┆ The                     ┆ false    ┆ false    │\n",
       "│ 101      ┆ 4361       ┆ 41672       ┆ 1               ┆ true       ┆ Fulton                  ┆ false    ┆ false    │\n",
       "│ 102      ┆ 14610      ┆ 29725       ┆ 1               ┆ true       ┆ County                  ┆ false    ┆ false    │\n",
       "│ 103      ┆ 54713      ┆ 49998       ┆ 1               ┆ true       ┆ Grand                   ┆ false    ┆ false    │\n",
       "│ 104      ┆ 45742      ┆ 19078       ┆ 1               ┆ true       ┆ Jury                    ┆ false    ┆ false    │\n",
       "│ 105      ┆ 53250      ┆ 53250       ┆ 1               ┆ true       ┆ said                    ┆ false    ┆ false    │\n",
       "│ 106      ┆ 8699       ┆ 35796       ┆ 1               ┆ true       ┆ Friday                  ┆ false    ┆ false    │\n",
       "│ 107      ┆ 45680      ┆ 45680       ┆ 1               ┆ true       ┆ an                      ┆ false    ┆ false    │\n",
       "│ 108      ┆ 30305      ┆ 30305       ┆ 1               ┆ true       ┆ investigation           ┆ false    ┆ false    │\n",
       "│ 109      ┆ 2739       ┆ 2739        ┆ 1               ┆ true       ┆ of                      ┆ false    ┆ false    │\n",
       "│ 110      ┆ 38486      ┆ 35571       ┆ 1               ┆ false      ┆ Atlanta                 ┆ false    ┆ false    │\n",
       "│ 111      ┆ 49732      ┆ 49732       ┆ 1               ┆ true       ┆ 's                      ┆ false    ┆ false    │\n",
       "│ 112      ┆ 42720      ┆ 42720       ┆ 1               ┆ true       ┆ recent                  ┆ false    ┆ false    │\n",
       "│ 113      ┆ 12294      ┆ 12294       ┆ 1               ┆ true       ┆ primary                 ┆ false    ┆ false    │\n",
       "│ 114      ┆ 29461      ┆ 29461       ┆ 1               ┆ true       ┆ election                ┆ false    ┆ false    │\n",
       "│ 115      ┆ 42473      ┆ 42473       ┆ 1               ┆ true       ┆ produced                ┆ false    ┆ false    │\n",
       "│ 116      ┆ 1601       ┆ 1601        ┆ 1               ┆ false      ┆ `                       ┆ true     ┆ false    │\n",
       "│ 117      ┆ 1601       ┆ 1601        ┆ 1               ┆ true       ┆ `                       ┆ true     ┆ false    │\n",
       "│ 118      ┆ 39507      ┆ 39507       ┆ 1               ┆ true       ┆ no                      ┆ false    ┆ false    │\n",
       "│ 119      ┆ 9335       ┆ 9335        ┆ 1               ┆ true       ┆ evidence                ┆ false    ┆ false    │\n",
       "│ 120      ┆ 42833      ┆ 42833       ┆ 1               ┆ true       ┆ ''                      ┆ true     ┆ false    │\n",
       "│ 121      ┆ 13607      ┆ 13607       ┆ 1               ┆ true       ┆ that                    ┆ false    ┆ false    │\n",
       "└──────────┴────────────┴─────────────┴─────────────────┴────────────┴─────────────────────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "pl.scan_parquet(f'{path_to_brown_corpus}/tokens.parquet').with_row_index('position').filter(pl.col('position').is_between(99, 121)).join(\n",
    "    pl.scan_parquet(f'{path_to_brown_corpus}/vocab.parquet').select(pl.col('token_id'), pl.col('token'), pl.col('is_punct'), pl.col('is_space')),\n",
    "    left_on='orth_index', right_on='token_id', how='left', maintain_order='left').collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f451875",
   "metadata": {},
   "source": [
    "### spaces.parquet\n",
    "\n",
    "Space tokens (see note above) are stored in `spaces.parquet`. Space tokens are stored separate from word and punctuation tokens. This allows consistency with most tools for corpus linguistics. The spaces table follows the format of the tokens table. Spaces are represented using `position` and the corresponding token_id of the whitespace. The original token sequences can be reconstructed from the `tokens.parquet` and `spaces.parquet` files. Conc has functionality to recover specific texts by recombining the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e580910",
   "metadata": {},
   "source": [
    "From Conc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4fe92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>position</th><th>orth_index</th><th>lower_index</th><th>token2doc_index</th><th>has_spaces</th></tr></thead><tbody><tr><td>100</td><td>27276</td><td>27276</td><td>1</td><td>false</td></tr><tr><td>345</td><td>2956</td><td>2956</td><td>1</td><td>false</td></tr><tr><td>637</td><td>2956</td><td>2956</td><td>1</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────────┬────────────┬─────────────┬─────────────────┬────────────┐\n",
       "│ position ┆ orth_index ┆ lower_index ┆ token2doc_index ┆ has_spaces │\n",
       "╞══════════╪════════════╪═════════════╪═════════════════╪════════════╡\n",
       "│ 100      ┆ 27276      ┆ 27276       ┆ 1               ┆ false      │\n",
       "│ 345      ┆ 2956       ┆ 2956        ┆ 1               ┆ false      │\n",
       "│ 637      ┆ 2956       ┆ 2956        ┆ 1               ┆ false      │\n",
       "└──────────┴────────────┴─────────────┴─────────────────┴────────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "# corpus.spaces is a Polars dataframe\n",
    "corpus.spaces.head(3).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7644f9",
   "metadata": {},
   "source": [
    "Directly accessing the parquet file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d22e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>position</th><th>orth_index</th><th>lower_index</th><th>token2doc_index</th><th>has_spaces</th></tr></thead><tbody><tr><td>100</td><td>27276</td><td>27276</td><td>1</td><td>false</td></tr><tr><td>345</td><td>2956</td><td>2956</td><td>1</td><td>false</td></tr><tr><td>637</td><td>2956</td><td>2956</td><td>1</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────────┬────────────┬─────────────┬─────────────────┬────────────┐\n",
       "│ position ┆ orth_index ┆ lower_index ┆ token2doc_index ┆ has_spaces │\n",
       "╞══════════╪════════════╪═════════════╪═════════════════╪════════════╡\n",
       "│ 100      ┆ 27276      ┆ 27276       ┆ 1               ┆ false      │\n",
       "│ 345      ┆ 2956       ┆ 2956        ┆ 1               ┆ false      │\n",
       "│ 637      ┆ 2956       ┆ 2956        ┆ 1               ┆ false      │\n",
       "└──────────┴────────────┴─────────────┴─────────────────┴────────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "pl.scan_parquet(f'{path_to_brown_corpus}/spaces.parquet').head(3).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644b340",
   "metadata": {},
   "source": [
    "### puncts.parquet\n",
    "\n",
    "The `puncts.parquet` file stores an index of the position of punctuation tokens in the corpus. Below are the first three rows of a `puncts.parquet` file. If you look above the positions align with punctuation tokens in the tokens.parquet file. These are intended to be used for filtering tokens to exclude punctuation where necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29777a",
   "metadata": {},
   "source": [
    "From Conc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732f429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>position</th></tr></thead><tbody><tr><td>116</td></tr><tr><td>117</td></tr><tr><td>120</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────────┐\n",
       "│ position │\n",
       "╞══════════╡\n",
       "│ 116      │\n",
       "│ 117      │\n",
       "│ 120      │\n",
       "└──────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "# corpus.puncts is a Polars dataframe\n",
    "corpus.puncts.head(3).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d7946",
   "metadata": {},
   "source": [
    "Directly from the parquet file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9cfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>position</th></tr></thead><tbody><tr><td>116</td></tr><tr><td>117</td></tr><tr><td>120</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌──────────┐\n",
       "│ position │\n",
       "╞══════════╡\n",
       "│ 116      │\n",
       "│ 117      │\n",
       "│ 120      │\n",
       "└──────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "pl.scan_parquet(f'{path_to_brown_corpus}/puncts.parquet').head(3).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8fcfc1",
   "metadata": {},
   "source": [
    "### metadata.parquet\n",
    "\n",
    "The `metadata.parquet` should not be confused with the metadata of the corpus itself, which is accessible via `corpus.json`.\n",
    "\n",
    "If populated, the `metadata.parquet` file contains metadata for each document in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974cd63",
   "metadata": {},
   "source": [
    "From Conc you can access the metadata dataframe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d92a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>speech_id</th><th>date</th><th>speaker</th><th>chamber</th><th>state</th></tr></thead><tbody><tr><td>530182158</td><td>&quot;1895-01-10T00:00:00.000000&quot;</td><td>&quot;Mr. COCKRELL&quot;</td><td>&quot;S&quot;</td><td>&quot;Unknown&quot;</td></tr><tr><td>890274849</td><td>&quot;1966-08-31T00:00:00.000000&quot;</td><td>&quot;Mr. LONG of Louisiana&quot;</td><td>&quot;S&quot;</td><td>&quot;Louisiana&quot;</td></tr><tr><td>880088363</td><td>&quot;1963-09-11T00:00:00.000000&quot;</td><td>&quot;Mr. FULBRIGHT&quot;</td><td>&quot;S&quot;</td><td>&quot;Unknown&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌───────────┬────────────────────────────┬───────────────────────┬─────────┬───────────┐\n",
       "│ speech_id ┆ date                       ┆ speaker               ┆ chamber ┆ state     │\n",
       "╞═══════════╪════════════════════════════╪═══════════════════════╪═════════╪═══════════╡\n",
       "│ 530182158 ┆ 1895-01-10T00:00:00.000000 ┆ Mr. COCKRELL          ┆ S       ┆ Unknown   │\n",
       "│ 890274849 ┆ 1966-08-31T00:00:00.000000 ┆ Mr. LONG of Louisiana ┆ S       ┆ Louisiana │\n",
       "│ 880088363 ┆ 1963-09-11T00:00:00.000000 ┆ Mr. FULBRIGHT         ┆ S       ┆ Unknown   │\n",
       "└───────────┴────────────────────────────┴───────────────────────┴─────────┴───────────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "corpus = Corpus().load(path_to_congress_corpus) # loading a corpus with some metadata!\n",
    "corpus.metadata.head(3).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362a070",
   "metadata": {},
   "source": [
    "Directly from the parquet file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c7aa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>speech_id</th><th>date</th><th>speaker</th><th>chamber</th><th>state</th></tr></thead><tbody><tr><td>530182158</td><td>&quot;1895-01-10T00:00:00.000000&quot;</td><td>&quot;Mr. COCKRELL&quot;</td><td>&quot;S&quot;</td><td>&quot;Unknown&quot;</td></tr><tr><td>890274849</td><td>&quot;1966-08-31T00:00:00.000000&quot;</td><td>&quot;Mr. LONG of Louisiana&quot;</td><td>&quot;S&quot;</td><td>&quot;Louisiana&quot;</td></tr><tr><td>880088363</td><td>&quot;1963-09-11T00:00:00.000000&quot;</td><td>&quot;Mr. FULBRIGHT&quot;</td><td>&quot;S&quot;</td><td>&quot;Unknown&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌───────────┬────────────────────────────┬───────────────────────┬─────────┬───────────┐\n",
       "│ speech_id ┆ date                       ┆ speaker               ┆ chamber ┆ state     │\n",
       "╞═══════════╪════════════════════════════╪═══════════════════════╪═════════╪═══════════╡\n",
       "│ 530182158 ┆ 1895-01-10T00:00:00.000000 ┆ Mr. COCKRELL          ┆ S       ┆ Unknown   │\n",
       "│ 890274849 ┆ 1966-08-31T00:00:00.000000 ┆ Mr. LONG of Louisiana ┆ S       ┆ Louisiana │\n",
       "│ 880088363 ┆ 1963-09-11T00:00:00.000000 ┆ Mr. FULBRIGHT         ┆ S       ┆ Unknown   │\n",
       "└───────────┴────────────────────────────┴───────────────────────┴─────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "display(pl.scan_parquet(f'{path_to_congress_corpus}/metadata.parquet').head(3).collect(engine='streaming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f054f81",
   "metadata": {},
   "source": [
    "Metadata is represented in the same order as documents are stored in the `tokens.parquet` file. The tokens with token2doc_index 1 correspond to the first metadata row.\n",
    "\n",
    "For corpora created from files using the `Corpus.build_from_files` method, there will always be a field for the source file at the time of creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0fb134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>file</th></tr></thead><tbody><tr><td>&quot;an-ideal-family.txt&quot;</td></tr><tr><td>&quot;at-the-bay.txt&quot;</td></tr><tr><td>&quot;bank-holiday.txt&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "┌─────────────────────┐\n",
       "│ file                │\n",
       "╞═════════════════════╡\n",
       "│ an-ideal-family.txt │\n",
       "│ at-the-bay.txt      │\n",
       "│ bank-holiday.txt    │\n",
       "└─────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "corpus = Corpus().load(path_to_gardenparty_corpus)\n",
    "display(pl.scan_parquet(f'{corpus.corpus_path}/metadata.parquet').head(3).collect(engine='streaming'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfafa0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
