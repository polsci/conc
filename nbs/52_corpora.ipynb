{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpora\n",
    "\n",
    "> Functions to work with multiple corpora and download and build sample corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import os\n",
    "from great_tables import GT\n",
    "import polars as pl\n",
    "import msgspec\n",
    "from fastcore.script import call_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc.core import CorpusMetadata, logger\n",
    "from conc.corpus import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = f'{os.environ.get(\"HOME\")}/data/'\n",
    "save_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "polars_conf = pl.Config.set_tbl_width_chars(300)\n",
    "polars_conf = pl.Config.set_fmt_str_lengths(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def list_corpora(\n",
    "\t\tpath: str # path to load corpus\n",
    "\t\t) -> pl.DataFrame: # Dataframe with path, corpus, corpus name, document count, token count\n",
    "\t\"\"\" Scan a directory for available corpora \"\"\"\n",
    "\t\n",
    "\tavailable_corpora = {'corpus': [], 'name': [], 'date_created': [], 'document_count': [], 'token_count': []}\n",
    "\tfor dir in os.listdir(path):\n",
    "\t\tif os.path.isdir(os.path.join(path, dir)) and os.path.isfile( os.path.join(path, dir, 'corpus.json')):\n",
    "\t\t\twith open(os.path.join(path, dir, 'corpus.json'), 'rb') as f:\n",
    "\t\t\t\tdata = msgspec.json.decode(f.read(), type=CorpusMetadata)\n",
    "\n",
    "\t\t\tavailable_corpora['corpus'].append(dir)\n",
    "\t\t\tfor k in ['name', 'document_count', 'token_count', 'date_created']:\n",
    "\t\t\t\tattr = getattr(data, k)\n",
    "\t\t\t\tif isinstance(attr, int):\n",
    "\t\t\t\t\tattr = f'{attr:,}'\n",
    "\t\t\t\tavailable_corpora[k].append(attr)\n",
    "\n",
    "\treturn pl.DataFrame(available_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────────────────┬───────────────────────────────────────┬─────────────────────┬────────────────┬─────────────┐\n",
      "│ corpus                                       ┆ name                                  ┆ date_created        ┆ document_count ┆ token_count │\n",
      "╞══════════════════════════════════════════════╪═══════════════════════════════════════╪═════════════════════╪════════════════╪═════════════╡\n",
      "│ introduce-yourself.corpus                    ┆ Introduce Yourself                    ┆ 2025-06-10 09:16:41 ┆ 28             ┆ 9,913       │\n",
      "│ gutenberg.corpus                             ┆ Gutenberg Corpus                      ┆ 2025-06-09 12:44:36 ┆ 18             ┆ 2,546,286   │\n",
      "│ us-congressional-speeches-subset-200k.corpus ┆ US Congressional Speeches Subset 200k ┆ 2025-06-09 15:04:47 ┆ 200,000        ┆ 39,963,039  │\n",
      "│ us-congressional-speeches-subset-100k.corpus ┆ US Congressional Speeches Subset 100k ┆ 2025-06-09 15:03:44 ┆ 100,000        ┆ 19,927,241  │\n",
      "│ garden-party.corpus                          ┆ Garden Party Corpus                   ┆ 2025-06-09 12:44:36 ┆ 15             ┆ 74,664      │\n",
      "│ us-congressional-speeches-subset-10k.corpus  ┆ US Congressional Speeches Subset 10k  ┆ 2025-06-09 15:03:14 ┆ 10,000         ┆ 1,954,972   │\n",
      "│ us-congressional-speeches-subset-500k.corpus ┆ US Congressional Speeches Subset 500k ┆ 2025-06-09 15:07:14 ┆ 500,000        ┆ 99,902,593  │\n",
      "│ brown.corpus                                 ┆ Brown Corpus                          ┆ 2025-06-09 15:46:25 ┆ 500            ┆ 1,138,566   │\n",
      "│ toy.corpus                                   ┆ Toy Corpus                            ┆ 2025-06-09 12:44:20 ┆ 6              ┆ 38          │\n",
      "│ reuters.corpus                               ┆ Reuters Corpus                        ┆ 2025-06-09 12:44:27 ┆ 10,788         ┆ 1,552,919   │\n",
      "└──────────────────────────────────────────────┴───────────────────────────────────────┴─────────────────────┴────────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(list_corpora(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "toy_data = []\n",
    "toy_data.append(['1.txt', 'The cat sat on the mat.', 'feline', 'cat'])\n",
    "toy_data.append(['2.txt', 'The dog sat on the mat.', 'canine', 'dog'])\n",
    "toy_data.append(['3.txt', 'The cat is meowing.', 'feline', 'cat'])\n",
    "toy_data.append(['4.txt', 'The dog is barking.', 'canine', 'dog'])\n",
    "toy_data.append(['5.txt', 'The cat is climbing a tree.', 'feline', 'cat'])\n",
    "toy_data.append(['6.txt', 'The dog is digging a hole.', 'canine', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "32\n",
      "15\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# checking on counts above\n",
    "toy_data_test = [doc[1] for doc in toy_data] \n",
    "toy_data_test = [re.findall(r'\\b\\w+\\b|[^\\w\\s]', text) for text in toy_data_test]\n",
    "toy_data_test = [token.lower() for sublist in toy_data_test for token in sublist if token.strip()]\n",
    "#print(toy_data_test)\n",
    "# token count\n",
    "print(len(toy_data_test)) # should be 38\n",
    "# word token count\n",
    "print(len(toy_data_test) - sum([1 for token in toy_data_test if token == '.'])) # should be 32\n",
    "toy_data_test_unique = set(toy_data_test)\n",
    "# unique tokens\n",
    "print(len(toy_data_test_unique))\n",
    "toy_data_test_unique_word = set([token for token in toy_data_test_unique if token != '.'])\n",
    "# unique word tokens\n",
    "print(len(toy_data_test_unique_word))\n",
    "\n",
    "# based on this - toy corpus should have ... \n",
    "# document_count = 6\n",
    "# token_count = 38\n",
    "# word_token_count = 32\n",
    "# unique_tokens = 15\n",
    "# unique_word_tokens = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_toy_corpus_sources(source_path:str # path to location of sources for building corpora\n",
    "\t\t\t\t\t\t\t ):\n",
    "\t\"\"\" Create txt files and csv to test build of toy corpus. \"\"\"\n",
    "\n",
    "\ttoy_path = os.path.join(source_path, 'toy')\n",
    "\tif not os.path.exists(toy_path):\n",
    "\t\tos.makedirs(toy_path, exist_ok=True)\n",
    "\tfor row in toy_data:\n",
    "\t\twith open(f'{source_path}/toy/{row[0]}', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(row[1])\n",
    "\tdf = pl.DataFrame(toy_data, orient='row', schema=(('source', str), ('text', str), ('category', str), ('species', str)))\n",
    "\tdf.write_csv(os.path.join(source_path, 'toy.csv'))\n",
    "\tdf.write_csv(os.path.join(source_path, 'toy.csv.gz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "create_toy_corpus_sources(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_toy_corpus(\n",
    "        csv_path:str # path to location of csv for building corpora\n",
    "        ) -> GT: \n",
    "    \"\"\" Show toy corpus in a table. \"\"\"\n",
    "    \n",
    "    toy_corpus_df = pl.read_csv(csv_path)\n",
    "    GT(toy_corpus_df).tab_options(table_margin_left = 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"tfvftjhwav\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#tfvftjhwav table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#tfvftjhwav thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#tfvftjhwav p { margin: 0; padding: 0; }\n",
       " #tfvftjhwav .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #tfvftjhwav .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #tfvftjhwav .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #tfvftjhwav .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #tfvftjhwav .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #tfvftjhwav .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #tfvftjhwav .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #tfvftjhwav .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #tfvftjhwav .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #tfvftjhwav .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #tfvftjhwav .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #tfvftjhwav .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #tfvftjhwav .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #tfvftjhwav .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #tfvftjhwav .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #tfvftjhwav .gt_from_md> :first-child { margin-top: 0; }\n",
       " #tfvftjhwav .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #tfvftjhwav .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #tfvftjhwav .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #tfvftjhwav .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #tfvftjhwav .gt_row_group_first td { border-top-width: 2px; }\n",
       " #tfvftjhwav .gt_row_group_first th { border-top-width: 2px; }\n",
       " #tfvftjhwav .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #tfvftjhwav .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #tfvftjhwav .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #tfvftjhwav .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #tfvftjhwav .gt_left { text-align: left; }\n",
       " #tfvftjhwav .gt_center { text-align: center; }\n",
       " #tfvftjhwav .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #tfvftjhwav .gt_font_normal { font-weight: normal; }\n",
       " #tfvftjhwav .gt_font_bold { font-weight: bold; }\n",
       " #tfvftjhwav .gt_font_italic { font-style: italic; }\n",
       " #tfvftjhwav .gt_super { font-size: 65%; }\n",
       " #tfvftjhwav .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #tfvftjhwav .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"source\">source</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"text\">text</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"category\">category</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"species\">species</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">1.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat sat on the mat.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">2.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog sat on the mat.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">3.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat is meowing.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">4.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog is barking.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">5.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The cat is climbing a tree.</td>\n",
       "    <td class=\"gt_row gt_left\">feline</td>\n",
       "    <td class=\"gt_row gt_left\">cat</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">6.txt</td>\n",
       "    <td class=\"gt_row gt_left\">The dog is digging a hole.</td>\n",
       "    <td class=\"gt_row gt_left\">canine</td>\n",
       "    <td class=\"gt_row gt_left\">dog</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_toy_corpus(os.path.join(source_path, 'toy.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_nltk_corpus_sources(source_path:str # path to location of sources for building corpora\n",
    "\t\t\t\t\t\t\t ):\n",
    "\t\"\"\" Get NLTK corpora as sources for development or testing Conc functionality. \"\"\"\n",
    "\n",
    "\ttry:\n",
    "\t\timport nltk\n",
    "\texcept ImportError as e:\n",
    "\t\traise ImportError('This function requires NLTK. To minimise requirements this is not installed by default. You can install NLTK with \"pip install nltk\"')\n",
    "\n",
    "\timport nltk\n",
    "\tnltk.download('gutenberg')\n",
    "\tnltk.download('brown')\n",
    "\tnltk.download('reuters')\n",
    "\tfrom nltk.corpus import gutenberg\n",
    "\tfrom nltk.corpus import reuters\n",
    "\tfrom nltk.corpus import brown\n",
    "\n",
    "\tdef clean_text(text):\n",
    "\t\t# to match words/punc that followed by /tags\n",
    "\t\tpattern = re.compile(r\"(\\S+)(/[^ ]+)\") # match non-space followed by / and non-space\n",
    "\t\treturn pattern.sub(r\"\\1\", text)\n",
    "\n",
    "\tif not os.path.exists(source_path):\n",
    "\t\tos.makedirs(source_path, exist_ok=True)\n",
    "\tif not os.path.exists(f'{source_path}/brown'):\n",
    "\t\tos.makedirs(f'{source_path}/brown', exist_ok=True)\n",
    "\tbrown_path = os.path.join(source_path, 'brown.csv.gz')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in brown.fileids():\n",
    "\t\tcorpus_data.append([fileid, clean_text(brown.raw(fileid))])\n",
    "\t\twith open(f'{source_path}/brown/{fileid}.txt', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(clean_text(brown.raw(fileid)))\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str)))\n",
    "\tdf.write_csv(brown_path)\n",
    "\n",
    "\tgutenberg_path = os.path.join(source_path, 'gutenberg.csv.gz')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in gutenberg.fileids():\n",
    "\t\tcorpus_data.append([fileid, clean_text(gutenberg.raw(fileid))])\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str)))\n",
    "\tdf.write_csv(gutenberg_path)\n",
    "\n",
    "\treuters_path = os.path.join(source_path, 'reuters.csv.gz')\n",
    "\tcorpus_data = []\n",
    "\tfor fileid in reuters.fileids():\n",
    "\t\tfileid_name = fileid.split('/')[1]\n",
    "\t\tcorpus_data.append([fileid_name, clean_text(reuters.raw(fileid))])\n",
    "\tdf = pl.DataFrame(corpus_data, orient='row', schema=(('source', str), ('text', str)))\n",
    "\tdf.write_csv(reuters_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /home/geoff/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "get_nltk_corpus_sources(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts for the Brown corpus from nltk can be used to test Conc functionality. The Reuters and Gutenberg corpora are also prepared by `get_nltk_corpus_sources`. Running the function will download the texts and save the texts as a .csv.gz files with columns: source and text. The Brown Corpus is also saved as .txt files to test the Corpus.build_from_texts method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_garden_party(source_path: str #path to location of sources for building corpora\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\" Get corpus of The Garden Party by Katherine Mansfield for development of Conc and testing Conc functionality. \"\"\"\n",
    "\n",
    "\tpath = 'https://github.com/ucdh/scraping-garden-party/raw/master/garden-party-corpus.zip'\n",
    "\n",
    "\timport requests\n",
    "\ttry:\n",
    "\t\timport requests\n",
    "\texcept ImportError as e:\n",
    "\t\traise ImportError('This function requires the requests library. To minimise requirements this is not installed by default. You can install requests with \"pip install requests\"')\n",
    "\n",
    "\tr = requests.get(path)\n",
    "\twith open(f'{source_path}/garden-party-corpus.zip', 'wb') as f:\n",
    "\t\tf.write(r.content)\n",
    "\t# converting to .tar and tar.gz files for testing\n",
    "\timport zipfile\n",
    "\twith zipfile.ZipFile(f'{source_path}/garden-party-corpus.zip', 'r') as z:\n",
    "\t\tz.extractall(f'{source_path}/garden-party-corpus')\n",
    "\timport shutil # make tar.gz\n",
    "\tshutil.make_archive(f'{source_path}/garden-party-corpus', 'gztar', f'{source_path}/garden-party-corpus')\n",
    "\tshutil.move(f'{source_path}/garden-party-corpus.tar.gz', f'{source_path}/garden-party-corpus.tar.gz')\n",
    "\tshutil.make_archive(f'{source_path}/garden-party-corpus', 'tar', f'{source_path}/garden-party-corpus')\n",
    "\tshutil.move(f'{source_path}/garden-party-corpus.tar', f'{source_path}/garden-party-corpus.tar')\n",
    "\tshutil.rmtree(f'{source_path}/garden-party-corpus')\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_garden_party` function downloads a zip file of an example corpus based on Katherine Mansfield short stories. This function creates a .tar and a .tar.gz version of the texts for testing Corpus build methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "get_garden_party(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create large corpora for development and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_large_dataset(source_path: str #path to location of sources for building corpora\n",
    "                    ):\n",
    "    \"\"\" Get 1m rows of https://huggingface.co/datasets/Eugleo/us-congressional-speeches-subset for testing. \"\"\"\n",
    "    df = pl.read_parquet('hf://datasets/Eugleo/us-congressional-speeches-subset/data/train-*.parquet')\n",
    "    df.sample(1000000).select(['speech_id', 'date', 'speaker', 'chamber', 'state', 'text']).write_csv(f'{source_path}/us-congressional-speeches-subset-1m.csv.gz')\n",
    "    del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# get_large_dataset(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Define the chunk size\n",
    "# chunk_size = 100_000  # Adjust based on your memory constraints\n",
    "\n",
    "# # Lazily load the CSV file\n",
    "# df = pl.scan_csv(f'{source_path}us-congressional-speeches-subset-1m.csv.gz')\n",
    "\n",
    "# # Add the new column 'is_empty'\n",
    "# df = df.with_columns(\n",
    "#     (pl.col('text').str.strip_chars().eq('')).alias('is_empty')\n",
    "# )\n",
    "\n",
    "# # get length of is_empty where True\n",
    "# count = df.filter(pl.col(\"is_empty\") == True).collect().height\n",
    "# print(f\"Number of empty rows: {count}\")\n",
    "\n",
    "# any empty?\n",
    "#len(df[df['text'].is_null()])\n",
    "\n",
    "# get distribution of date (by year), speaker, chamber, state\n",
    "# dates are in iso format - extract year and summarize\n",
    "# df = df.with_columns(pl.col('date').str.slice(0, 4).alias('year'))\n",
    "# df.group_by('year').agg(pl.count('year').alias('count')).sort('year', descending=True).head(10).collect()\n",
    "\n",
    "# #df.group_by('speaker').agg(pl.count('speaker').alias('count')).sort('count', descending=True).head(20)\n",
    "# #df.group_by('chamber').agg(pl.count('chamber').alias('count')).sort('count', descending=True).head(20)\n",
    "# df.group_by('state').agg(pl.count('state').alias('count')).sort('count', descending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_large_dataset_sizes(source_path: str, #path to location of sources for building corpora\n",
    "\t\t\t\t\t\tsizes: list = [10000, 100000, 200000, 500000] # list of sizes for test data-sets\n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Create datasets of different sizes from data source retrieved by get_large_dataset for testing. \"\"\"\n",
    "\tfor max_i in sizes:\n",
    "\t\tmax_i_label = int(max_i / 1000)\n",
    "\t\tdf = pl.read_csv(f'{source_path}/us-congressional-speeches-subset-1m.csv.gz')\n",
    "\t\tdf.sample(max_i).write_csv(f'{source_path}/us-congressional-speeches-subset-{max_i_label}k.csv.gz')\n",
    "\t\tlogger.info(f'Creating dataset of {max_i_label}k rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# create_large_dataset_sizes(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build sample corpora\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def build_sample_corpora(\n",
    "\t\tsource_path:str, # path to folder with corpora\n",
    "\t\tsave_path:str, # path to save corpora\n",
    "\t\tforce_rebuild:bool = False # force rebuild of corpora, useful for development and testing\n",
    "\t\t):\n",
    "\t\"\"\"Build all test corpora from source files.\"\"\"\n",
    "\n",
    "\tcorpora = {}\n",
    "\tcorpora['toy'] = {'name': 'Toy Corpus', 'slug': 'toy', 'description': 'Toy corpus is a very small dataset for testing and library development. ', 'extension': '.csv.gz'}\n",
    "\tcorpora['brown'] = {'name': 'Brown Corpus', 'slug': 'brown', 'description': 'A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.', 'extension': '.csv.gz'}\n",
    "\tcorpora['reuters'] = {'name': 'Reuters Corpus', 'slug': 'reuters', 'description': 'Reuters corpus (Reuters-21578, Distribution 1.0). \"The copyright for the text of newswire articles and Reuters annotations in the Reuters-21578 collection resides with Reuters Ltd. Reuters Ltd. and Carnegie Group, Inc. have agreed to allow the free distribution of this data *for research purposes only*. If you publish results based on this data set, please acknowledge its use, refer to the data set by the name (Reuters-21578, Distribution 1.0), and inform your readers of the current location of the data set.\" https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.', 'extension': '.csv.gz'}\n",
    "\tcorpora['gutenberg'] = {'name': 'Gutenberg Corpus', 'slug': 'gutenberg', 'description': 'Project Gutenberg Selections NLTK Corpus. Source: https://gutenberg.org/. Public domain. This version downloaded via NLTK https://www.nltk.org/nltk_data/.', 'extension': '.csv.gz'}\n",
    "\tcorpora['garden-party-corpus'] = {'name': 'Garden Party Corpus', 'slug': 'garden-party', 'description': 'A corpus of short stories from The Garden Party: and Other Stories by Katherine Mansfield. Texts downloaded from Project Gutenberg https://gutenberg.org/ and are in the public domain. The text files contain the short story without the title. https://github.com/ucdh/scraping-garden-party', 'extension': '.zip'}\n",
    "\n",
    "\tif not os.path.exists(source_path):\n",
    "\t\tos.makedirs(source_path)\n",
    "\tif not os.path.exists(save_path):\n",
    "\t\tos.makedirs(save_path)\n",
    "\n",
    "\tif not os.path.exists(f'{source_path}toy.csv.gz'):\n",
    "\t\tcreate_toy_corpus_sources(source_path)\n",
    "\n",
    "\tif not os.path.exists(f'{source_path}garden-party-corpus.zip'):\n",
    "\t\tget_garden_party(source_path)\n",
    "\n",
    "\tif not os.path.exists(f'{source_path}brown.csv.gz'):\n",
    "\t\tget_nltk_corpus_sources(source_path)\n",
    "\n",
    "\tfor corpus_name, corpus_details in corpora.items():\n",
    "\t\tif force_rebuild and os.path.isdir(f'{save_path}{corpus_details[\"slug\"]}.corpus'):\n",
    "\t\t\timport shutil\n",
    "\t\t\tshutil.rmtree(f'{save_path}{corpus_details[\"slug\"]}.corpus', ignore_errors=True)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tcorpus = Corpus().load(f\"{save_path}{corpus_details['slug']}.corpus\")\n",
    "\t\texcept FileNotFoundError:\n",
    "\n",
    "\t\t\tif 'csv' in corpus_details['extension']:\n",
    "\t\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_csv(source_path = f'{source_path}{corpus_name}.csv.gz', text_column='text', metadata_columns=['source'], save_path = save_path)\n",
    "\t\t\telse:\n",
    "\t\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_files(source_path = f'{source_path}{corpus_name}{corpus_details[\"extension\"]}', save_path = save_path)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise e\n",
    "\t\tdel corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_sample_corpora` function downloads sources and creates sample corpora for development and testing for releases. These datasets are a good way to get started working with Conc. Sample corpora available are:\n",
    "\n",
    "* Brown Corpus (via NLTK)  \n",
    "* Gutenberg Corpus (via NLTK)  \n",
    "* Reuters Corpus (via NLTK)  \n",
    "* Garden Party Corpus (Katherine Mansfield short stories)  \n",
    "* Toy Corpus (6 very short texts for testing only)  \n",
    "\n",
    "After installing Conc you can invoke this function from the command line to download and build the sample corpora:\n",
    "```sh\n",
    "conc_build_sample_corpora path/to/save/sources path/to/save/corpora\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `build_sample_corpora` was accessible via conc.corpus as `build_test_corpora` up to version 0.1.1. This functionality is only accessible from `conc.corpora` now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "build_sample_corpora(source_path, save_path, force_rebuild=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
