{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus\n",
    "\n",
    "> Create a conc corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# requirements - numpy pandas polars spacy nltk great_tables\n",
    "# dev requirements - nbdev, jupyterlab, memory_profiler\n",
    "# TODO check\n",
    "\n",
    "import re\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from great_tables import GT\n",
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "from spacy.attrs import ORTH, LOWER # TODO - add ENT_TYPE, ENT_IOB?\n",
    "import sys\n",
    "import pickle\n",
    "import string\n",
    "from fastcore.basics import patch\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc import __version__\n",
    "from conc.core import logger, set_logger_state, PAGE_SIZE, EOF_TOKEN_STR\n",
    "from conc.result import Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "polars_conf = pl.Config.set_tbl_hide_column_data_types(True)\n",
    "polars_conf = pl.Config.set_tbl_hide_dataframe_shape(True)\n",
    "polars_conf = pl.Config.set_tbl_rows(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "_RE_PUNCT = re.compile(r\"^[^\\s^\\w^\\d]$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# first release will support english and spacy as a backend to parse the text - support for other languages and backends will come later.\n",
    "try:\n",
    "\tnlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "\tlogger.error('Error loading model en_core_web_sm. You probably need to run python -m spacy download en_core_web_sm to download the model.')\t\n",
    "\t# download\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "EOF_TOKEN = nlp.vocab[EOF_TOKEN_STR].orth # starts with space so eof_token can't match anything from corpus\n",
    "NOT_DOC_TOKEN = -1\n",
    "INDEX_HEADER_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Corpus:\n",
    "\t\"\"\"Represention of text corpus, with methods to build, load and save a corpus from a variety of formats and to work with the corpus data.\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, \n",
    "\t\t\t\tname: str|None = None, # name of corpus\n",
    "\t\t\t\tdescription: str|None = None # description of corpus\n",
    "\t\t\t\t):\n",
    "\t\t# information about corpus\n",
    "\t\tself.name = name\n",
    "\t\tself.description = description\n",
    "\n",
    "\t\t# conc version that built the corpus\n",
    "\t\tself.conc_version = None\n",
    "\t\t\n",
    "\t\t# paths\n",
    "\t\tself.corpus_path = None\n",
    "\t\tself.source_path = None\n",
    "\n",
    "\t\t# settings\n",
    "\t\tself.EOF_TOKEN = None\n",
    "\n",
    "\t\t# special token ids\n",
    "\t\tself.punct_tokens = None\n",
    "\t\tself.space_tokens = None\n",
    "\n",
    "\t\t# metadata for corpus\n",
    "\t\tself.document_count = None\n",
    "\t\tself.token_count = None\n",
    "\t\tself.unique_tokens = None\n",
    "\n",
    "\t\tself.word_token_count = None\n",
    "\t\tself.unique_word_tokens = None\n",
    "\n",
    "\t\t# token data\n",
    "\t\tself.orth_index = None\n",
    "\t\tself.lower_index = None\n",
    "\n",
    "\t\t# lookup mapping doc_id to every token in doc\n",
    "\t\tself.token2doc_index = None\n",
    "\n",
    "\t\t# lookups to get token string or frequency \n",
    "\t\tself.vocab = None\n",
    "\t\tself.frequency_lookup = None\n",
    "\n",
    "\t\t# offsets for each document in token data\n",
    "\t\tself.offsets = None\n",
    "\n",
    "\t\t# punct and space positions in token data\n",
    "\t\tself.punct_positions = None\n",
    "\t\tself.space_positions = None\n",
    "\n",
    "\t\t# metadata for each document\n",
    "\t\tself.metadata = []\n",
    "\n",
    "\t\t# lookups to get spacy tokenizer or internal ids\n",
    "\t\tself.original_to_new = None\n",
    "\t\tself.new_to_original = None\n",
    "\t\t\n",
    "\t\t# temporary data used when processing text, not saved to disk permanently on save\n",
    "\t\tself.frequency_table = None\n",
    "\t\tself.ngram_index = {}\n",
    "\t\tself.results_cache = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, load and save a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _process_punct_positions(self: Corpus):\n",
    "\t\"\"\" Process punct positions in token data. \"\"\"\n",
    "\tself.punct_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip(string.punctuation) == ''}.keys()))\n",
    "\t# faster to retrieve with isin than where\n",
    "\tpunct_mask = np.isin(self.lower_index, self.punct_tokens) \n",
    "\t# storing this as smaller\n",
    "\tself.punct_positions = np.nonzero(punct_mask)[0] \n",
    "\n",
    "# Spacy includes space tokens in the vocab for non-destructive tokenisation, storing positions so can filter them out \n",
    "# for processing and analysis.\n",
    "\n",
    "@patch\n",
    "def _process_space_positions(self: Corpus):\n",
    "\t\"\"\" Process space positions in token data. \"\"\"\n",
    "\tself.space_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip() == ''}.keys()))\n",
    "\t# faster to retrieve with isin than where\n",
    "\tspace_mask = np.isin(self.lower_index, self.space_tokens) \n",
    "\t# storing this as smaller\n",
    "\tself.space_positions = np.nonzero(space_mask)[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _create_indices(self: Corpus, \n",
    "\t\t\t\t   orth_index: list[np.ndarray], \n",
    "\t\t\t\t   lower_index: list[np.ndarray], \n",
    "\t\t\t\t   token2doc_index: list[np.ndarray]\n",
    "\t\t\t\t   ):\n",
    "\t\"\"\" Create iinternal representation of the corpus for faster analysis and efficient representation on disk. \"\"\"\n",
    "\n",
    "\tunique_values, inverse = np.unique(np.concatenate(orth_index + lower_index), return_inverse=True)\n",
    "\t# add a dummy value at the 0 index to avoid 0 being used as a token id\n",
    "\tunique_values = np.insert(unique_values, 0, 0)\n",
    "\tinverse += 1\n",
    "\tnew_values = np.arange(len(unique_values), dtype=np.uint32)\n",
    "\tself.original_to_new = dict(zip(unique_values, new_values))\n",
    "\tself.new_to_original = dict(zip(new_values, unique_values))\n",
    "\n",
    "\tself.orth_index = np.array(np.split(inverse, 2)[0], dtype=np.uint32)\n",
    "\tself.lower_index = np.array(np.split(inverse, 2)[1], dtype=np.uint32)\n",
    "\tdel inverse\n",
    "\n",
    "\tvocab = {k:nlp.vocab.strings[k] for k in unique_values}\n",
    "\tvocab[0] = 'ERROR: not a token'\n",
    "\n",
    "\tself.vocab = {**{k:vocab[self.new_to_original[k]] for k in new_values}}\n",
    "\n",
    "\tself.EOF_TOKEN = self.original_to_new[EOF_TOKEN]\n",
    "\n",
    "\tself._process_punct_positions()\n",
    "\tself._process_space_positions()\n",
    "\n",
    "\tself.frequency_lookup = dict(zip(*np.unique(self.lower_index, return_counts=True)))\n",
    "\tdel self.frequency_lookup[self.EOF_TOKEN]\n",
    "\n",
    "\tself.token2doc_index = np.concatenate(token2doc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load(self: Corpus, \n",
    "\t\t corpus_path: str # path to load corpus\n",
    "\t\t ):\n",
    "\t\"\"\" Load corpus from disk. \"\"\"\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tif not os.path.isfile(corpus_path):\n",
    "\t\traise FileNotFoundError(f\"Path '{corpus_path}' is not a file\")\n",
    "\tnpz = np.load(corpus_path)\n",
    "\tdata = pickle.loads(npz['corpus'])\n",
    "\tfor k, v in data.items():\n",
    "\t\tsetattr(self, k, v)\n",
    "\tself.orth_index = npz['orth_index']\n",
    "\tself.lower_index = npz['lower_index']\n",
    "\tself.token2doc_index = npz['token2doc_index']\n",
    "\tself.offsets = npz['offsets']\n",
    "\n",
    "\tself.punct_tokens = npz['punct_tokens']\n",
    "\tself.space_tokens = npz['space_tokens']\n",
    "\tself.punct_positions = npz['punct_positions']\n",
    "\tself.space_positions = npz['space_positions']\n",
    "\n",
    "\tself.corpus_path = corpus_path\n",
    "\tlogger.info(f'Load time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save(self: Corpus, \n",
    "\t\t corpus_path: str # path to save corpus\n",
    "\t\t ):\n",
    "\t\"\"\" Save corpus to disk. \"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tif not os.path.isdir(os.path.dirname(corpus_path)):\n",
    "\t\tos.makedirs(os.path.dirname(corpus_path))\n",
    "\tself.corpus_path = corpus_path\n",
    "\tcorpus_bytes = pickle.dumps({k: getattr(self, k) for k in ['metadata', 'vocab', 'frequency_lookup', 'original_to_new', 'new_to_original', 'document_count', 'token_count', 'unique_tokens', 'word_token_count', 'unique_word_tokens', 'source_path', 'name', 'description', 'conc_version', 'EOF_TOKEN']})\n",
    "\twith open(corpus_path, 'wb') as f:\n",
    "\t\tnp.savez_compressed(f, corpus=corpus_bytes, orth_index=self.orth_index, lower_index=self.lower_index, token2doc_index=self.token2doc_index, offsets=self.offsets, punct_tokens=self.punct_tokens, space_tokens=self.space_tokens, punct_positions=self.punct_positions, space_positions=self.space_positions)\n",
    "\tlogger.info(f'Save time: {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build(self: Corpus, \n",
    "\t\t  iterator: iter, # iterator of texts\n",
    "\t\t  batch_size:int=1000 # batch size for spacy tokenizer\n",
    "\t\t  ):\n",
    "\t\"\"\"Build a corpus from an iterator of texts.\"\"\"\n",
    "\t\n",
    "\t# get from library\n",
    "\tself.conc_version = __version__\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\teof_arr = np.array([EOF_TOKEN], dtype=np.uint64)\n",
    "\tnot_doc_arr = np.array([NOT_DOC_TOKEN], dtype=np.int16)\n",
    "\tindex_header_arr = np.array([EOF_TOKEN] * INDEX_HEADER_LENGTH, dtype=np.uint64) # this is added to start and end of index to prevent out of bound issues on searches\n",
    "\n",
    "\torth_index = [index_header_arr]\n",
    "\tlower_index = [index_header_arr]\n",
    "\ttoken2doc_index = [np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32)]\n",
    "\n",
    "\toffset = INDEX_HEADER_LENGTH\n",
    "\tself.offsets = [] # TODO - check that this is being used  - consider removing\n",
    "\n",
    "\tdoc_order = 0\n",
    "\tfor doc in nlp.tokenizer.pipe(iterator, batch_size=batch_size): # test varying this TODO\n",
    "\t\t#TODO  - as corpus size increases memory requirements will increase - consider buffering orth_index, lower_index, token2doc_index and writing to disk periodically\n",
    "\t\torth_index.append(doc.to_array(ORTH))\n",
    "\t\torth_index.append(eof_arr)\n",
    "\n",
    "\t\tlower_index_tmp = doc.to_array(LOWER)\n",
    "\t\tlower_index.append(lower_index_tmp)\n",
    "\t\tlower_index.append(eof_arr)\n",
    "\n",
    "\t\ttoken2doc_index.append(np.array([doc_order] * len(lower_index_tmp), dtype=np.int32))\n",
    "\t\ttoken2doc_index.append(not_doc_arr)\n",
    "\n",
    "\t\tself.offsets.append(offset) \n",
    "\t\toffset = offset + len(lower_index_tmp) + 1\n",
    "\t\tdoc_order += 1\n",
    "\n",
    "\torth_index.append(index_header_arr)\n",
    "\tlower_index.append(index_header_arr)\n",
    "\ttoken2doc_index.append(np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32))\n",
    "\n",
    "\tself._create_indices(orth_index, lower_index, token2doc_index)\n",
    "\n",
    "\tself.document_count = len(self.offsets)\n",
    "\t# adjusting for text breaks and jeaders at start and end of index\n",
    "\tself.token_count = self.lower_index.shape[0] - self.document_count - len(index_header_arr) - len(index_header_arr) \n",
    "\tself.unique_tokens = len(self.frequency_lookup)\n",
    "\n",
    "\tself.word_token_count = len(self.lower_index) - len(self.punct_positions) - len(self.space_positions)\n",
    "\tself.unique_word_tokens = len(self.frequency_lookup) - len(self.punct_tokens) - len(self.space_tokens)\n",
    "\n",
    "\tdel orth_index\n",
    "\tdel lower_index\n",
    "\tdel token2doc_index\n",
    "\n",
    "\tlogger.info(f'Build time: {(time.time() - start_time):.3f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _prepare_files(self: Corpus, \n",
    "\t\t\t\t\tsource_path: str, # path to folder with text files \n",
    "\t\t\t\t\tfile_mask:str='*.txt', # mask to select files \n",
    "\t\t\t\t\tmetadata_file: str|None=None, # path to a CSV with metadata\n",
    "\t\t\t\t\tmetadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from metadata\n",
    "\t\t\t\t\tencoding:str='utf8' # encoding of text files\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Prepare text files and metadata for building a corpus. Returns an iterator to get file text for processing.\"\"\"\n",
    "\n",
    "\t# allowing import from zip and tar files\n",
    "\tif os.path.isdir(source_path):\n",
    "\t\tfiles = glob.glob(os.path.join(source_path, file_mask))\n",
    "\t\ttype = 'folder'\n",
    "\telif os.path.isfile(source_path):\n",
    "\t\timport fnmatch\n",
    "\t\tif source_path.endswith('.zip'):\n",
    "\t\t\timport zipfile\n",
    "\t\t\twith zipfile.ZipFile(source_path, 'r') as z:\n",
    "\t\t\t\tfiles = []\n",
    "\t\t\t\tfor f in z.namelist():\n",
    "\t\t\t\t\tif fnmatch.fnmatch(f, file_mask):\n",
    "\t\t\t\t\t\tfiles.append(f)\n",
    "\t\t\t\tif len(files) > 0:\n",
    "\t\t\t\t\ttype = 'zip'\n",
    "\t\telif source_path.endswith('.tar') or source_path.endswith('.tar.gz'):\n",
    "\t\t\timport tarfile\n",
    "\t\t\twith tarfile.open(source_path, 'r') as t:\n",
    "\t\t\t\tfiles = []\n",
    "\t\t\t\tfor f in t.getnames():\n",
    "\t\t\t\t\tif fnmatch.fnmatch(f, file_mask):\n",
    "\t\t\t\t\t\tfiles.append(f)\n",
    "\t\t\t\tif len(files) > 0:\n",
    "\t\t\t\t\ttype = 'tar'\n",
    "\t\telse:\n",
    "\t\t\traise FileNotFoundError(f\"Path '{source_path}' is not a directory, zip or tar file\")\n",
    "\t\n",
    "\tif not files:\n",
    "\t\traise FileNotFoundError(f\"No files matching {file_mask} found in '{source_path}'\")\n",
    "\n",
    "\torder = pl.DataFrame({metadata_file_column: [os.path.basename(p) for p in files]})\n",
    "\n",
    "\tif metadata_file:\n",
    "\t\tif not os.path.isfile(metadata_file):\n",
    "\t\t\traise FileNotFoundError(f\"Metadata file '{metadata_file}' not found\")\n",
    "\t\ttry:\n",
    "\t\t\tmetadata_columns = set([metadata_file_column] + metadata_columns)\n",
    "\t\t\t\n",
    "\t\t\t# ordering metadata based on order of files so token data and metadata aligned\n",
    "\t\t\tmetadata = pl.read_csv(metadata_file).select(metadata_columns)\n",
    "\t\t\tself.metadata = order.join(metadata, on=metadata_file_column, how='left')\n",
    "\t\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\t\traise\n",
    "\telse:\n",
    "\t\tself.metadata = order\n",
    "\n",
    "\tself.source_path = source_path\n",
    "\n",
    "\tif type == 'folder':\n",
    "\t\tfor p in files:\n",
    "\t\t\tyield open(p, \"rb\").read().decode(encoding)\n",
    "\telif type == 'zip':\n",
    "\t\twith zipfile.ZipFile(source_path, 'r') as z:\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tyield z.read(f).decode(encoding)\n",
    "\telif type == 'tar':\n",
    "\t\twith tarfile.open(source_path, 'r') as t:\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tyield t.extractfile(f).read().decode(encoding)\t\t\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_from_files(self: Corpus,\n",
    "\t\t\t\t\tsource_path: str, # path to folder with text files \n",
    "\t\t\t\t\tfile_mask:str='*.txt', # mask to select files \n",
    "\t\t\t\t\tmetadata_file: str|None=None, # path to a CSV with metadata\n",
    "\t\t\t\t\tmetadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from metadata\n",
    "\t\t\t\t\tencoding:str='utf-8', # encoding of text files\n",
    "\t\t\t\t\tbatch_size:int=1000 # batch size for spacy tokenizer\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Build a corpus from text files in a folder.\"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\titerator = self._prepare_files(source_path, file_mask, metadata_file, metadata_file_column, metadata_columns, encoding)\n",
    "\tself.build(iterator, batch_size)\n",
    "\tlogger.info(f'Build from files time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test = Corpus('test')\n",
    "texts = []\n",
    "for text in test._prepare_files('../test-corpora/source/toy', file_mask='*1.txt'):\n",
    "\ttexts.append(text)\n",
    "assert len(texts) == 1\n",
    "assert texts[0] == 'The cat sat on the mat.'\n",
    "\n",
    "texts = []\n",
    "for text in test._prepare_files('../test-corpora/source/toy', file_mask='*.txt', metadata_file='../test-corpora/source/toy.csv', metadata_file_column = 'source', metadata_columns=['category']):\n",
    "\ttexts.append(text)\n",
    "\n",
    "assert len(texts) == 6\n",
    "assert 'The cat sat on the mat.' in texts\n",
    "assert test.metadata.shape[0] == 6\n",
    "assert test.metadata.columns == ['source', 'category']\n",
    "\n",
    "cat_sat_index = texts.index('The cat sat on the mat.') \n",
    "assert test.metadata['source'][cat_sat_index] == '1.txt'\n",
    "assert test.metadata['category'][cat_sat_index] == 'feline'\n",
    "\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _prepare_csv(self: Corpus, \n",
    "\t\t\t\t\tsource_path:str, # path to csv file\n",
    "\t\t\t\t\ttext_column:str='text', # column in csv with text\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from csv\n",
    "\t\t\t\t\tencoding:str='utf8' # encoding of csv passed to Polars read_csv, see their documentation\n",
    "\t\t\t\t\t) -> iter: # iterator to return rows for processing\n",
    "\t\"\"\"Prepare to import from CSV, including metadata. Returns an iterator to process the text column.\"\"\"\n",
    "\n",
    "\t# TODO - add encoding parameter\n",
    "\n",
    "\tif not os.path.isfile(source_path):\n",
    "\t\traise FileNotFoundError(f'Path ({source_path}) is not a file')\n",
    "\t\n",
    "\ttry:\n",
    "\t\tdf = pl.read_csv(source_path, encoding = encoding).select([text_column] + metadata_columns)\n",
    "\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\traise\n",
    "\n",
    "\tself.source_path = source_path\n",
    "\tself.metadata = df.select(metadata_columns)\n",
    "\n",
    "\tfor row in df.iter_rows():\n",
    "\t\tyield row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_from_csv(self: Corpus, \n",
    "\t\t\t\t   source_path:str, # path to csv file\n",
    "\t\t\t\t   text_column:str='text', # column in csv with text\n",
    "\t\t\t\t   metadata_columns:list[str]=[], # list of column names to import from csv\n",
    "\t\t\t\t   encoding:str='utf8', # encoding of csv passed to Polars read_csv, see their documentation\n",
    "\t\t\t\t   batch_size:int=1000 # batch size for Spacy tokenizer\n",
    "\t\t\t\t   ):\n",
    "\t\"\"\"Build a corpus from a csv file.\"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\titerator = self._prepare_csv(source_path, text_column, metadata_columns, encoding)\n",
    "\tself.build(iterator, batch_size)\n",
    "\tlogger.info(f'Build from csv time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test = Corpus('test')\n",
    "texts = []\n",
    "\n",
    "for text in test._prepare_csv('../test-corpora/source/toy.csv', text_column='text', metadata_columns=['source', 'category']):\n",
    "\ttexts.append(text)\n",
    "\n",
    "assert len(texts) == 6\n",
    "cat_sat_index = 0\n",
    "assert texts[cat_sat_index] == 'The cat sat on the mat.'\n",
    "assert test.metadata.shape[0] == 6\n",
    "assert test.metadata.columns == ['source', 'category']\n",
    "assert test.metadata['source'][cat_sat_index] == '1.txt'\n",
    "assert test.metadata['category'][cat_sat_index] == 'feline'\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = '../test-corpora/source/'\n",
    "save_path = '../test-corpora/saved/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 10:58:57 - INFO - load - Load time: 0.002 seconds\n",
      "2025-03-11 10:58:57 - INFO - load - Load time: 0.113 seconds\n",
      "2025-03-11 10:58:57 - INFO - load - Load time: 0.132 seconds\n",
      "2025-03-11 10:58:57 - INFO - load - Load time: 0.149 seconds\n",
      "2025-03-11 10:58:57 - INFO - build - Build time: 0.260 seconds\n",
      "2025-03-11 10:58:57 - INFO - build_from_files - Build from files time: 0.260 seconds\n",
      "2025-03-11 10:58:57 - INFO - save - Save time: 0.080 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "corpora = {}\n",
    "corpora['toy'] = {'name': 'Toy Corpus', 'description': 'Toy corpus for testing', 'extension': '.csv.gz'}\n",
    "corpora['brown'] = {'name': 'Brown Corpus', 'description': 'A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html', 'extension': '.csv.gz'}\n",
    "corpora['reuters'] = {'name': 'Reuters Corpus', 'description': 'From NLTK TODO', 'extension': '.csv.gz'}\n",
    "corpora['gutenberg'] = {'name': 'Gutenberg Corpus', 'description': 'From NLTK TODO', 'extension': '.csv.gz'}\n",
    "corpora['garden-party-corpus'] = {'name': 'Garden Party Corpus', 'description': 'https://github.com/ucdh/scraping-garden-party', 'extension': '.zip'}\n",
    "\n",
    "set_logger_state('verbose')\n",
    "for corpus_name, corpus_details in corpora.items():\n",
    "\ttry:\n",
    "\t\tcorpus = Corpus().load(f'{save_path}{corpus_name}.corpus')\n",
    "\texcept FileNotFoundError:\n",
    "\t\tif 'csv' in corpus_details['extension']:\n",
    "\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_csv(f'{source_path}{corpus_name}.csv.gz', text_column='text', metadata_columns=['source'])\n",
    "\t\telse:\n",
    "\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_files(f'{source_path}{corpus_name}{corpus_details[\"extension\"]}')\n",
    "\t\tcorpus.save(f'{save_path}{corpus_name}.corpus')\n",
    "\texcept Exception as e:\n",
    "\t\traise e\n",
    "set_logger_state('quiet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add tests for build and save and load\n",
    "\n",
    "try:\n",
    "\ttoy = Corpus().load(f'{save_path}/toy.corpus')\n",
    "except FileNotFoundError:\n",
    "\tbrown = Corpus(name = corpora['toy']['name'], description = corpora['toy']['description']).build_from_csv(f'{source_path}/toy', text_column='text', metadata_columns=['source'])\n",
    "\tbrown.save(f'{save_path}/toy.corpus')\n",
    "except Exception as e:\n",
    "\traise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add tests for build and save and load\n",
    "\n",
    "try:\n",
    "\tbrown = Corpus().load(f'{save_path}/brown.corpus')\n",
    "except FileNotFoundError:\n",
    "\tbrown = Corpus(name = corpora['brown']['name'], description = corpora['brown']['description']).build_from_csv(f'{source_path}/brown', text_column='text', metadata_columns=['source'])\n",
    "\tbrown.save(f'{save_path}/brown.corpus')\n",
    "except Exception as e:\n",
    "\traise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def info(self: Corpus, \n",
    "\t\t include_memory_usage:bool = False, # include memory usage in output\n",
    "\t\t formatted:bool = True # return formatted output\n",
    "\t\t ) -> str: # formatted information about the corpus\n",
    "\t\"\"\" Return information about the corpus. \"\"\"\n",
    "\t\n",
    "\tresult = []\n",
    "\tattributes = ['name', 'description', 'conc_version', 'corpus_path', 'source_path', 'document_count', 'token_count', 'unique_tokens', 'word_token_count', 'unique_word_tokens']\n",
    "\tfor attr in attributes:\n",
    "\t\tvalue = getattr(self, attr)\n",
    "\t\tif isinstance(value, bool):\n",
    "\t\t\tresult.append('True' if value else 'False')\n",
    "\t\telif isinstance(value, int):\n",
    "\t\t\tresult.append(f'{value:,}')\n",
    "\t\telse:\n",
    "\t\t\tresult.append(str(value))\n",
    "\n",
    "\tif include_memory_usage:\n",
    "\t\tsize_attributes = ['orth_index', 'lower_index', 'token2doc_index', 'vocab', 'frequency_lookup', 'offsets', 'metadata', 'original_to_new', 'new_to_original', 'results_cache', 'ngram_index', 'frequency_table']\n",
    "\t\tfor attr in size_attributes:\n",
    "\t\t\tsize = sys.getsizeof(getattr(self, attr))\n",
    "\t\t\tattributes.append(attr + ' (MB)')\n",
    "\t\t\tresult.append(f'{size/1024/1024:.3f}')\n",
    "\t\n",
    "\tif formatted:\n",
    "\t\tattributes = [attr.replace('_', ' ').title() for attr in attributes]\n",
    "\n",
    "\treturn pl.DataFrame({'Attribute': attributes, 'Value': result})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def summary(self: Corpus, \n",
    "\t\t\tinclude_memory_usage:bool = False # include memory usage in output\n",
    "\t\t\t):\n",
    "\t\"\"\" Print information about the corpus in a formatted table. \"\"\"\n",
    "\tresult = Result('summary', self.info(include_memory_usage), 'Corpus Summary', '', {}, [])\n",
    "\tresult.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def __str__(self: Corpus):\n",
    "\t\"\"\" Formatted information about the corpus. \"\"\"\n",
    "\t\n",
    "\treturn str(self.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _index_name(self: Corpus, index):\n",
    "\t\"\"\"Get name of index from spacy.\"\"\"\n",
    "\n",
    "\treturn list(spacy.attrs.IDS.keys())[list(spacy.attrs.IDS.values()).index(index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get summary information on your corpus, including the number of documents, the token count and the number of unique tokens as a dataframe using the `info` method. You can also just print the corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────┬─────────────────────────────────┐\n",
      "│ Attribute          ┆ Value                           │\n",
      "╞════════════════════╪═════════════════════════════════╡\n",
      "│ Name               ┆ Brown Corpus                    │\n",
      "│ Description        ┆ A Standard Corpus of Present-D… │\n",
      "│ Conc Version       ┆ 0.0.1                           │\n",
      "│ Corpus Path        ┆ ../test-corpora/saved//brown.c… │\n",
      "│ Source Path        ┆ ../test-corpora/source/brown.c… │\n",
      "│ Document Count     ┆ 500                             │\n",
      "│ Token Count        ┆ 1,140,905                       │\n",
      "│ Unique Tokens      ┆ 42,937                          │\n",
      "│ Word Token Count   ┆ 980,844                         │\n",
      "│ Unique Word Tokens ┆ 42,907                          │\n",
      "└────────────────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────┬─────────────────────────────────┐\n",
      "│ Attribute          ┆ Value                           │\n",
      "╞════════════════════╪═════════════════════════════════╡\n",
      "│ Name               ┆ Toy Corpus                      │\n",
      "│ Description        ┆ Toy corpus for testing          │\n",
      "│ Conc Version       ┆ 0.0.1                           │\n",
      "│ Corpus Path        ┆ ../test-corpora/saved//toy.cor… │\n",
      "│ Source Path        ┆ ../test-corpora/source/toy.csv… │\n",
      "│ Document Count     ┆ 6                               │\n",
      "│ Token Count        ┆ 38                              │\n",
      "│ Unique Tokens      ┆ 15                              │\n",
      "│ Word Token Count   ┆ 238                             │\n",
      "│ Unique Word Tokens ┆ 14                              │\n",
      "└────────────────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "print(toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the same information in a nicer format by using the `summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"sqvwerumfe\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#sqvwerumfe table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#sqvwerumfe thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#sqvwerumfe p { margin: 0; padding: 0; }\n",
       " #sqvwerumfe .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #sqvwerumfe .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #sqvwerumfe .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #sqvwerumfe .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #sqvwerumfe .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #sqvwerumfe .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #sqvwerumfe .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #sqvwerumfe .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #sqvwerumfe .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #sqvwerumfe .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #sqvwerumfe .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #sqvwerumfe .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #sqvwerumfe .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #sqvwerumfe .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #sqvwerumfe .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #sqvwerumfe .gt_from_md> :first-child { margin-top: 0; }\n",
       " #sqvwerumfe .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #sqvwerumfe .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #sqvwerumfe .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #sqvwerumfe .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #sqvwerumfe .gt_row_group_first td { border-top-width: 2px; }\n",
       " #sqvwerumfe .gt_row_group_first th { border-top-width: 2px; }\n",
       " #sqvwerumfe .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #sqvwerumfe .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #sqvwerumfe .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #sqvwerumfe .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #sqvwerumfe .gt_left { text-align: left; }\n",
       " #sqvwerumfe .gt_center { text-align: center; }\n",
       " #sqvwerumfe .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #sqvwerumfe .gt_font_normal { font-weight: normal; }\n",
       " #sqvwerumfe .gt_font_bold { font-weight: bold; }\n",
       " #sqvwerumfe .gt_font_italic { font-style: italic; }\n",
       " #sqvwerumfe .gt_super { font-size: 65%; }\n",
       " #sqvwerumfe .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #sqvwerumfe .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"2\" class=\"gt_heading gt_title gt_font_normal\">Corpus Summary</td>\n",
       "  </tr>\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"2\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\"></td>\n",
       "  </tr>\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Attribute\">Attribute</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Value\">Value</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Name</td>\n",
       "    <td class=\"gt_row gt_left\">Brown Corpus</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Description</td>\n",
       "    <td class=\"gt_row gt_left\">A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Conc Version</td>\n",
       "    <td class=\"gt_row gt_left\">0.0.1</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Corpus Path</td>\n",
       "    <td class=\"gt_row gt_left\">../test-corpora/saved//brown.corpus</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Source Path</td>\n",
       "    <td class=\"gt_row gt_left\">../test-corpora/source/brown.csv.gz</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Document Count</td>\n",
       "    <td class=\"gt_row gt_left\">500</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Token Count</td>\n",
       "    <td class=\"gt_row gt_left\">1,140,905</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Unique Tokens</td>\n",
       "    <td class=\"gt_row gt_left\">42,937</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Word Token Count</td>\n",
       "    <td class=\"gt_row gt_left\">980,844</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Unique Word Tokens</td>\n",
       "    <td class=\"gt_row gt_left\">42,907</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "brown.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a corpus\n",
    "\n",
    "Explain the various indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_frequency_table(self: Corpus):\n",
    "\t\"\"\" Prepare the frequency table for the corpus. \"\"\"\n",
    "\t# TODO work out case sensitivity issues - currently if do token lookup for The - not there\n",
    "\tif self.frequency_table is None:\n",
    "\t\t# note: don't sort this - leave in order of token_id - sorts can be done when required\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.frequency_table = pl.DataFrame({'token_id': list(self.frequency_lookup.keys()), 'frequency': list(self.frequency_lookup.values())})  \n",
    "\t\tself.frequency_table = self.frequency_table.join(pl.DataFrame({'token_id': list(self.vocab.keys()), 'token': list(self.vocab.values())}), on='token_id', how='left')\n",
    "\t\tself.frequency_table = self.frequency_table.with_columns(self.frequency_table['token_id'].is_in(self.punct_tokens).alias('is_punct')).with_columns(self.frequency_table['token_id'].is_in(self.space_tokens).alias('is_space'))\t\n",
    "\t\tself.frequency_table = self.frequency_table.with_row_index(name='rank', offset=1)\n",
    "\t\tlogger.info(f'Frequency table created in {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _mask_from_positions(self: Corpus, \n",
    "\t\t\t\t\t\t positions # positions to create mask from\n",
    "\t\t\t\t\t\t ):\n",
    "\t\"\"\" Convert positions to mask \"\"\"\n",
    "\tmask_from_positions = np.zeros(self.lower_index.shape, dtype=bool)\n",
    "\tmask_from_positions[positions] = True\n",
    "\treturn mask_from_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_tokens_array(self: Corpus):\n",
    "\t\"\"\" Prepare the tokens array for the corpus. \"\"\"\n",
    "\tif 'tokens_array' not in self.results_cache:\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.results_cache['tokens_array'] = np.array(list(self.vocab.values()))\n",
    "\t\tlogger.info(f'Create tokens_array in {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_tokens_sort_order(self: Corpus):\n",
    "\t\"\"\" Prepare the tokens sort order for the corpus. \"\"\"\n",
    "\tif 'tokens_sort_order' not in self.results_cache:\n",
    "\t\tself._init_tokens_array()\n",
    "\t\t# lowercasing then sorting ...\n",
    "\t\ttokens_array_lower = np.strings.lower(self.results_cache['tokens_array'])\n",
    "\t\tself.results_cache['tokens_sort_order'] = np.argsort(np.argsort(tokens_array_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "# TODO maybe convert to using tokens_array rather than frequency_table\n",
    "def token_to_id(self: Corpus, \n",
    "\t\t\t\ttoken: str # token to get id for\n",
    "\t\t\t\t) -> int|bool: # return token id or False if not found in the corpus\n",
    "\t\"\"\" Get the id for a token string. \"\"\"\n",
    "\n",
    "\tself._init_frequency_table()\n",
    "\ttoken = self.frequency_table.filter(pl.col('token') == token)['token_id']\n",
    "\tif token.shape[0] == 0:\n",
    "\t\treturn False\n",
    "\telse:\n",
    "\t\ttoken = token[0]\n",
    "\treturn token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ID of the token 'dog' like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23289"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.token_to_id('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_ids_to_tokens(self: Corpus, \n",
    "\t\t\t\t\t\ttoken_ids: np.ndarray|list # token ids to retrieve as tokens\n",
    "\t\t\t\t\t\t) -> np.ndarray: # return token strings for token ids\n",
    "\t\"\"\" Get token strings for a list of token ids. \"\"\" \n",
    "\n",
    "\tself._init_tokens_array()\n",
    "\tif isinstance(token_ids, list):\n",
    "\t\ttoken_ids = np.array(token_ids)\n",
    "\treturn self.results_cache['tokens_array'][token_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, conc uses Numpy vector operations where possible. A list or numpy array of Token IDs can be converted to a numpy array of token strings like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['acid', '395,000', 'mckinney'], dtype='<U30')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = [23288, 24576, 47803]\n",
    "brown.token_ids_to_tokens(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_ids_to_sort_order(self: Corpus, \n",
    "\t\t\t\t\t\t\ttoken_ids: np.ndarray # token ids to get rank \n",
    "\t\t\t\t\t\t\t) -> np.ndarray: # rank of token ids\n",
    "\t\"\"\" Get the rank of token ids in the frequency table. \"\"\"\n",
    "\t#TODO document that this is a rank\n",
    "\tself._init_tokens_sort_order()\t\n",
    "\n",
    "\treturn self.results_cache['tokens_sort_order'][token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22848, 23289, 18808]\n",
      "['the' 'dog' 'went']\n",
      "[50087 15848 54497]\n"
     ]
    }
   ],
   "source": [
    "test_token_ids = [\n",
    "brown.token_to_id('the'),\n",
    "brown.token_to_id('dog'),\n",
    "brown.token_to_id('went'),\n",
    "]\n",
    "\n",
    "print(test_token_ids)\n",
    "print(brown.token_ids_to_tokens(test_token_ids))\n",
    "print(brown.token_ids_to_sort_order(test_token_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def frequency_of(self: Corpus, \n",
    "\t\t\t\t token:str|int # token id or string to get frequency for\n",
    "\t\t\t\t ) -> int|bool: # return frequency of token or False if not found\n",
    "\t\"\"\" Get the frequency of a specific token. \"\"\"\n",
    "\t# TODO - make work with case insensitive tokens\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tself._init_frequency_table()\n",
    "\t\n",
    "\tif type(token) == str:\n",
    "\t\ttoken = self.token_to_id(token)\n",
    "\t\tif token == False:\n",
    "\t\t\treturn False\n",
    "\n",
    "\tlogger.info(f'Token frequency retrieval time: {(time.time() - start_time):.5f} seconds')\n",
    "\n",
    "\tif token in self.frequency_lookup:\n",
    "\t\treturn int(self.frequency_lookup[token])\n",
    "\telse:\n",
    "\t\treturn False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token [id=24577, go] occurs 625 times.\n",
      "Token [go] occurs 625 times.\n"
     ]
    }
   ],
   "source": [
    "token = 'go'\n",
    "token_id = brown.token_to_id(token)\n",
    "print(f'Token [id={token_id}, {token}] occurs {brown.frequency_of(token_id)} times.')\n",
    "print(f'Token [{token}] occurs {brown.frequency_of(token)} times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# lower_without_punct = test.lower_index[~(test._mask_from_positions(test.punct_positions))]\n",
    "# lower_without_space = test.lower_index[~(test._mask_from_positions(test.space_positions))]\n",
    "# lower_without_space_punct = test.lower_index[~(test._mask_from_positions(test.space_positions) | test._mask_from_positions(test.punct_positions))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def tokenize(self: Corpus, \n",
    "\t\t\t string:str, # string to tokenize \n",
    "\t\t\t return_doc = False, # return doc object\n",
    "\t\t\t simple_indexing = False # use simple indexing\n",
    "             ): # return tokenized string\n",
    "\t\"\"\" Tokenize a string using the Spacy tokenizer. \"\"\"\n",
    "\t# TODO implement case insensitive tokenization\n",
    "\t# TODO implement wildcard search and multiple strings\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tplaceholder_string = 'zzxxzzplaceholderzzxxzz' # so doesn't split tokens\n",
    "\tis_wildcard_search = False\n",
    "\tif simple_indexing == True:\n",
    "\t\tindex_id = LOWER\n",
    "\t\tstrings_to_tokenize = [string.strip()]\n",
    "\telse:\n",
    "\t\traise('only simple_indexing implemented')\n",
    "\t\t# TODO rework\n",
    "\t\t# if '*' in string:\n",
    "\t\t# \tis_wildcard_search = True\n",
    "\t\t# \tstring = string.replace('*',placeholder_string)\n",
    "\t\t# if string.islower() == True:\n",
    "\t\t# \tindex_id = LOWER\n",
    "\t\t# else:\n",
    "\t\t# \tindex_id = ORTH\n",
    "\t\t# if '|' in string:\n",
    "\t\t# \tstrings_to_tokenize = string.split('|')\n",
    "\t\t# else:\n",
    "\t\t# \tstrings_to_tokenize = [string.strip()]\n",
    "\ttoken_sequences = []\n",
    "\tfor doc in nlp.tokenizer.pipe(strings_to_tokenize):\n",
    "\t\ttoken_sequences.append(tuple(doc.to_array(index_id)))\n",
    "\t# if is_wildcard_search == True:\n",
    "\t# \ttmp_token_sequence = []\n",
    "\t# \tsequence_count = 1\n",
    "\t# \tfor token in doc:\n",
    "\t# \t\ttmp_token_sequence.append([])\n",
    "\t# \t\tif placeholder_string in token.text:\n",
    "\t# \t\t\tchunked_string = token.text.split(placeholder_string)\n",
    "\t# \t\t\tif len(chunked_string) > 2 or (len(chunked_string) == 2 and chunked_string[0] != '' and chunked_string[1] != ''):\n",
    "\t# \t\t\t\t# use regex\n",
    "\t# \t\t\t\tapproach = 'regex'\n",
    "\t# \t\t\t\tregex = re.compile('.*'.join(chunked_string))\n",
    "\t# \t\t\telif chunked_string[0] == '':\n",
    "\t# \t\t\t\tapproach = 'endswith'\n",
    "\t# \t\t\telse:\n",
    "\t# \t\t\t\tapproach = 'startswith'\n",
    "\t# \t\t\tfor token_id in loaded_corpora[corpus_name]['frequency_lookup']:\n",
    "\t# \t\t\t\tpossible_word = False\n",
    "\t# \t\t\t\tword = loaded_corpora[corpus_name]['vocab'][token_id]\n",
    "\t# \t\t\t\tif approach == 'regex':\n",
    "\t# \t\t\t\t\tif regex.match(word):\n",
    "\t# \t\t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\telif getattr(word,approach)(''.join(chunked_string)):\n",
    "\t# \t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\tif possible_word != False:\n",
    "\t# \t\t\t\t\ttmp_token_sequence[token.i].append(loaded_corpora[corpus_name]['vocab'][possible_word])\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttmp_token_sequence[token.i].append(token.orth)\n",
    "\t# \t\tsequence_count *= len(tmp_token_sequence[token.i])\n",
    "\t# \trotated_token_sequence = []\n",
    "\t# \ttoken_repeat = sequence_count\n",
    "\t# \tfor pos in range(len(tmp_token_sequence)):\n",
    "\t# \t\trotated_token_sequence.append([])\n",
    "\t# \t\tif len(tmp_token_sequence[pos]) == 1:\n",
    "\t# \t\t\trotated_token_sequence[pos] += sequence_count * [tmp_token_sequence[pos][0]]\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttoken_repeat = token_repeat // len(tmp_token_sequence[pos])\n",
    "\t# \t\t\twhile len(rotated_token_sequence[pos]) < sequence_count:\n",
    "\t# \t\t\t\tfor token in tmp_token_sequence[pos]:\n",
    "\t# \t\t\t\t\trotated_token_sequence[pos] += token_repeat * [token]\n",
    "\t# \ttoken_sequences = list(zip(*rotated_token_sequence))\n",
    "\t# \t#for tokens in tmp_token_sequence:\n",
    "\t# \t#    for token in tokens:\n",
    "\t# covert token_sequences to reindexed tokens using original_to_new\n",
    "\ttoken_sequences = [tuple([self.original_to_new[token] for token in sequence]) for sequence in token_sequences]\n",
    "\tlogger.info(f'Tokenization time: {(time.time() - start_time):.5f} seconds')\n",
    "\tif return_doc == True:\n",
    "\t\treturn token_sequences, index_id, doc\n",
    "\telse:\n",
    "\t\treturn token_sequences, index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.uint32(23289),)] LOWER\n"
     ]
    }
   ],
   "source": [
    "token_str = 'dog'\n",
    "brown_token_sequence, brown_index_id = brown.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "print(brown_token_sequence, brown._index_name(brown_index_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find positions of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_token_index(self: Corpus, \n",
    "\t\t\t\t\ttoken_sequence: list[np.ndarray], # token sequence to get index for \n",
    "\t\t\t\t\tindex_id: int # index to search (i.e. ORTH, LOWER)\n",
    "\t\t\t\t\t) -> np.ndarray: # positions of token sequence\n",
    "\t\"\"\" Get the positions of a token sequence in the corpus. \"\"\"\n",
    "\t\n",
    "\t#TODO - refactor token_sequence?\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tresults = []\n",
    "\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\tvariants_len = len(token_sequence)\n",
    "\n",
    "\tif index_id == ORTH:\n",
    "\t\tindex = 'orth_index'\n",
    "\telse:\n",
    "\t\tindex = 'lower_index'\n",
    "\n",
    "\tif (index, sequence_len) not in self.ngram_index:\n",
    "\t\tslices = [] # TODO adjust so not just lower below - so need a var to pass to this function with whether islower\n",
    "\t\t[slices.append(np.roll(getattr(self, index), shift)) for shift in -np.arange(sequence_len)]\n",
    "\t\tseq = np.vstack(slices).T\n",
    "\t\tself.ngram_index[(index, sequence_len)] = seq\n",
    "\n",
    "\tif variants_len == 1:\n",
    "\t\tresults.append(np.where(np.all(self.ngram_index[(index, sequence_len)] == token_sequence[0], axis=1))[0])\n",
    "\telse:\n",
    "\t\tcondition_list = []\n",
    "\t\tchoice_list = variants_len * [True]\n",
    "\t\tfor seq in token_sequence:\n",
    "\t\t\tcondition_list.append(self.ngram_index[(index, sequence_len)] == seq)\n",
    "\t\tresults.append(np.where(np.all(np.select(condition_list, choice_list),axis=1))[0])\n",
    "\n",
    "\tlogger.info(f'Token indexing ({len(results[0])}) time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  18944,   18981,   18992,   19062,   19069,   37777,   89076,\n",
      "        125511,  137608,  138261,  138296,  138305,  138349,  144502,\n",
      "        189104,  249691,  249831,  250054,  250067,  250093,  250161,\n",
      "        250187,  250247,  250275,  250386,  251335,  251354,  251414,\n",
      "        251473,  251505,  251559,  251569,  251894,  253602,  254562,\n",
      "        256120,  256224,  256397,  331441,  360984,  439241,  439245,\n",
      "        439300,  439305,  464727,  464756,  464778,  522492,  649908,\n",
      "        695780,  695829,  695989,  696181,  696460,  696839,  696916,\n",
      "        697014,  863902,  863909,  865540,  865558,  877577,  877619,\n",
      "        877706,  889653,  997085, 1014338, 1030313, 1052840, 1052849,\n",
      "       1054274, 1077178, 1087042, 1088300, 1088332, 1088919, 1107306,\n",
      "       1130649, 1139762])]\n"
     ]
    }
   ],
   "source": [
    "token_str = 'dog'\n",
    "brown_token_sequence, brown_token_id = brown.tokenize(token_str, simple_indexing=True)\n",
    "brown_token_index = brown.get_token_index(brown_token_sequence, brown_index_id)\n",
    "print(brown_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
