{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus\n",
    "\n",
    "> Create a conc corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from great_tables import GT\n",
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "from spacy.attrs import ORTH, LOWER, SPACY # May extend to POS, TAG, SENT_START, LEMMA\n",
    "import string\n",
    "from fastcore.basics import patch\n",
    "import time\n",
    "from slugify import slugify\n",
    "import msgspec # tested against orjson - with validation was faster, without around the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc import __version__\n",
    "from conc.core import logger, set_logger_state, spacy_attribute_name, CorpusMetadata, PAGE_SIZE, EOF_TOKEN_STR, ERR_TOKEN_STR, REPOSITORY_URL, DOCUMENTATION_URL, CITATION_STR, PYPI_URL\n",
    "from conc.result import Result\n",
    "from conc.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "polars_conf = pl.Config.set_tbl_hide_column_data_types(True)\n",
    "polars_conf = pl.Config.set_tbl_hide_dataframe_shape(True)\n",
    "polars_conf = pl.Config.set_tbl_rows(50)\n",
    "polars_conf = pl.Config.set_tbl_width_chars(300)\n",
    "polars_conf = pl.Config.set_fmt_str_lengths(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "_RE_PUNCT = re.compile(r\"^[^\\s^\\w^\\d]$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "NOT_DOC_TOKEN = -1\n",
    "INDEX_HEADER_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = f'{os.environ.get(\"HOME\")}/data/'\n",
    "save_path = f'{os.environ.get(\"HOME\")}/data/conc-test-corpora/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Corpus:\n",
    "\t\"\"\"Represention of text corpus, with methods to build, load and save a corpus from a variety of formats and to work with the corpus data.\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, \n",
    "\t\t\t\tname: str = '', # name of corpus\n",
    "\t\t\t\tdescription: str = '' # description of corpus\n",
    "\t\t\t\t):\n",
    "\t\t# information about corpus\n",
    "\t\tself.name = name\n",
    "\t\tself.description = description\n",
    "\t\tself.slug = None\n",
    "\n",
    "\t\t# conc version that built the corpus\n",
    "\t\tself.conc_version = None\n",
    "\t\t\n",
    "\t\t# paths\n",
    "\t\tself.corpus_path = None\n",
    "\t\tself.source_path = None\n",
    "\n",
    "\t\t# settings\n",
    "\t\tself.SPACY_MODEL = None\n",
    "\t\tself.SPACY_MODEL_VERSION = None\n",
    "\t\tself.SPACY_EOF_TOKEN = None # set below as nlp.vocab[EOF_TOKEN_STR].orth in build or through load  - EOF_TOKEN_STR starts with space so eof_token can't match anything from corpus\n",
    "\t\tself.EOF_TOKEN = None\n",
    "\n",
    "\t\t# special token ids\n",
    "\t\tself.punct_tokens = None\n",
    "\t\tself.space_tokens = None\n",
    "\n",
    "\t\t# metadata for corpus\n",
    "\t\tself.document_count = None\n",
    "\t\tself.token_count = None\n",
    "\t\tself.unique_tokens = None\n",
    "\n",
    "\t\tself.word_token_count = None\n",
    "\t\tself.unique_word_tokens = None\n",
    "\n",
    "\t\tself.date_created = None\n",
    "\n",
    "\t\t# token data\n",
    "\t\tself.tokens = None\n",
    "\t\t# self.orth_index = None\n",
    "\t\t# self.lower_index = None\n",
    "\n",
    "\t\t# lookup mapping doc_id to every token in doc\n",
    "\t\t# self.token2doc_index = None\n",
    "\n",
    "\t\t# lookups to get token string or frequency \n",
    "\t\tself.vocab = None\n",
    "\t\t# self.frequency_lookup = None\n",
    "\n",
    "\t\t# offsets for each document in token data\n",
    "\t\t# self.offsets = None\n",
    "\n",
    "\t\t# punct and space positions in token data\n",
    "\t\t# self.punct_positions = None\n",
    "\t\t# self.space_positions = None\n",
    "\t\tself.puncts = None\n",
    "\t\tself.spaces = None\n",
    "\n",
    "\t\t# metadata for each document\n",
    "\t\tself.metadata = None\n",
    "\n",
    "\t\t# lookups to get spacy tokenizer or internal ids\n",
    "\t\t# self.original_to_new = None\n",
    "\t\t# self.new_to_original = None\n",
    "\t\t\n",
    "\t\t# temporary data used when processing text, not \n",
    "\t\t# \n",
    "\t\t# \n",
    "\t\t# \n",
    "\t\t# d to disk permanently on save\n",
    "\t\t\n",
    "\t\t# self.frequency_table = None\n",
    "\t\tself.ngram_index = {}\n",
    "\t\tself.results_cache = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and save a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_spacy_model(self: Corpus,\n",
    "                model: str = 'en_core_web_sm', # spacy model to use for tokenization\n",
    "\t\t\t\tversion: str|None = None # version of spacy model expected, if mismatch will raise a warning\n",
    "\t\t\t\t):\n",
    "\ttry:\n",
    "\t\tself._nlp = spacy.load(model)\n",
    "\t\tself._nlp.disable_pipes(['parser', 'ner', 'lemmatizer', 'tagger', 'senter', 'tok2vec', 'attribute_ruler'])\n",
    "\t\tself._nlp.max_length = 10_000_000 # set max length to a large number to avoid issues with long documents\n",
    "\texcept OSError as e:\n",
    "\t\tlogger.error(f'Error loading model {model}. You need to run python -m spacy download YOURMODEL to download the model. See https://spacy.io/models for available models.')\n",
    "\t\traise e\n",
    "\t\n",
    "\tif version is not None:\n",
    "\t\tif self._nlp.meta['version'] != version:\n",
    "\t\t\tlogger.warning(f'Spacy model version mismatch: expecting {version}, got {self._nlp.meta[\"version\"]}. This may cause issues with tokenization.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _process_punct_positions(self: Corpus):\n",
    "\t\"\"\" Process punctuation positions in token data and populates punct_tokens and punct_positions. \"\"\"\n",
    "\n",
    "\tself.punct_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip(string.punctuation) == ''}.keys()))\n",
    "\tpunct_mask = np.isin(self.lower_index, self.punct_tokens) # faster to retrieve with isin than where\n",
    "\tself.punct_positions = np.nonzero(punct_mask)[0] # storing this as smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation tokens are defined using Python `string.punctuation` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def _process_space_positions(self: Corpus):\n",
    "\t\"\"\" Process whitespace positions in token data and populates space_tokens and space_positions. \"\"\"\n",
    "\n",
    "\tself.space_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip() == ''}.keys()))\n",
    "\tspace_mask = np.isin(self.lower_index, self.space_tokens) \t# faster to retrieve with isin than where\n",
    "\tself.space_positions = np.nonzero(space_mask)[0] # storing this as smaller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy includes space tokens in the vocab for non-destructive tokenisation. Positions of space tokens are stored so they can be filtered out for analysis and reporting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "# reminder of string.punctuation characters\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens consisting of only punctuation are defined as punctuation tokens. These can be removed or included in analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_build_process(self:Corpus,\n",
    "\t\t\t\t\t\tsave_path: str, # path to save corpus data \n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Create slug, corpus_path, and create directory if needed. \"\"\"\n",
    "\n",
    "\tself.conc_version = __version__\n",
    "\tself.slug = slugify(self.name, stopwords=['corpus'])\n",
    "\tself.corpus_path = f'{save_path}/{self.slug}.corpus'\n",
    "\n",
    "\tif not os.path.isdir(self.corpus_path):\n",
    "\t\tos.makedirs(self.corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _update_build_process(self: Corpus, \n",
    "                           orth_index: list[np.ndarray], # orthographic token ids\n",
    "                           lower_index: list[np.ndarray], # lower case token ids\n",
    "                           token2doc_index: list[np.ndarray], # token to document mapping\n",
    "                           has_spaces: list[np.ndarray], # arrays of whether token has space\n",
    "                           store_pos: int # current store pos\n",
    "                           ) -> int: # next store pos\n",
    "    \"\"\" Write in-progress build data to Parquet disk store. \"\"\"\n",
    "\n",
    "    pl.DataFrame([np.concatenate(orth_index), np.concatenate(lower_index), np.concatenate(token2doc_index), np.concatenate(has_spaces)], schema = [('orth_index', pl.UInt64), ('lower_index', pl.UInt64), ('token2doc_index', pl.Int32), ('has_spaces', pl.Boolean)] ).write_parquet(f'{self.corpus_path}/build_{store_pos}.parquet')\n",
    "    return store_pos + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: currently streaming either with sink_parquet or collect(engine='streaming') can break the order of the dataframe (not just whole rows, but within specific columns leading to misaligned data). Streaming is not being used for the build, this will be reassessed in the future as the new Polars streaming functionality matures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _complete_build_process(self: Corpus, \n",
    "\t\t\t\t\t\t\tbuild_process_cleanup: bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t\t\t\t\t\t):\n",
    "\t\"\"\" Complete the disk-based build to create representation of the corpus. \"\"\"\n",
    "\n",
    "\tlogger.memory_usage('init', init=True)\n",
    "\tinput_df = pl.scan_parquet(f'{self.corpus_path}/build_*.parquet')\n",
    "\t# combining indexes to reindex\n",
    "\tcombined_df = pl.concat([input_df.select(pl.col('orth_index').alias('index')), input_df.select(pl.col('lower_index').alias('index'))])\n",
    "\n",
    "\tinput_length = input_df.select(pl.len()).collect(engine='streaming').item() # tested vs count - len seems to have slight memory overhead, but more correct (i.e. count only counts non-null)\n",
    "\tlogger.memory_usage(f'got input length {input_length}')\n",
    "\n",
    "\t# get unique vocab ids (combining orth and lower) and create new index\n",
    "\tvocab_df  = combined_df.select(pl.col('index').unique().sort().alias('source_id')).with_row_index('token_id', offset=1) #.collect(engine='streaming')\n",
    "\tlogger.memory_usage('collected vocab')\n",
    "\n",
    "\t# combined_df = (combined_df.with_columns(pl.col('index').replace(vocab_df.select(pl.col('source_id'))['source_id'], vocab_df.select(pl.col('token_id'))['token_id']).cast(pl.UInt32)))\n",
    "\t# combined_df = combined_df.with_columns(pl.col('index').cast(pl.UInt32))\n",
    "\n",
    "\tcombined_df = (\n",
    "\t\tcombined_df\n",
    "\t\t.join(vocab_df, left_on=\"index\", right_on=\"source_id\", how=\"left\", maintain_order=\"left\")\n",
    "\t\t.drop(\"index\")\n",
    "\t\t.rename({\"token_id\": \"index\"})\n",
    "\t\t.with_columns(pl.col(\"index\").cast(pl.UInt32).alias(\"index\"))\n",
    "\t)\n",
    "\n",
    "\ttokens_df = pl.concat(\n",
    "\t\t\t\t\t\t\t\t\t[combined_df.select(pl.col('index').alias('orth_index')).slice(0, input_length), \n",
    "\t\t\t\t\t\t\t\t\tcombined_df.select(pl.col('index').alias('lower_index')).slice(input_length),\n",
    "\t\t\t\t\t\t\t\t\tinput_df.select(pl.col('token2doc_index')),\n",
    "\t\t\t\t\t\t\t\t\tinput_df.select(pl.col('has_spaces'))\n",
    "\t\t\t\t\t\t\t\t\t], how='horizontal'\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\n",
    "\tdel combined_df\n",
    "\tdel input_df\n",
    "\tlogger.memory_usage('freed up combined_df and input_df')\n",
    "\n",
    "\tvocab_query = vocab_df.select(pl.col('source_id')).collect(engine='streaming').to_numpy().flatten() # get vocab ids as numpy array for faster processing\n",
    "\n",
    "\tvocab = {k:self._nlp.vocab[k].text for k in vocab_query} # get vocab strings from spacy vocab\n",
    "\ttoken_strs = list(vocab.values())\n",
    "\tlogger.memory_usage('got vocab strings')\n",
    "\tvocab_df = vocab_df.with_columns(pl.Series(token_strs).alias('token'))\n",
    "\tlogger.memory_usage('added vocab strings')\n",
    "\n",
    "\tself.EOF_TOKEN = vocab_df.filter(pl.col('source_id') == self.SPACY_EOF_TOKEN).select(pl.col('token_id')).collect(engine='streaming').item() # casting to int for storage\n",
    "\t\n",
    "\tself.punct_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip(string.punctuation) == '']\n",
    "\tlogger.memory_usage(f'got punct tokens')\n",
    "\tself.space_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip() == '']\n",
    "\tlogger.memory_usage(f'got space tokens')\n",
    "\n",
    "\tdel token_strs\n",
    "\n",
    "\t# Create LazyFrames for punct_positions and space_positions\n",
    "\ttokens_df.select(pl.col('lower_index')).with_row_index('position').filter(pl.col('lower_index').is_in(self.punct_tokens)).select('position').sink_parquet(f'{self.corpus_path}/puncts.parquet', maintain_order = True) #.collect(engine='streaming').to_numpy().flatten()\n",
    "\tlogger.memory_usage('saved punct positions')\n",
    "\ttokens_df.select(pl.col('lower_index')).with_row_index('position').filter(pl.col('lower_index').is_in(self.space_tokens)).select('position').sink_parquet(f'{self.corpus_path}/spaces.parquet', maintain_order = True) #.collect(engine='streaming').to_numpy().flatten()\n",
    "\tlogger.memory_usage('saved space positions')\n",
    "\n",
    "\t# get counts from tokens_df\n",
    "\tfrequency_lower = tokens_df.filter(pl.col('lower_index') != self.EOF_TOKEN).select(pl.col('lower_index')).group_by('lower_index').agg(pl.count('lower_index').alias('frequency_lower')) #.collect(engine='streaming')\n",
    "\tfrequency_orth = tokens_df.filter(pl.col('orth_index') != self.EOF_TOKEN).select(pl.col('orth_index')).group_by('orth_index').agg(pl.count('orth_index').alias('frequency_orth')) #.collect(engine='streaming')\n",
    "\tvocab_df = vocab_df.join(frequency_lower, left_on = 'token_id', right_on = 'lower_index', how='left', maintain_order=\"left\").join(frequency_orth, left_on = 'token_id', right_on = 'orth_index', how='left', maintain_order=\"left\")\n",
    "\tlogger.memory_usage('added frequency to vocab')\n",
    "\n",
    "\tself.unique_tokens = frequency_lower.select(pl.len()).collect(engine='streaming').item() # was len(frequency_lower) before used polars streaming\n",
    "\tlogger.memory_usage(f'got unique tokens {self.document_count}')\n",
    "\n",
    "\tdel frequency_lower\n",
    "\tdel frequency_orth\n",
    "\n",
    "\t# add column for is_punct and is_space based on punct_tokens and space_tokens and token_id\n",
    "\tvocab_df = vocab_df.with_columns((pl.col(\"token_id\").is_in(self.punct_tokens)).alias(\"is_punct\"))\n",
    "\tvocab_df = vocab_df.with_columns((pl.col(\"token_id\").is_in(self.space_tokens)).alias(\"is_space\"))\n",
    "\tvocab_df = vocab_df.drop('source_id').sort(by = pl.col('frequency_orth'), descending = True, nulls_last = True).with_row_index(name='rank', offset=1)\n",
    "\tlogger.memory_usage('added is_punct is_space to vocab')\n",
    "\n",
    "\tvocab_df.collect().write_parquet(f'{self.corpus_path}/vocab.parquet') #, maintain_order = True \n",
    "\tlogger.memory_usage('wrote vocab to disk')\n",
    "\tdel vocab_df\n",
    "\ttokens_df.collect().write_parquet(f'{self.corpus_path}/tokens.parquet') # , maintain_order = True\n",
    "\tlogger.memory_usage('wrote tokens to disk')\n",
    "\tdel tokens_df\n",
    "\n",
    "\t#self.document_count = tokens_df.select(pl.col('token2doc_index').filter(pl.col('token2doc_index') != NOT_DOC_TOKEN).unique().count()).collect(engine='streaming').item()\n",
    "\tself.document_count = pl.scan_parquet(f'{self.corpus_path}/tokens.parquet').select(pl.col('token2doc_index')).max().collect().item()\n",
    "\tlogger.memory_usage(f'got doc count {self.document_count}')\n",
    "\t# adjusting for text breaks and headers at start and end of index\n",
    "\tself.token_count = input_length - self.document_count - INDEX_HEADER_LENGTH - INDEX_HEADER_LENGTH \n",
    "\tlogger.memory_usage('got token count')\n",
    "\n",
    "\tself.punct_token_count = pl.scan_parquet(f'{self.corpus_path}/puncts.parquet').select(pl.len()).collect(engine='streaming').item() # may be more efficient to do this prior to disk write\n",
    "\tlogger.memory_usage('got punct token count')\n",
    "\tself.space_token_count = pl.scan_parquet(f'{self.corpus_path}/spaces.parquet').select(pl.len()).collect(engine='streaming').item() # may be more efficient to do this prior to disk write\n",
    "\tlogger.memory_usage('got space token count')\n",
    "\tself.word_token_count = self.token_count - self.punct_token_count - self.space_token_count\n",
    "\tself.unique_word_tokens = self.unique_tokens - len(self.punct_tokens) - len(self.space_tokens)\n",
    "\t\n",
    "\tself.date_created = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "\tif build_process_cleanup:\n",
    "\t\tfor f in glob.glob(f'{self.corpus_path}/build_*.parquet'):\n",
    "\t\t\tos.remove(f)\n",
    "\t\tlogger.memory_usage('removed build files')\n",
    "\t\n",
    "\tlogger.memory_usage('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _create_indices(self: Corpus, \n",
    "\t\t\t\t   orth_index: list[np.ndarray], # list of np arrays of orth token ids \n",
    "\t\t\t\t   lower_index: list[np.ndarray], # list of np arrays of lower token ids\n",
    "\t\t\t\t   token2doc_index: list[np.ndarray] # list of np arrays of doc ids\n",
    "\t\t\t\t   ):\n",
    "\t\"\"\" (Depreciated) Use Numpy to create internal representation of the corpus for faster analysis and efficient representation on disk. Only used when the disk-based build process is not used. \"\"\"\n",
    "\n",
    "\traise DeprecationWarning('This method is depreciated, the current build process uses _complete_build_process instead.')\n",
    "\n",
    "\tself.token2doc_index = np.concatenate(token2doc_index)\n",
    "\tunique_values, inverse = np.unique(np.concatenate(orth_index + lower_index), return_inverse=True)\n",
    "\n",
    "\t# adding a dummy value at the 0 index to avoid 0 being used as a token id\n",
    "\tunique_values = np.insert(unique_values, 0, 0)\n",
    "\tinverse += 1\n",
    "\tnew_values = np.arange(len(unique_values), dtype=np.uint32)\n",
    "\tself.original_to_new = dict(zip(unique_values, new_values))\n",
    "\tself.new_to_original = dict(zip(new_values, unique_values))\n",
    "\n",
    "\tself.orth_index = np.array(np.split(inverse, 2)[0], dtype=np.uint32)\n",
    "\tself.lower_index = np.array(np.split(inverse, 2)[1], dtype=np.uint32)\n",
    "\tdel inverse\n",
    "\n",
    "\tvocab = {k:self._nlp.vocab.strings[k] for k in unique_values}\n",
    "\tvocab[0] = ERR_TOKEN_STR\n",
    "\n",
    "\tself.vocab = {**{k:vocab[self.new_to_original[k]] for k in new_values}}\n",
    "\n",
    "\tself.EOF_TOKEN = self.original_to_new[self.SPACY_EOF_TOKEN]\n",
    "\n",
    "\tself._process_punct_positions()\n",
    "\tself._process_space_positions()\n",
    "\n",
    "\tself.frequency_lookup = dict(zip(*np.unique(self.lower_index, return_counts=True)))\n",
    "\tdel self.frequency_lookup[self.EOF_TOKEN]\n",
    "\tdel unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_corpus_dataframes(self: Corpus):\n",
    "\t\"\"\" Initialize dataframes after build or load \"\"\"\n",
    "\t\n",
    "\tself.vocab = pl.scan_parquet(f'{self.corpus_path}/vocab.parquet')\n",
    "\tself.tokens = pl.scan_parquet(f'{self.corpus_path}/tokens.parquet')\n",
    "\tself.puncts = pl.scan_parquet(f'{self.corpus_path}/puncts.parquet')\n",
    "\tself.spaces = pl.scan_parquet(f'{self.corpus_path}/spaces.parquet')\n",
    "\tself.metadata = pl.scan_parquet(f'{self.corpus_path}/metadata.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "README_TEMPLATE = \"\"\"# {name}\n",
    "\n",
    "## About\n",
    "\n",
    "This directory contains a corpus created using the [Conc]({REPOSITORY_URL}) Python library. \n",
    "\n",
    "## Corpus Information\n",
    "\n",
    "{description}\n",
    "\n",
    "Date created: {date_created}  \n",
    "Document count: {document_count}  \n",
    "Token count: {token_count}  \n",
    "Word token count: {word_token_count}  \n",
    "Unique tokens: {unique_tokens}  \n",
    "Unique word tokens: {unique_word_tokens}  \n",
    "Conc Version Number: {conc_version}  \n",
    "spaCy model: {SPACY_MODEL}, version {SPACY_MODEL_VERSION}  \n",
    "\n",
    "## Using this corpus\n",
    " \n",
    "Conc can be installed [via pip]({PYPI_URL}):  \n",
    "```\n",
    "pip install conc\n",
    "```\n",
    "Documentation to get you started with Conc are available:\n",
    "[Conc Documentation]({DOCUMENTATION_URL})\n",
    "\n",
    "## Cite Conc\n",
    "\n",
    "{CITATION_STR}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_corpus_metadata(self: Corpus, \n",
    "\t\t ):\n",
    "\t\"\"\" Save corpus metadata. \"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tjson_bytes = msgspec.json.encode(CorpusMetadata(**{k: getattr(self, k) for k in ['name', 'description', 'slug', 'conc_version', 'document_count', 'token_count', 'word_token_count', 'punct_token_count', 'space_token_count', 'unique_tokens', 'unique_word_tokens', 'date_created', 'EOF_TOKEN', 'SPACY_EOF_TOKEN', 'SPACY_MODEL', 'SPACY_MODEL_VERSION', 'punct_tokens', 'space_tokens']}))\n",
    "\n",
    "\twith open(f'{self.corpus_path}/corpus.json', 'wb') as f:\n",
    "\t\tf.write(json_bytes)\n",
    "\n",
    "\twith open(f'{self.corpus_path}/README.md', 'w', encoding='utf-8') as f:\n",
    "\t\tf.write(README_TEMPLATE.format(\n",
    "\t\t\tname=self.name,\n",
    "\t\t\tREPOSITORY_URL=REPOSITORY_URL,\n",
    "\t\t\tPYPI_URL=PYPI_URL,\n",
    "\t\t\tDOCUMENTATION_URL=DOCUMENTATION_URL,\n",
    "\t\t\tCITATION_STR=CITATION_STR,\n",
    "\t\t\tdescription=self.description,\n",
    "\t\t\tdate_created=self.date_created,\n",
    "\t\t\tdocument_count=self.document_count,\n",
    "\t\t\ttoken_count=self.token_count,\n",
    "\t\t\tword_token_count=self.word_token_count,\n",
    "\t\t\tunique_tokens=self.unique_tokens,\n",
    "\t\t\tunique_word_tokens=self.unique_word_tokens,\n",
    "\t\t\tconc_version=self.conc_version,\n",
    "\t\t\tSPACY_MODEL=self.SPACY_MODEL,\n",
    "\t\t\tSPACY_MODEL_VERSION=self.SPACY_MODEL_VERSION\n",
    "\t\t))\n",
    "\t\t\n",
    "\tlogger.info(f'Saved corpus metadata time: {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build(self: Corpus, \n",
    "\t\t  save_path:str, # directory where corpus will be created, a subdirectory will be automatically created with the corpus content\n",
    "\t\t  iterator: iter, # iterator of texts\n",
    "\t\t  model: str='en_core_web_sm', # spacy model to use for tokenisation\n",
    "\t\t  spacy_batch_size:int=500, # batch size for spacy tokenizer\n",
    "\t\t  build_process_batch_size:int=5000, # save in-progress build to disk every n docs\n",
    "\t\t  build_process_cleanup:bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t  ):\n",
    "\t\"\"\"Build a corpus from an iterator of texts.\"\"\"\n",
    "\n",
    "\tself._init_spacy_model(model)\n",
    "\t\n",
    "\tself.SPACY_MODEL = model\n",
    "\tself.SPACY_MODEL_VERSION = self._nlp.meta['version']\n",
    "\tself.SPACY_EOF_TOKEN = self._nlp.vocab[EOF_TOKEN_STR].orth\n",
    "\t\n",
    "\tif self.corpus_path is None: # leaving for testing ... this should already be set if build has been initiated in standard way via build_from_csv, build_from_files or whatever other methods are implemented to handle build/imports in future\n",
    "\t\tself._init_build_process(save_path)\n",
    "\t\n",
    "\tlogger.memory_usage('init', init=True)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\teof_arr = np.array([self.SPACY_EOF_TOKEN], dtype=np.uint64)\n",
    "\tnot_doc_arr = np.array([NOT_DOC_TOKEN], dtype=np.int16)\n",
    "\tindex_header_arr = np.array([self.SPACY_EOF_TOKEN] * INDEX_HEADER_LENGTH, dtype=np.uint64) # this is added to start and end of index to prevent out of bound issues on searches\n",
    "\thas_spaces_eof_arr = np.array([False], dtype=np.bool)\n",
    "\n",
    "\torth_index = [index_header_arr]\n",
    "\tlower_index = [index_header_arr]\n",
    "\ttoken2doc_index = [np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32)]\n",
    "\thas_spaces = [np.array([0] * len(index_header_arr), dtype=np.bool)]\n",
    "\n",
    "\toffset = INDEX_HEADER_LENGTH\n",
    "\n",
    "\tstore_pos = 0\n",
    "\n",
    "\tdoc_order = 1\n",
    "\tfor doc in self._nlp.pipe(iterator, batch_size = spacy_batch_size): # was previously using self._nlp.tokenizer.pipe(iterator, batch_size=batch_size): but this is faster, test other options at some point\n",
    "\t\torth_index.append(doc.to_array(ORTH))\n",
    "\t\torth_index.append(eof_arr)\n",
    "\n",
    "\t\tlower_index_tmp = doc.to_array(LOWER)\n",
    "\t\tlower_index.append(lower_index_tmp)\n",
    "\t\tlower_index.append(eof_arr)\n",
    "\n",
    "\t\ttoken2doc_index.append(np.array([doc_order] * len(lower_index_tmp), dtype=np.int32))\n",
    "\t\ttoken2doc_index.append(not_doc_arr)\n",
    "\n",
    "\t\thas_spaces.append(doc.to_array(SPACY))\n",
    "\t\thas_spaces.append(has_spaces_eof_arr)\n",
    "\t\t# self.offsets.append(offset) \n",
    "\t\t# offset = offset + len(lower_index_tmp) + 1\n",
    "\t\tdoc_order += 1\n",
    "\n",
    "\t\t# update store every build_process_batch_size docs\n",
    "\t\tif doc_order % build_process_batch_size == 0:\n",
    "\t\t\t#was based on condition build_process_path is not None before disk-based build process\n",
    "\t\t\tstore_pos = self._update_build_process(orth_index, lower_index, token2doc_index, has_spaces, store_pos)\n",
    "\t\t\tlower_index, orth_index, token2doc_index, has_spaces = [], [], [], []\n",
    "\t\t\tlogger.memory_usage(f'processed {doc_order} documents')\n",
    "\t\t\t\n",
    "\tdel iterator\n",
    "\torth_index.append(index_header_arr)\n",
    "\tlower_index.append(index_header_arr)\n",
    "\ttoken2doc_index.append(np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32))\n",
    "\thas_spaces.append(np.array([0] * len(index_header_arr), dtype=np.bool))\n",
    "\n",
    "\tlogger.memory_usage(f'Completing build process')\n",
    "\tif save_path is not None:\n",
    "\t\tstore_pos = self._update_build_process(orth_index, lower_index, token2doc_index, has_spaces, store_pos)\n",
    "\t\tlower_index, orth_index, token2doc_index, has_spaces = [], [], [], []\n",
    "\t\tself._complete_build_process(build_process_cleanup = build_process_cleanup)\n",
    "\telse:\n",
    "\t\t# depreciated - leaving for now\n",
    "\t\tself._create_indices(orth_index, lower_index, token2doc_index)\n",
    "\t\t# self.document_count = len(self.offsets)\n",
    "\n",
    "\t\tself.token_count = self.lower_index.shape[0] - self.document_count - len(index_header_arr) - len(index_header_arr) \n",
    "\t\tself.unique_tokens = len(self.frequency_lookup)\n",
    "\n",
    "\t\tself.word_token_count = self.token_count - len(self.punct_positions) - len(self.space_positions)\n",
    "\t\tself.unique_word_tokens = len(self.frequency_lookup) - len(self.punct_tokens) - len(self.space_tokens)\n",
    "\n",
    "\tdel orth_index\n",
    "\tdel lower_index\n",
    "\tdel token2doc_index\n",
    "\tdel has_spaces\n",
    "\t\n",
    "\tlogger.memory_usage(f'Completed build process')\n",
    "\n",
    "\t# save corpus metadata\n",
    "\tself.save_corpus_metadata()\n",
    "\n",
    "\tself._init_corpus_dataframes()\n",
    "\n",
    "\tlogger.info(f'Build time: {(time.time() - start_time):.3f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _prepare_files(self: Corpus, \n",
    "\t\t\t\t\tsource_path: str, # path to folder with text files, path can be a directory, zip or tar/tar.gz file\n",
    "\t\t\t\t\tfile_mask:str='*.txt', # mask to select files \n",
    "\t\t\t\t\tmetadata_file: str|None=None, # path to a CSV with metadata\n",
    "\t\t\t\t\tmetadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from metadata\n",
    "\t\t\t\t\tencoding:str='utf8' # encoding of text files\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Prepare text files and metadata for building a corpus. Returns an iterator to get file text for processing.\"\"\"\n",
    "\n",
    "\t# allowing import from zip and tar files\n",
    "\tif os.path.isdir(source_path):\n",
    "\t\tfiles = glob.glob(os.path.join(source_path, file_mask))\n",
    "\t\ttype = 'folder'\n",
    "\telif os.path.isfile(source_path):\n",
    "\t\timport fnmatch\n",
    "\t\tif source_path.endswith('.zip'):\n",
    "\t\t\timport zipfile\n",
    "\t\t\twith zipfile.ZipFile(source_path, 'r') as z:\n",
    "\t\t\t\tfiles = []\n",
    "\t\t\t\tfor f in z.namelist():\n",
    "\t\t\t\t\tif fnmatch.fnmatch(f, file_mask):\n",
    "\t\t\t\t\t\tfiles.append(f)\n",
    "\t\t\t\tif len(files) > 0:\n",
    "\t\t\t\t\ttype = 'zip'\n",
    "\t\telif source_path.endswith('.tar') or source_path.endswith('.tar.gz'):\n",
    "\t\t\timport tarfile\n",
    "\t\t\twith tarfile.open(source_path, 'r') as t:\n",
    "\t\t\t\tfiles = []\n",
    "\t\t\t\tfor f in t.getnames():\n",
    "\t\t\t\t\tif fnmatch.fnmatch(f, file_mask):\n",
    "\t\t\t\t\t\tfiles.append(f)\n",
    "\t\t\t\tif len(files) > 0:\n",
    "\t\t\t\t\ttype = 'tar'\n",
    "\t\telse:\n",
    "\t\t\traise FileNotFoundError(f\"Path '{source_path}' is not a directory, zip or tar file\")\n",
    "\t\n",
    "\tif not files:\n",
    "\t\traise FileNotFoundError(f\"No files matching {file_mask} found in '{source_path}'\")\n",
    "\n",
    "\tmetadata = pl.LazyFrame({metadata_file_column: [os.path.basename(p) for p in files]})\n",
    "\n",
    "\tif metadata_file:\n",
    "\t\tif not os.path.isfile(metadata_file):\n",
    "\t\t\traise FileNotFoundError(f\"Metadata file '{metadata_file}' not found\")\n",
    "\t\ttry:\n",
    "\t\t\tif metadata_file_column not in metadata_columns:\n",
    "\t\t\t\tmetadata_columns.insert(0, metadata_file_column)\n",
    "\t\t\t\n",
    "\t\t\tmetadata = pl.scan_csv(metadata_file).select(metadata_columns)\n",
    "\t\t\t# reordering files on metadata so token data and metadata aligned\n",
    "\t\t\tfiles = metadata.select(pl.col(metadata_file_column)).collect(engine='streaming').to_numpy().flatten().tolist() # get file names from metadata\n",
    "\t\t\tfiles = [os.path.join(source_path, f) for f in files if os.path.basename(f) in files] \n",
    "\t\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\t\traise\n",
    "\t\n",
    "\tmetadata.sink_parquet(f'{self.corpus_path}/metadata.parquet')\n",
    "\n",
    "\tself.source_path = source_path\n",
    "\n",
    "\tif type == 'folder':\n",
    "\t\tfor p in files:\n",
    "\t\t\tyield open(p, \"rb\").read().decode(encoding)\n",
    "\telif type == 'zip':\n",
    "\t\twith zipfile.ZipFile(source_path, 'r') as z:\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tyield z.read(f).decode(encoding)\n",
    "\telif type == 'tar':\n",
    "\t\twith tarfile.open(source_path, 'r') as t:\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tyield t.extractfile(f).read().decode(encoding)\t\t\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_from_files(self: Corpus,\n",
    "\t\t\t\t\tsource_path: str, # path to folder with text files \n",
    "\t\t\t\t\tsave_path: str, # path to save corpus\n",
    "\t\t\t\t\tfile_mask:str='*.txt', # mask to select files \n",
    "\t\t\t\t\tmetadata_file: str|None=None, # path to a CSV with metadata\n",
    "\t\t\t\t\tmetadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from metadata\n",
    "\t\t\t\t\tencoding:str='utf-8', # encoding of text files\n",
    "\t\t\t\t\tmodel:str='en_core_web_sm', # spacy model to use for tokenisation\n",
    "\t\t\t\t\tspacy_batch_size:int=1000, # batch size for spacy tokenizer\n",
    "\t\t\t\t\tbuild_process_batch_size:int=5000, # save in-progress build to disk every n docs\n",
    "\t\t\t\t\tbuild_process_cleanup:bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Build a corpus from text files in a folder.\"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tself._init_build_process(save_path)\n",
    "\titerator = self._prepare_files(source_path, file_mask, metadata_file, metadata_file_column, metadata_columns, encoding) #, build_process_path=build_process_path\n",
    "\tself.build(save_path = save_path, iterator = iterator, model = model, spacy_batch_size = spacy_batch_size, build_process_batch_size = build_process_batch_size, build_process_cleanup = build_process_cleanup) #build_process_path = build_process_path, \n",
    "\tlogger.info(f'Build from files time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _prepare_csv(self: Corpus, \n",
    "\t\t\t\t\tsource_path:str, # path to csv file\n",
    "\t\t\t\t\ttext_column:str='text', # column in csv with text\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from csv\n",
    "\t\t\t\t\tencoding:str='utf8', # encoding of csv passed to Polars read_csv, see their documentation\n",
    "\t\t\t\t\tbuild_process_batch_size:int=5000 # save in-progress build to disk every n rows\n",
    "\t\t\t\t\t) -> iter: # iterator to return rows for processing\n",
    "\t\"\"\"Prepare to import from CSV, including metadata. Returns an iterator to process the text column.\"\"\"\n",
    "\n",
    "\tif not os.path.isfile(source_path):\n",
    "\t\traise FileNotFoundError(f'Path ({source_path}) is not a file')\n",
    "\t\n",
    "\ttry:\n",
    "\t\tdf = pl.scan_csv(source_path, encoding = encoding).select([text_column] + metadata_columns)\n",
    "\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\traise\n",
    "\n",
    "\tself.source_path = source_path\n",
    "\t\n",
    "\tdf.select(metadata_columns).sink_parquet(f'{self.corpus_path}/metadata.parquet')\n",
    "\n",
    "\tfor slice_df in df.collect(engine='streaming').iter_slices(n_rows=build_process_batch_size):  \n",
    "\t\tfor row in slice_df.iter_rows():\n",
    "\t\t\tyield row[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_from_csv(self: Corpus, \n",
    "\t\t\t\t   source_path:str, # path to csv file\n",
    "\t\t\t\t   save_path: str, # path to save corpus\n",
    "\t\t\t\t   text_column:str='text', # column in csv with text\n",
    "\t\t\t\t   metadata_columns:list[str]=[], # list of column names to import from csv\n",
    "\t\t\t\t   encoding:str='utf8', # encoding of csv passed to Polars read_csv, see their documentation\n",
    "\t\t\t\t   model:str='en_core_web_sm', # spacy model to use for tokenisation\n",
    "\t\t\t\t   spacy_batch_size:int=1000, # batch size for Spacy tokenizer\n",
    "\t\t\t\t   #build_process_path:str=None, # path to save an in-progress build to disk to reduce memory usage\n",
    "\t\t\t\t   build_process_batch_size:int=5000, # save in-progress build to disk every n docs\n",
    "\t\t\t\t   build_process_cleanup:bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t\t\t   ):\n",
    "\t\"\"\"Build a corpus from a csv file.\"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tself._init_build_process(save_path)\n",
    "\titerator = self._prepare_csv(source_path = source_path, text_column = text_column, metadata_columns = metadata_columns, encoding = encoding, build_process_batch_size = build_process_batch_size)\n",
    "\tself.build(save_path = save_path, iterator = iterator, model = model, spacy_batch_size = spacy_batch_size, build_process_batch_size = build_process_batch_size, build_process_cleanup = build_process_cleanup)\n",
    "\tlogger.info(f'Build from csv time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# document_count = 6\n",
    "# token_count = 38\n",
    "# word_token_count = 32\n",
    "# unique_tokens = 15\n",
    "# unique_word_tokens = 14\n",
    "\n",
    "test = Corpus('test').build_from_files(source_path = f'{source_path}toy', save_path = save_path, file_mask='*.txt', metadata_file=f'{source_path}toy.csv', metadata_file_column = 'source', metadata_columns=['category'], model='en_core_web_sm', spacy_batch_size=1000, build_process_batch_size=5000, build_process_cleanup=True)\n",
    "\n",
    "assert test.document_count == 6\n",
    "assert test.token_count == 38\n",
    "assert test.word_token_count == 32\n",
    "assert test.unique_tokens == 15\n",
    "assert test.unique_word_tokens == 14\n",
    "\n",
    "assert type(test.metadata) == pl.LazyFrame\n",
    "test.metadata = test.metadata.collect()\n",
    "assert test.metadata.columns == ['source', 'category']\n",
    "\n",
    "#display(test.metadata.head(20))\n",
    "\n",
    "assert os.path.isfile(f'{test.corpus_path}/corpus.json')\n",
    "assert os.path.isfile(f'{test.corpus_path}/README.md')\n",
    "assert os.path.isfile(f'{test.corpus_path}/vocab.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/tokens.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/puncts.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/spaces.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/metadata.parquet')\n",
    "#display(test.vocab.collect().head(20))\n",
    "test_result = test.vocab.filter(pl.col('token') == 'the').collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token_id')).item() == 10\n",
    "assert test_result.select(pl.col('frequency_lower')).item() == 8\n",
    "assert test_result.select(pl.col('frequency_orth')).item() == 2\n",
    "assert test_result.select(pl.col('is_punct')).item() == False\n",
    "assert test_result.select(pl.col('is_space')).item() == False\n",
    "\n",
    "test_result = test.vocab.filter(pl.col('token') == '.').collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token_id')).item() == 15\n",
    "assert test_result.select(pl.col('frequency_lower')).item() == 6\n",
    "assert test_result.select(pl.col('frequency_orth')).item() == 6\n",
    "assert test_result.select(pl.col('is_punct')).item() == True\n",
    "assert test_result.select(pl.col('is_space')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 0).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('lower_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == NOT_DOC_TOKEN\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 99).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 100).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('lower_index')).item() == 10\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == True\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 104).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == 10\n",
    "assert test_result.select(pl.col('lower_index')).item() == 10\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == True\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 105).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 106).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == 15\n",
    "assert test_result.select(pl.col('lower_index')).item() == 15\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 7).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('lower_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == NOT_DOC_TOKEN\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "#test_result = test.tokens.with_row_index('position').filter(pl.col('position') > 99).collect(engine='streaming')\n",
    "#print(test_result.head(10))\n",
    "\n",
    "if os.path.isdir(test.corpus_path):\n",
    "\tshutil.rmtree(test.corpus_path)\n",
    "\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# document_count = 6\n",
    "# token_count = 38\n",
    "# word_token_count = 32\n",
    "# unique_tokens = 15\n",
    "# unique_word_tokens = 14\n",
    "\n",
    "test = Corpus('test').build_from_csv(source_path = f'{source_path}toy.csv', save_path = save_path, text_column='text', metadata_columns=['source', 'category'], model='en_core_web_sm', spacy_batch_size=1000, build_process_batch_size=5000, build_process_cleanup=True)\n",
    "\n",
    "assert test.document_count == 6\n",
    "assert test.token_count == 38\n",
    "assert test.word_token_count == 32\n",
    "assert test.unique_tokens == 15\n",
    "assert test.unique_word_tokens == 14\n",
    "\n",
    "assert type(test.metadata) == pl.LazyFrame\n",
    "test.metadata = test.metadata.collect()\n",
    "assert test.metadata.columns == ['source', 'category']\n",
    "\n",
    "#display(test.metadata.head(20))\n",
    "\n",
    "assert os.path.isfile(f'{test.corpus_path}/corpus.json')\n",
    "assert os.path.isfile(f'{test.corpus_path}/README.md')\n",
    "assert os.path.isfile(f'{test.corpus_path}/vocab.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/tokens.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/puncts.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/spaces.parquet')\n",
    "assert os.path.isfile(f'{test.corpus_path}/metadata.parquet')\n",
    "#display(test.vocab.collect().head(20))\n",
    "test_result = test.vocab.filter(pl.col('token') == 'the').collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token_id')).item() == 10\n",
    "assert test_result.select(pl.col('frequency_lower')).item() == 8\n",
    "assert test_result.select(pl.col('frequency_orth')).item() == 2\n",
    "assert test_result.select(pl.col('is_punct')).item() == False\n",
    "assert test_result.select(pl.col('is_space')).item() == False\n",
    "\n",
    "test_result = test.vocab.filter(pl.col('token') == '.').collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token_id')).item() == 15\n",
    "assert test_result.select(pl.col('frequency_lower')).item() == 6\n",
    "assert test_result.select(pl.col('frequency_orth')).item() == 6\n",
    "assert test_result.select(pl.col('is_punct')).item() == True\n",
    "assert test_result.select(pl.col('is_space')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 0).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('lower_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == NOT_DOC_TOKEN\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 99).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 100).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('lower_index')).item() == 10\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == True\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 104).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == 10\n",
    "assert test_result.select(pl.col('lower_index')).item() == 10\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == True\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 105).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 106).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == 15\n",
    "assert test_result.select(pl.col('lower_index')).item() == 15\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == 1\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "test_result = test.tokens.with_row_index('position').filter(pl.col('position') == 7).collect(engine='streaming')\n",
    "assert test_result.select(pl.col('orth_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('lower_index')).item() == test.EOF_TOKEN\n",
    "assert test_result.select(pl.col('token2doc_index')).item() == NOT_DOC_TOKEN\n",
    "assert test_result.select(pl.col('has_spaces')).item() == False\n",
    "\n",
    "#test_result = test.tokens.with_row_index('position').filter(pl.col('position') > 99).collect(engine='streaming')\n",
    "#print(test_result.head(10))\n",
    "\n",
    "if os.path.isdir(test.corpus_path):\n",
    "\tshutil.rmtree(test.corpus_path)\n",
    "\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load(self: Corpus, \n",
    "\t\t corpus_path: str # path to load corpus\n",
    "\t\t ):\n",
    "\t\"\"\" Load corpus from disk and load the corresponding spaCy model. \"\"\"\n",
    "\n",
    "\tlogger.memory_usage('init', init=True)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tif not os.path.isdir(corpus_path):\n",
    "\t\traise FileNotFoundError(f\"Path '{corpus_path}' is not a directory\")\n",
    "\t\n",
    "\texpected_files = ['corpus.json', 'vocab.parquet', 'tokens.parquet', 'puncts.parquet', 'spaces.parquet']\n",
    "\tif not all(os.path.isfile(os.path.join(corpus_path, f)) for f in expected_files):\n",
    "\t\traise FileNotFoundError(f\"Path '{corpus_path}' does not contain all expected files: {expected_files}\")\n",
    "\n",
    "\tself.corpus_path = corpus_path\n",
    "\n",
    "\twith open(f'{self.corpus_path}/corpus.json', 'rb') as f:\n",
    "\t\tdata = msgspec.json.decode(f.read(), type=CorpusMetadata)\n",
    "\n",
    "\tfor k in data.__slots__:\n",
    "\t\tsetattr(self, k, getattr(data, k))\n",
    "\n",
    "\tself._init_spacy_model(self.SPACY_MODEL, version = self.SPACY_MODEL_VERSION)\n",
    "\n",
    "\tself._init_corpus_dataframes()\n",
    "\n",
    "\tlogger.info(f'Load time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 15:50:33 - INFO - <module> - Loading toy corpus\n",
      "2025-06-08 15:50:33 - INFO - memory_usage - init, memory usage: 673.109375 MB\n",
      "2025-06-08 15:50:33 - INFO - load - Load time: 0.213 seconds\n",
      "2025-06-08 15:50:33 - INFO - <module> - Loading brown corpus\n",
      "2025-06-08 15:50:33 - INFO - memory_usage - init, memory usage: 690.3515625 MB\n",
      "2025-06-08 15:50:34 - INFO - load - Load time: 0.213 seconds\n",
      "2025-06-08 15:50:34 - INFO - <module> - Loading reuters corpus\n",
      "2025-06-08 15:50:34 - INFO - memory_usage - init, memory usage: 715.72265625 MB\n",
      "2025-06-08 15:50:34 - INFO - load - Load time: 0.215 seconds\n",
      "2025-06-08 15:50:34 - INFO - <module> - Loading gutenberg corpus\n",
      "2025-06-08 15:50:34 - INFO - memory_usage - init, memory usage: 741.30078125 MB\n",
      "2025-06-08 15:50:33 - INFO - load - Load time: -0.643 seconds\n",
      "2025-06-08 15:50:33 - INFO - <module> - Loading garden-party-corpus corpus\n",
      "2025-06-08 15:50:33 - INFO - memory_usage - init, memory usage: 691.4453125 MB\n",
      "2025-06-08 15:50:33 - INFO - load - Load time: 0.200 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "force_rebuild = False\n",
    "\n",
    "corpora = {}\n",
    "corpora['toy'] = {'name': 'Toy Corpus', 'slug': 'toy', 'description': 'Toy corpus is a very small dataset for testing and library development. ', 'extension': '.csv.gz'}\n",
    "corpora['brown'] = {'name': 'Brown Corpus', 'slug': 'brown', 'description': 'A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.', 'extension': '.csv.gz'}\n",
    "corpora['reuters'] = {'name': 'Reuters Corpus', 'slug': 'reuters', 'description': 'Reuters corpus (Reuters-21578, Distribution 1.0). \"The copyright for the text of newswire articles and Reuters annotations in the Reuters-21578 collection resides with Reuters Ltd. Reuters Ltd. and Carnegie Group, Inc. have agreed to allow the free distribution of this data *for research purposes only*. If you publish results based on this data set, please acknowledge its use, refer to the data set by the name (Reuters-21578, Distribution 1.0), and inform your readers of the current location of the data set.\" https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.', 'extension': '.csv.gz'}\n",
    "corpora['gutenberg'] = {'name': 'Gutenberg Corpus', 'slug': 'gutenberg', 'description': 'Project Gutenberg Selections NLTK Corpus. Source: https://gutenberg.org/. Public domain. This version downloaded via NLTK https://www.nltk.org/nltk_data/.', 'extension': '.csv.gz'}\n",
    "corpora['garden-party-corpus'] = {'name': 'Garden Party Corpus', 'slug': 'garden-party', 'description': 'A corpus of short stories from The Garden Party: and Other Stories by Katherine Mansfield. Texts downloaded from Project Gutenberg https://gutenberg.org/ and are in the public domain. The text files contain the short story without the title. https://github.com/ucdh/scraping-garden-party', 'extension': '.zip'}\n",
    "\n",
    "set_logger_state('verbose')\n",
    "for corpus_name, corpus_details in corpora.items():\n",
    "\tif force_rebuild and os.path.isdir(f'{save_path}{corpus_details[\"slug\"]}.corpus'):\n",
    "\t\tshutil.rmtree(f'{save_path}{corpus_details[\"slug\"]}.corpus', ignore_errors=True)\n",
    "\n",
    "\tlogger.info(f'Loading {corpus_name} corpus')\n",
    "\ttry:\n",
    "\t\tcorpus = Corpus().load(f\"{save_path}{corpus_details['slug']}.corpus\")\n",
    "\texcept FileNotFoundError:\n",
    "\t\tif 'csv' in corpus_details['extension']:\n",
    "\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_csv(source_path = f'{source_path}{corpus_name}.csv.gz', text_column='text', metadata_columns=['source'], save_path = save_path)\n",
    "\t\telse:\n",
    "\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_files(source_path = f'{source_path}{corpus_name}{corpus_details[\"extension\"]}', save_path = save_path)\n",
    "\texcept Exception as e:\n",
    "\t\traise e\n",
    "\tdel corpus\n",
    "set_logger_state('quiet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 15:50:33 - INFO - memory_usage - init, memory usage: 691.4453125 MB\n",
      "2025-06-08 15:50:34 - INFO - memory_usage - init, memory usage: 713.06640625 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - Completing build process, memory usage: 753.91015625 MB, difference: 40.84375 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - init, memory usage: 765.765625 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - got input length 1141605, memory usage: 765.640625 MB, difference: -0.125 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - collected vocab, memory usage: 765.640625 MB, difference: 0.0 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - freed up combined_df and input_df, memory usage: 765.640625 MB, difference: 0.0 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - got vocab strings, memory usage: 811.46484375 MB, difference: 45.82421875 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - added vocab strings, memory usage: 813.46484375 MB, difference: 2.0 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - got punct tokens, memory usage: 846.859375 MB, difference: 33.39453125 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - got space tokens, memory usage: 846.859375 MB, difference: 0.0 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - saved punct positions, memory usage: 901.5390625 MB, difference: 54.6796875 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - saved space positions, memory usage: 933.59765625 MB, difference: 32.05859375 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - added frequency to vocab, memory usage: 933.59765625 MB, difference: 0.0 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - got unique tokens None, memory usage: 944.0234375 MB, difference: 10.42578125 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - added is_punct is_space to vocab, memory usage: 944.0234375 MB, difference: 0.0 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - wrote vocab to disk, memory usage: 1347.921875 MB, difference: 403.8984375 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - wrote tokens to disk, memory usage: 1351.421875 MB, difference: 3.5 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - got doc count 500, memory usage: 1351.046875 MB, difference: -0.375 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - got token count, memory usage: 1351.046875 MB, difference: 0.0 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - got punct token count, memory usage: 1353.30078125 MB, difference: 2.25390625 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - got space token count, memory usage: 1355.4765625 MB, difference: 2.17578125 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - removed build files, memory usage: 1355.4765625 MB, difference: 0.0 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - done, memory usage: 1355.4765625 MB, difference: 0.0 MB\n",
      "2025-06-08 15:50:36 - INFO - memory_usage - Completed build process, memory usage: 1355.4765625 MB, difference: 0.0 MB\n",
      "2025-06-08 15:50:36 - INFO - save_corpus_metadata - Saved corpus metadata time: 0.000 seconds\n",
      "2025-06-08 15:50:36 - INFO - build - Build time: 2.601 seconds\n",
      "2025-06-08 15:50:36 - INFO - build_from_csv - Build from csv time: 2.814 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "set_logger_state('verbose')\n",
    "if os.path.isdir(f'{save_path}/brown.corpus'):\n",
    "\tshutil.rmtree(f'{save_path}/brown.corpus')\n",
    "\n",
    "try:\n",
    "\tbrown = Corpus().load(f'{save_path}/brown.corpus')\n",
    "except FileNotFoundError:\n",
    "\tbrown = Corpus(name = corpora['brown']['name'], description = corpora['brown']['description']).build_from_csv(f'{source_path}/brown.csv.gz', save_path = save_path, text_column='text', metadata_columns=['source'])\n",
    "except Exception as e:\n",
    "\traise e\n",
    "del brown\n",
    "set_logger_state('quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "toy = Corpus().load(f'{save_path}/toy.corpus')\n",
    "assert toy.document_count == 6\n",
    "assert toy.token_count == 38\n",
    "assert toy.word_token_count == 32\n",
    "assert toy.unique_tokens == 15\n",
    "assert toy.unique_word_tokens == 14\n",
    "\n",
    "if os.path.isdir(f'{save_path}/toy.corpus'):\n",
    "\tshutil.rmtree(f'{save_path}/toy.corpus')\n",
    "\t\n",
    "try:\n",
    "\ttoy = Corpus().load(f'{save_path}/toy.corpus')\n",
    "except FileNotFoundError:\n",
    "\ttoy = Corpus(name = corpora['toy']['name'], description = corpora['toy']['description']).build_from_csv(f'{source_path}toy.csv.gz', save_path = save_path, text_column='text', metadata_columns=['source'])\n",
    "except Exception as e:\n",
    "\traise e\n",
    "\n",
    "assert toy.document_count == 6\n",
    "assert toy.token_count == 38\n",
    "assert toy.word_token_count == 32\n",
    "assert toy.unique_tokens == 15\n",
    "assert toy.unique_word_tokens == 14\n",
    "\n",
    "del toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def info(self: Corpus, \n",
    "\t\t include_disk_usage:bool = False, # include information of size on disk in output\n",
    "\t\t formatted:bool = True # return formatted output\n",
    "\t\t ) -> str: # formatted information about the corpus\n",
    "\t\"\"\" Return information about the corpus. \"\"\"\n",
    "\t\n",
    "\tresult = []\n",
    "\tattributes = ['name', 'description', 'date_created', 'conc_version', 'corpus_path', 'document_count', 'token_count', 'word_token_count', 'unique_tokens', 'unique_word_tokens']\n",
    "\tfor attr in attributes:\n",
    "\t\tvalue = getattr(self, attr)\n",
    "\t\tif isinstance(value, bool):\n",
    "\t\t\tresult.append('True' if value else 'False')\n",
    "\t\telif isinstance(value, int):\n",
    "\t\t\tresult.append(f'{value:,}')\n",
    "\t\telse:\n",
    "\t\t\tresult.append(str(value))\n",
    "\n",
    "\tif include_disk_usage:\n",
    "\t\tfiles = {'corpus.json': 'Corpus Metadata', 'metadata.parquet': 'Document Metadata', 'tokens.parquet': 'Tokens', 'vocab.parquet': 'Vocab', 'puncts.parquet': 'Punctuation positions', 'spaces.parquet': 'Space positions'}\n",
    "\t\tfor file, file_descriptor in files.items():\n",
    "\t\t\tsize = os.path.getsize(f'{self.corpus_path}/{file}')\n",
    "\t\t\tattributes.append(file_descriptor + ' (MB)')\n",
    "\t\t\tresult.append(f'{size/1024/1024:.3f}')\n",
    "\n",
    "\t# maybe add in status of these: 'results_cache', 'ngram_index', 'frequency_table'\n",
    "\t# size = sys.getsizeof(getattr(self, attr))\n",
    "\t\n",
    "\tif formatted:\n",
    "\t\tattributes = [attr.replace('_', ' ').title() for attr in attributes]\n",
    "\n",
    "\treturn pl.DataFrame({'Attribute': attributes, 'Value': result})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def summary(self: Corpus, \n",
    "\t\t\tinclude_memory_usage:bool = False # include memory usage in output\n",
    "\t\t\t):\n",
    "\t\"\"\" Print information about the corpus in a formatted table. \"\"\"\n",
    "\tresult = Result('summary', self.info(include_memory_usage), 'Corpus Summary', '', {}, [])\n",
    "\tresult.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def __str__(self: Corpus):\n",
    "\t\"\"\" Formatted information about the corpus. \"\"\"\n",
    "\t\n",
    "\treturn str(self.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get summary information on your corpus, including the number of documents, the token count and the number of unique tokens as a dataframe using the `info` method. You can also just print the corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "brown = Corpus().load(f'{save_path}brown.corpus')\n",
    "toy = Corpus().load(f'{save_path}toy.corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ Attribute          ┆ Value                                                                                                                                                                                                                                              │\n",
      "╞════════════════════╪════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╡\n",
      "│ Name               ┆ Brown Corpus                                                                                                                                                                                                                                       │\n",
      "│ Description        ┆ A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 │\n",
      "│                    ┆ http://www.hit.uib.no/icame/brown/bcm.html. This version …                                                                                                                                                                                         │\n",
      "│ Date Created       ┆ 2025-06-08 15:50:36                                                                                                                                                                                                                                │\n",
      "│ Conc Version       ┆ 0.0.1                                                                                                                                                                                                                                              │\n",
      "│ Corpus Path        ┆ /home/geoff/data/conc-test-corpora/brown.corpus                                                                                                                                                                                                    │\n",
      "│ Document Count     ┆ 500                                                                                                                                                                                                                                                │\n",
      "│ Token Count        ┆ 1,140,905                                                                                                                                                                                                                                          │\n",
      "│ Word Token Count   ┆ 980,144                                                                                                                                                                                                                                            │\n",
      "│ Unique Tokens      ┆ 42,937                                                                                                                                                                                                                                             │\n",
      "│ Unique Word Tokens ┆ 42,907                                                                                                                                                                                                                                             │\n",
      "└────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(brown) # equivalent to print(brown.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info` method can also provide information on the disk usage of the corpus setting the `include_disk_usage` parameter to `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ Attribute                  ┆ Value                                                                                                                                                                                                                                              │\n",
      "╞════════════════════════════╪════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╡\n",
      "│ Name                       ┆ Brown Corpus                                                                                                                                                                                                                                       │\n",
      "│ Description                ┆ A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 │\n",
      "│                            ┆ http://www.hit.uib.no/icame/brown/bcm.html. This version …                                                                                                                                                                                         │\n",
      "│ Date Created               ┆ 2025-06-08 15:50:36                                                                                                                                                                                                                                │\n",
      "│ Conc Version               ┆ 0.0.1                                                                                                                                                                                                                                              │\n",
      "│ Corpus Path                ┆ /home/geoff/data/conc-test-corpora/brown.corpus                                                                                                                                                                                                    │\n",
      "│ Document Count             ┆ 500                                                                                                                                                                                                                                                │\n",
      "│ Token Count                ┆ 1,140,905                                                                                                                                                                                                                                          │\n",
      "│ Word Token Count           ┆ 980,144                                                                                                                                                                                                                                            │\n",
      "│ Unique Tokens              ┆ 42,937                                                                                                                                                                                                                                             │\n",
      "│ Unique Word Tokens         ┆ 42,907                                                                                                                                                                                                                                             │\n",
      "│ Corpus Metadata (Mb)       ┆ 0.001                                                                                                                                                                                                                                              │\n",
      "│ Document Metadata (Mb)     ┆ 0.001                                                                                                                                                                                                                                              │\n",
      "│ Tokens (Mb)                ┆ 4.459                                                                                                                                                                                                                                              │\n",
      "│ Vocab (Mb)                 ┆ 0.562                                                                                                                                                                                                                                              │\n",
      "│ Punctuation Positions (Mb) ┆ 0.426                                                                                                                                                                                                                                              │\n",
      "│ Space Positions (Mb)       ┆ 0.007                                                                                                                                                                                                                                              │\n",
      "└────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(brown.info(include_disk_usage=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the same information in a nicer format by using the `summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"pobdznahmn\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#pobdznahmn table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#pobdznahmn thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#pobdznahmn p { margin: 0; padding: 0; }\n",
       " #pobdznahmn .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #pobdznahmn .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #pobdznahmn .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #pobdznahmn .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #pobdznahmn .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #pobdznahmn .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #pobdznahmn .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #pobdznahmn .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #pobdznahmn .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #pobdznahmn .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #pobdznahmn .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #pobdznahmn .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #pobdznahmn .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #pobdznahmn .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #pobdznahmn .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #pobdznahmn .gt_from_md> :first-child { margin-top: 0; }\n",
       " #pobdznahmn .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #pobdznahmn .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #pobdznahmn .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #pobdznahmn .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #pobdznahmn .gt_row_group_first td { border-top-width: 2px; }\n",
       " #pobdznahmn .gt_row_group_first th { border-top-width: 2px; }\n",
       " #pobdznahmn .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #pobdznahmn .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #pobdznahmn .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #pobdznahmn .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #pobdznahmn .gt_left { text-align: left; }\n",
       " #pobdznahmn .gt_center { text-align: center; }\n",
       " #pobdznahmn .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #pobdznahmn .gt_font_normal { font-weight: normal; }\n",
       " #pobdznahmn .gt_font_bold { font-weight: bold; }\n",
       " #pobdznahmn .gt_font_italic { font-style: italic; }\n",
       " #pobdznahmn .gt_super { font-size: 65%; }\n",
       " #pobdznahmn .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #pobdznahmn .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"2\" class=\"gt_heading gt_title gt_font_normal\">Corpus Summary</td>\n",
       "  </tr>\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"2\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\"></td>\n",
       "  </tr>\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Attribute\">Attribute</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Value\">Value</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Name</td>\n",
       "    <td class=\"gt_row gt_left\">Brown Corpus</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Description</td>\n",
       "    <td class=\"gt_row gt_left\">A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html. This version downloaded via NLTK https://www.nltk.org/nltk_data/.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Date Created</td>\n",
       "    <td class=\"gt_row gt_left\">2025-06-08 15:50:36</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Conc Version</td>\n",
       "    <td class=\"gt_row gt_left\">0.0.1</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Corpus Path</td>\n",
       "    <td class=\"gt_row gt_left\">/home/geoff/data/conc-test-corpora/brown.corpus</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Document Count</td>\n",
       "    <td class=\"gt_row gt_left\">500</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Token Count</td>\n",
       "    <td class=\"gt_row gt_left\">1,140,905</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Word Token Count</td>\n",
       "    <td class=\"gt_row gt_left\">980,144</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Unique Tokens</td>\n",
       "    <td class=\"gt_row gt_left\">42,937</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Unique Word Tokens</td>\n",
       "    <td class=\"gt_row gt_left\">42,907</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "brown.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, Conc uses Polars and Numpy vector operations where possible to speed up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_token_arrays(self: Corpus):\n",
    "\t\"\"\" Prepare the temporary token arrays for the corpus. \"\"\"\n",
    "\tif 'tokens_array' not in self.results_cache:\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.results_cache['tokens_array'] = self.vocab.sort(by = pl.col('token_id')).select(pl.col('token')).collect(engine='streaming').to_numpy().flatten()\n",
    "\t\tself.results_cache['tokens_array'] = np.insert(self.results_cache['tokens_array'], 0, ERR_TOKEN_STR) # adding a dummy value at the 0 index to align token strings with token_ids\n",
    "\t\tlogger.info(f'Created tokens_array in {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\t\tstart_time = time.time() \n",
    "\t\t# new functionality for disk-based build \n",
    "\t\tself.results_cache['tokens_lookup'] = dict(zip(self.results_cache['tokens_array'], range(len(self.results_cache['tokens_array']))))\n",
    "\t\tlogger.info(f'Created tokens_lookup in {(time.time() - start_time):.3f} seconds')\n",
    "\t\t\n",
    "\t\tstart_time = time.time()  # move tokens sort order to build process - takes > 1 second for large corpora, but not needed for all results\n",
    "\t\t# building tokens_sort_order was implemented in _init_tokens_sort_order - depreciating to simplify as makes sense to build all these in one go\n",
    "\t\ttokens_array_lower = np.char.lower(self.results_cache['tokens_array'].astype(str))\n",
    "\t\tself.results_cache['tokens_sort_order'] = np.argsort(np.argsort(tokens_array_lower)) # lowercasing then sorting\t\n",
    "\t\tlogger.info(f'Created tokens_sort_order in {(time.time() - start_time):.3f} seconds')\n",
    "\t\tdel tokens_array_lower\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.7 ms, sys: 10.4 ms, total: 43.1 ms\n",
      "Wall time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "brown.results_cache = {}\n",
    "%time brown._init_token_arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_ids_to_tokens(self: Corpus, \n",
    "\t\t\t\t\t\ttoken_ids: np.ndarray|list # token ids to return token strings for \n",
    "\t\t\t\t\t\t) -> np.ndarray: # return token strings for token ids\n",
    "\t\"\"\" Get token strings for a list of token ids. \"\"\" \n",
    "\n",
    "\tself._init_token_arrays()\n",
    "\t\n",
    "\tif isinstance(token_ids, list):\n",
    "\t\ttoken_ids = np.array(token_ids)\n",
    "\tif np.any(token_ids < 0):\n",
    "\t\traise ValueError(\"Token ids must be non-negative integers.\")\n",
    "\t\n",
    "\treturn self.results_cache['tokens_array'][token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def tokens_to_token_ids(self: Corpus, \n",
    "\t\t\t\ttokens: list[str]|np.ndarray[str] # list of tokens to get ids for\n",
    "\t\t\t\t) -> np.ndarray[int]: # array of token ids, 0 for unknown tokens\n",
    "\t\"\"\" Convert a list or np.array of token string to token ids \"\"\"\n",
    "\t\n",
    "\tself._init_token_arrays()\n",
    "\t\n",
    "\tif isinstance(tokens, list):\n",
    "\t\ttokens = np.array(tokens, dtype=str)\n",
    "\t\n",
    "\treturn np.array([self.results_cache['tokens_lookup'].get(token, 0) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_to_id(self: Corpus, \n",
    "\t\t\t\ttoken: str # token to get id for\n",
    "\t\t\t\t) -> int: # return token id (0 if token not found in the corpus)\n",
    "\t\"\"\" Get the token id of a token string. \"\"\"\n",
    "\n",
    "\ttoken_ids = self.tokens_to_token_ids([token])\n",
    "\treturn int(token_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list or numpy array of token strings can be converted to a numpy array of token ids like this using `tokens_to_token_ids` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15682 37698 47121 13458   526 16875 22848 25923 23289]\n"
     ]
    }
   ],
   "source": [
    "tokens = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "token_ids = brown.tokens_to_token_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reverse this use `token_ids_to_tokens` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The' 'quick' 'brown' 'fox' 'jumps' 'over' 'the' 'lazy' 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens = brown.token_ids_to_tokens(token_ids) # token_ids was set above\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "assert np.array_equal(tokens, np.array(['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokens_to_token_ids` method will return a 0 for any tokens not in the corpus vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21572, 28602,     0, 31327])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['some', 'random', 'gazupinfava', 'words']\n",
    "brown.tokens_to_token_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "assert brown.tokens_to_token_ids(['gazupinfava']) == np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If zero is passed to `token_ids_to_tokens` it will return an error token as shown below. A negative value will raise a ValueError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ERROR: not a token'], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.token_ids_to_tokens([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "assert brown.token_ids_to_tokens(np.array([0])) == np.array([ERR_TOKEN_STR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Token ids must be non-negative integers.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "try:\n",
    "    brown.token_ids_to_tokens([1, 2, 0, -1])\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    assert True\n",
    "else:\n",
    "    assert False, \"Expected ValueError for invalid token id 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `token_to_id` method wraps `tokens_to_token_ids`. You can pass a single token string and get the token id back. As with `tokens_to_token_ids`, if the token is not in the vocabulary it will return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47121\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(brown.token_to_id('brown')) # returns token id\n",
    "print(brown.token_to_id('Supercalifragilisticexpialidocious')) # returns 0 if token not in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert type(brown.token_to_id('brown')) == int\n",
    "assert brown.token_to_id('Supercalifragilisticexpialidocious') == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_ids_to_sort_order(self: Corpus, \n",
    "\t\t\t\t\t\t\ttoken_ids: np.ndarray|list # token ids to return token strings for \n",
    "\t\t\t\t\t\t\t) -> np.ndarray: # rank of token ids\n",
    "\t\"\"\" Get the sort order of token strings corresponding to token ids \"\"\"\n",
    "\n",
    "\tself._init_token_arrays()\t\n",
    "\n",
    "\tif isinstance(token_ids, list):\n",
    "\t\ttoken_ids = np.array(token_ids)\n",
    "\tif np.any(token_ids < 0):\n",
    "\t\traise ValueError(\"Token ids must be non-negative integers.\")\n",
    "\t\n",
    "\treturn self.results_cache['tokens_sort_order'][token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The' 'quick' 'brown' 'fox' 'jumps' 'over' 'the' 'lazy' 'dog']\n",
      "[15682 37698 47121 13458   526 16875 22848 25923 23289]\n",
      "[50086 40358  7938 20497 27663 35983 50087 29054 15848]\n",
      "['brown' 'dog' 'fox' 'jumps' 'lazy' 'over' 'quick' 'The' 'the']\n"
     ]
    }
   ],
   "source": [
    "tokens = np.array(['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'])\n",
    "token_ids = brown.tokens_to_token_ids(tokens)\n",
    "sort_order = brown.token_ids_to_sort_order(token_ids)\n",
    "sorted_tokens = tokens[np.argsort(sort_order)]\n",
    "\n",
    "print(tokens)\n",
    "print(token_ids)\n",
    "print(sort_order)\n",
    "print(sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_token_count_text(self: Corpus, \n",
    "\t\t\t\t\texclude_punctuation:bool = False, # exclude punctuation tokens from the count\n",
    "\t\t\t\t\texclude_spaces:bool = False # exclude space tokens from the count\n",
    "\t\t\t\t\t) -> tuple[int, str, str]: # token count with adjustments based on exclusions, token descriptor, total descriptor\n",
    "\t\"\"\" Get the token count for the corpus with adjustments and text for output \"\"\"\n",
    "\n",
    "\tcount_tokens = self.token_count\n",
    "\ttokens_descriptor = 'all tokens'\n",
    "\ttotal_descriptor = 'Total tokens'\n",
    "\tif exclude_punctuation and exclude_spaces:\n",
    "\t\tcount_tokens = self.word_token_count\n",
    "\t\ttokens_descriptor = 'word tokens'\n",
    "\t\ttotal_descriptor = 'Total word tokens'\n",
    "\telif exclude_punctuation:\n",
    "\t\tspace_tokens_count = self.spaces.select(pl.len()).collect(engine='streaming').item()\n",
    "\t\tcount_tokens = self.word_token_count + space_tokens_count\n",
    "\t\ttokens_descriptor = 'word and space tokens'\n",
    "\t\ttotal_descriptor = 'Total word and space tokens'\n",
    "\telif exclude_spaces:\n",
    "\t\tpunct_tokens_count = self.puncts.select(pl.len()).collect(engine='streaming').item()\n",
    "\t\tcount_tokens = self.word_token_count + punct_tokens_count\n",
    "\t\ttokens_descriptor = 'word and punctuation tokens'\n",
    "\t\ttotal_descriptor = 'Total word and punctuation tokens'\n",
    "\n",
    "\treturn count_tokens, tokens_descriptor, total_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert toy.get_token_count_text(exclude_punctuation=True, exclude_spaces=True) == (32, 'word tokens', 'Total word tokens')\n",
    "assert toy.get_token_count_text(exclude_punctuation=False, exclude_spaces=True) == (38, 'word and punctuation tokens', 'Total word and punctuation tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def tokenize(self: Corpus, \n",
    "\t\t\t string:str, # string to tokenize \n",
    "\t\t\t#  return_tokens = False, # return token strings\n",
    "\t\t\t simple_indexing = False # use simple indexing\n",
    "             ): # return tokenized string\n",
    "\t\"\"\" Tokenize a string using the Spacy tokenizer. \"\"\"\n",
    "\t# NOTE: when extending this function - ensure get_token_positions is compatible (e.g. currently assumes fixed sequence length of sequences)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tplaceholder_string = 'zzxxzzplaceholderzzxxzz' # so doesn't split tokens\n",
    "\tis_wildcard_search = False\n",
    "\tif simple_indexing == True:\n",
    "\t\tindex_id = LOWER\n",
    "\t\tstrings_to_tokenize = [string.strip()]\n",
    "\telse:\n",
    "\t\traise('only simple_indexing implemented')\n",
    "\t\t# retained for future rework\n",
    "\t\t# if '*' in string:\n",
    "\t\t# \tis_wildcard_search = True\n",
    "\t\t# \tstring = string.replace('*',placeholder_string)\n",
    "\t\t# if string.islower() == True:\n",
    "\t\t# \tindex_id = LOWER\n",
    "\t\t# else:\n",
    "\t\t# \tindex_id = ORTH\n",
    "\t\t# if '|' in string:\n",
    "\t\t# \tstrings_to_tokenize = string.split('|')\n",
    "\t\t# else:\n",
    "\t\t# \tstrings_to_tokenize = [string.strip()]\n",
    "\ttoken_sequences = []\n",
    "\tfor doc in self._nlp.pipe(strings_to_tokenize): # was tokenizer.pipe(strings_to_tokenize) - retaining for reference\n",
    "\t\t# token_sequences.append(tuple(doc.to_array(index_id))) # not using spacy indexes once corpus created\n",
    "\t\ttoken_sequences.append(list(doc))\n",
    "\t# if is_wildcard_search == True:\n",
    "\t# \ttmp_token_sequence = []\n",
    "\t# \tsequence_count = 1\n",
    "\t# \tfor token in doc:\n",
    "\t# \t\ttmp_token_sequence.append([])\n",
    "\t# \t\tif placeholder_string in token.text:\n",
    "\t# \t\t\tchunked_string = token.text.split(placeholder_string)\n",
    "\t# \t\t\tif len(chunked_string) > 2 or (len(chunked_string) == 2 and chunked_string[0] != '' and chunked_string[1] != ''):\n",
    "\t# \t\t\t\t# use regex\n",
    "\t# \t\t\t\tapproach = 'regex'\n",
    "\t# \t\t\t\tregex = re.compile('.*'.join(chunked_string))\n",
    "\t# \t\t\telif chunked_string[0] == '':\n",
    "\t# \t\t\t\tapproach = 'endswith'\n",
    "\t# \t\t\telse:\n",
    "\t# \t\t\t\tapproach = 'startswith'\n",
    "\t# \t\t\tfor token_id in loaded_corpora[corpus_name]['frequency_lookup']:\n",
    "\t# \t\t\t\tpossible_word = False\n",
    "\t# \t\t\t\tword = loaded_corpora[corpus_name]['vocab'][token_id]\n",
    "\t# \t\t\t\tif approach == 'regex':\n",
    "\t# \t\t\t\t\tif regex.match(word):\n",
    "\t# \t\t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\telif getattr(word,approach)(''.join(chunked_string)):\n",
    "\t# \t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\tif possible_word != False:\n",
    "\t# \t\t\t\t\ttmp_token_sequence[token.i].append(loaded_corpora[corpus_name]['vocab'][possible_word])\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttmp_token_sequence[token.i].append(token.orth)\n",
    "\t# \t\tsequence_count *= len(tmp_token_sequence[token.i])\n",
    "\t# \trotated_token_sequence = []\n",
    "\t# \ttoken_repeat = sequence_count\n",
    "\t# \tfor pos in range(len(tmp_token_sequence)):\n",
    "\t# \t\trotated_token_sequence.append([])\n",
    "\t# \t\tif len(tmp_token_sequence[pos]) == 1:\n",
    "\t# \t\t\trotated_token_sequence[pos] += sequence_count * [tmp_token_sequence[pos][0]]\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttoken_repeat = token_repeat // len(tmp_token_sequence[pos])\n",
    "\t# \t\t\twhile len(rotated_token_sequence[pos]) < sequence_count:\n",
    "\t# \t\t\t\tfor token in tmp_token_sequence[pos]:\n",
    "\t# \t\t\t\t\trotated_token_sequence[pos] += token_repeat * [token]\n",
    "\t# \ttoken_sequences = list(zip(*rotated_token_sequence))\n",
    "\t# \t#for tokens in tmp_token_sequence:\n",
    "\t# \t#    for token in tokens:\n",
    "\t# covert token_sequences to reindexed tokens using original_to_new\n",
    "\t\n",
    "\t# convert sequences to lower case\n",
    "\tif index_id == LOWER:\n",
    "\t\ttoken_sequences = [[token.lower_ for token in sequence] for sequence in token_sequences]\n",
    "\ttoken_sequences = [tuple(self.tokens_to_token_ids(sequence)) for sequence in token_sequences]\n",
    "\t\n",
    "\tlogger.info(f'Tokenization time: {(time.time() - start_time):.5f} seconds')\n",
    "\t# if return_tokens == True:\n",
    "\t\t# return token_sequences, index_id, doc\n",
    "\t# else:\n",
    "\treturn token_sequences, index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.int64(23289),) LOWER\n",
      "(np.int64(23289),) LOWER\n",
      "(np.int64(47121), np.int64(13458)) LOWER\n",
      "(np.int64(22848), np.int64(47121), np.int64(13458)) LOWER\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "token_strs = ['dog', 'Dog', 'Brown Fox', 'the brown fox']\n",
    "\n",
    "for token_str in token_strs:\n",
    "    brown_token_sequence, brown_index_id = brown.tokenize(token_str, simple_indexing=True)\n",
    "    print(brown_token_sequence[0], spacy_attribute_name(brown_index_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with specific texts in the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_text(self:Corpus,\n",
    "        doc_id: int, # the id of the document\n",
    "        ):\n",
    "    \"\"\" Get tokens, space definitions and metadata for a text in the corpus \"\"\"\n",
    "    \n",
    "    if doc_id < 1 or doc_id > self.document_count:\n",
    "        raise ValueError(f\"Document ID {doc_id} is out of range. Document ID should be between 1 and the count of documents ({self.document_count}).\")\n",
    "\n",
    "    doc_tokens = self.tokens.filter(pl.col('token2doc_index') == doc_id).select(['orth_index', 'has_spaces']).collect()\n",
    "    tokens = self.token_ids_to_tokens(doc_tokens.select(pl.col('orth_index')).to_numpy().flatten())\n",
    "    has_spaces = doc_tokens.select(pl.col('has_spaces')).to_numpy().flatten()\n",
    "    metadata = self.metadata.with_row_index(offset = 1, name = 'document_id').filter(pl.col('document_id') == doc_id).collect()\n",
    "    return tokens, has_spaces, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def text(self:Corpus,\n",
    "        doc_id: int # the id of the document\n",
    "        ):\n",
    "    \"\"\" Get a text document \"\"\"\n",
    "\n",
    "    return Text(*self._get_text(doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert str(toy.text(1)) == 'The cat sat on the mat.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find positions of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_tokens_by_index(self: Corpus, \n",
    "\t\t\t   index: str = 'orth_index', # index to get tokens from i.e. 'orth_index' 'lower_index' 'token2doc_index'\n",
    "\t\t\t   exclude_punctuation: bool = False, # exclude punctuation tokens from the result\n",
    "\t\t\t   exclude_spaces: bool = False # exclude space tokens from the result\n",
    "\t\t\t\t) -> np.ndarray:\n",
    "\t\"\"\" Get tokens for a given index. \"\"\"\n",
    "\n",
    "\tif index not in ['orth_index', 'lower_index', 'token2doc_index']:\n",
    "\t\traise ValueError(\"Index must be one of 'orth_index', 'lower_index', 'token2doc_index'\")\n",
    "\n",
    "\tif index not in self.results_cache:\n",
    "\t\tself.results_cache[index] = self.tokens.select(pl.col(index)).collect(engine='streaming').to_numpy().flatten()\n",
    "\t\n",
    "\treturn self.results_cache[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens array: 1141605\n",
      "[27276 15682  4361 14610 54713 45742 53250  8699 45680 30305]\n",
      "['\\n\\n\\t' 'The' 'Fulton' 'County' 'Grand' 'Jury' 'said' 'Friday' 'an'\n",
      " 'investigation']\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "tokens = brown.get_tokens_by_index('orth_index')\n",
    "print(f'Length of tokens array: {len(tokens)}')\n",
    "print(tokens[100:110])  # print first 10 tokens\n",
    "print(brown.token_ids_to_tokens(tokens[100:110]))  # print first 10 token strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_ngrams_by_index(self: Corpus, \n",
    "\t\t\t\tngram_length:int, # length of ngrams to get\n",
    "\t\t\t\tindex:str  # index to get tokens from, e.g. 'orth_index' 'lower_index'\n",
    "\t\t\t\t) -> np.ndarray:\n",
    "\t\"\"\" Get ngrams for a given index and ngram length. \"\"\"\n",
    "\n",
    "\tif index not in ['orth_index', 'lower_index']:\n",
    "\t\traise ValueError(\"Index must be either 'orth_index' or 'lower_index'\")\n",
    "\n",
    "\tif (index, ngram_length) not in self.ngram_index:\n",
    "\t\tslices = []\n",
    "\t\t[slices.append(np.roll(self.get_tokens_by_index(index), shift)) for shift in -np.arange(ngram_length)]\n",
    "\t\tseq = np.vstack(slices).T\n",
    "\t\tself.ngram_index[(index, ngram_length)] = seq\n",
    "\n",
    "\treturn self.ngram_index[(index, ngram_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  6],\n",
       "       [ 6, 12],\n",
       "       [12,  8],\n",
       "       [ 8, 10],\n",
       "       [10, 13],\n",
       "       [13, 15],\n",
       "       [15, 17],\n",
       "       [17, 10],\n",
       "       [10, 11],\n",
       "       [11, 12]], dtype=uint32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy.get_ngrams_by_index(ngram_length=2, index='lower_index')[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# congress = Corpus().load(f'{save_path}/us-congressional-speeches-subset-500k.corpus')\n",
    "# sys.getsizeof(congress.get_tokens_by_index('orth_index'))/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_token_positions(self: Corpus, \n",
    "\t\t\t\t\ttoken_sequence: list[np.ndarray], # token sequence to get index for \n",
    "\t\t\t\t\tindex_id: int # index to search (i.e. ORTH, LOWER)\n",
    "\t\t\t\t\t) -> np.ndarray: # positions of token sequence\n",
    "\t\"\"\" Get the positions of a token sequence in the corpus. \"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tresults = []\n",
    "\n",
    "\tsequence_len = len(token_sequence[0]) # Check when extend tokenization\n",
    "\tvariants_len = len(token_sequence)\n",
    "\n",
    "\tif index_id == ORTH:\n",
    "\t\tindex = 'orth_index'\n",
    "\telse:\n",
    "\t\tindex = 'lower_index'\n",
    "\n",
    "\tif variants_len == 1:\n",
    "\t\tresults.append(np.where(np.all(self.get_ngrams_by_index(ngram_length = sequence_len, index = index) == token_sequence[0], axis=1))[0])\n",
    "\telse:\n",
    "\t\tcondition_list = []\n",
    "\t\tchoice_list = variants_len * [True]\n",
    "\t\tfor seq in token_sequence:\n",
    "\t\t\tcondition_list.append(self.get_ngrams_by_index(ngram_length = sequence_len, index = index) == seq)\n",
    "\t\tresults.append(np.where(np.all(np.select(condition_list, choice_list),axis=1))[0])\n",
    "\n",
    "\tlogger.info(f'Token indexing ({len(results[0])}) time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  18944,   18981,   18992,   19062,   19069,   37777,   89076,\n",
      "        125511,  137608,  138261,  138296,  138305,  138349,  144502,\n",
      "        189104,  249691,  249831,  250054,  250067,  250093,  250161,\n",
      "        250187,  250247,  250275,  250386,  251335,  251354,  251414,\n",
      "        251473,  251505,  251559,  251569,  251894,  253602,  254562,\n",
      "        256120,  256224,  256397,  331441,  360984,  439241,  439245,\n",
      "        439300,  439305,  464727,  464756,  464778,  522492,  649908,\n",
      "        695780,  695829,  695989,  696181,  696460,  696839,  696916,\n",
      "        697014,  863902,  863909,  865540,  865558,  877577,  877619,\n",
      "        877706,  889653,  997085, 1014338, 1030313, 1052840, 1052849,\n",
      "       1054274, 1077178, 1087042, 1088300, 1088332, 1088919, 1107306,\n",
      "       1130649, 1139762])]\n"
     ]
    }
   ],
   "source": [
    "token_str = 'dog'\n",
    "token_sequence, index_id = brown.tokenize(token_str, simple_indexing=True)\n",
    "token_positions = brown.get_token_positions(token_sequence, index_id)\n",
    "print(token_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "token_str = 'dog'\n",
    "token_sequence, index_id = toy.tokenize(token_str, simple_indexing=True)\n",
    "token_positions = toy.get_token_positions(token_sequence, index_id)\n",
    "assert np.array_equal(token_positions[0], np.array([109, 123, 137])) # validated using toy.token_ids_to_tokens(toy.get_tokens_by_index('orth_index')[100:-100])\n",
    "assert np.array_equal(toy.token_ids_to_tokens(toy.get_tokens_by_index('orth_index')[token_positions[0]]), np.array(['dog', 'dog', 'dog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _shift_zeroes_to_end(self:Corpus,\n",
    "\t\t\t\t\t\tarr:np.ndarray # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Move 0 value positions for punctuation and space removal \"\"\"\n",
    "\tresult = np.empty_like(arr)\n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tmask = col_data != 0\n",
    "\t\tresult[:mask.sum(), col] = col_data[mask]\n",
    "\t\tresult[mask.sum():, col] = 0\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _zero_after_value(self:Corpus,\n",
    "\t\t\t\t\t  arr:np.ndarray, # Numpy array of collocate frequencies to process\n",
    "\t\t\t\t\t  target: int # Target value to find in the array (e.g., an end-of-file token or a specific collocate frequency)\n",
    "\t\t\t\t\t  ):\n",
    "\t\"\"\" Set values from first occurence of target value to 0 in each column (for processing tokens outside text using eof token) \"\"\"\n",
    "\tarr = arr.copy()  \n",
    "\tfor col in range(arr.shape[1]):\n",
    "\t\tcol_data = arr[:, col]\n",
    "\t\tidx = np.where(col_data == target)[0]\n",
    "\t\tif idx.size > 0:\n",
    "\t\t\tfirst_idx = idx[0]\n",
    "\t\t\tarr[first_idx:, col] = 0\n",
    "\treturn arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_tokens_in_context(self:Corpus,\n",
    "\t\t\t\t\t\t\t   token_positions:np.ndarray, # Numpy array of token positions in the corpus\n",
    "\t\t\t\t\t\t\t   index:str, # Index to use - lower_index, orth_index\n",
    "\t\t\t\t\t\t\t   context_length:int = 5, # Number of context words to consider on each side of the token\n",
    "\t\t\t\t\t\t\t   position_offset:int = 1, # offset to start retrieving context words - negatve is left of node, positive for right - may want to adjust if sequence_len > 1\n",
    "\t\t\t\t\t\t\t   position_offset_step:int = 1, # step to move position offset by, this sets direct, -1 for left, 1 for right\n",
    "\t\t\t\t\t\t\t   exclude_punctuation:bool = True, # ignore punctuation from context retrieved\n",
    "\t\t\t\t\t\t\t   exclude_spaces:bool = True, # ignore spaces from context retrieved\n",
    "\t\t\t\t\t\t\t   convert_eof:bool = True # if True (for collocation functionality), contexts with end of file tokens will have eof token and tokens after set to zero, otherwise EOF retained (e.g. False used for ngrams)\n",
    "\t\t\t\t\t\t\t   ) -> Result:\n",
    "\t\"\"\" Get tokens in context for given token positions, context length and direction, operates one side at a time. \"\"\"\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tif context_length < 1:\n",
    "\t\t# return empty result\n",
    "\t\treturn np.zeros((0, 0), dtype=np.int32)\n",
    "\n",
    "\ttokens_for_removal = []\n",
    "\tif exclude_punctuation:\n",
    "\t\ttokens_for_removal += self.punct_tokens\n",
    "\tif exclude_spaces:\n",
    "\t\ttokens_for_removal += self.space_tokens\n",
    "\tlen_tokens_for_removal = len(tokens_for_removal)\n",
    "\n",
    "\tcollected = False\n",
    "\tcontext_tokens_arr = []\n",
    "\twhile collected == False:\n",
    "\t\tnew_positions = np.array(token_positions[0] + position_offset, dtype = token_positions[0].dtype)\n",
    "\t\tcontext_tokens_arr.append(self.get_tokens_by_index(index)[new_positions])\n",
    "\t\tposition_offset += position_offset_step\n",
    "\t\tif len(context_tokens_arr) >= context_length: \n",
    "\t\t\tcontext_tokens = np.array(context_tokens_arr, dtype = token_positions[0].dtype)\n",
    "\t\t\tlogger.info(f\"Context tokens collected: {context_tokens.shape}\")\n",
    "\t\t\tif len_tokens_for_removal > 0: # cleaning spaces and punctuation and check if need more iterations\n",
    "\t\t\t\tcontext_tokens = np.where(np.isin(context_tokens, tokens_for_removal), 0, context_tokens)\n",
    "\t\t\tcounts = np.count_nonzero(context_tokens, axis=0)\n",
    "\t\t\tif np.min(counts) < context_length:\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\tcollected = True\n",
    "\n",
    "\tcontext_tokens = self._shift_zeroes_to_end(context_tokens)\n",
    "\tcontext_tokens = context_tokens[:context_length, :]\n",
    "\n",
    "\tif convert_eof: # delete any context that contains self.EOF_TOKEN\n",
    "\t\tif self.EOF_TOKEN in context_tokens:\n",
    "\t\t\tcontext_tokens = self._zero_after_value(context_tokens, self.EOF_TOKEN)\n",
    "\n",
    "\tlogger.info(f\"Context retrieved in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\treturn context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
