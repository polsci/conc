{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus\n",
    "\n",
    "> Create a conc corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# requirements - numpy pandas polars spacy nltk great_tables\n",
    "# dev requirements - nbdev, jupyterlab, memory_profiler\n",
    "# TODO check\n",
    "\n",
    "import re\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from great_tables import GT\n",
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "from spacy.attrs import ORTH, LOWER # TODO - add ENT_TYPE, ENT_IOB?\n",
    "import sys\n",
    "import pickle\n",
    "import string\n",
    "from fastcore.basics import patch\n",
    "import time\n",
    "from slugify import slugify\n",
    "\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc import __version__\n",
    "from conc.core import logger, set_logger_state, PAGE_SIZE, EOF_TOKEN_STR\n",
    "from conc.result import Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "polars_conf = pl.Config.set_tbl_hide_column_data_types(True)\n",
    "polars_conf = pl.Config.set_tbl_hide_dataframe_shape(True)\n",
    "polars_conf = pl.Config.set_tbl_rows(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "_RE_PUNCT = re.compile(r\"^[^\\s^\\w^\\d]$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# first release will support english and spacy as a backend to parse the text - support for other languages and backends will come later.\n",
    "try:\n",
    "\tnlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "\tlogger.error('Error loading model en_core_web_sm. You probably need to run python -m spacy download en_core_web_sm to download the model.')\t\n",
    "\t# download\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "EOF_TOKEN = nlp.vocab[EOF_TOKEN_STR].orth # starts with space so eof_token can't match anything from corpus\n",
    "NOT_DOC_TOKEN = -1\n",
    "INDEX_HEADER_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Corpus:\n",
    "\t\"\"\"Represention of text corpus, with methods to build, load and save a corpus from a variety of formats and to work with the corpus data.\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, \n",
    "\t\t\t\tname: str|None = None, # name of corpus\n",
    "\t\t\t\tdescription: str|None = None # description of corpus\n",
    "\t\t\t\t):\n",
    "\t\t# information about corpus\n",
    "\t\tself.name = name\n",
    "\t\tself.description = description\n",
    "\t\tself.slug = None\n",
    "\n",
    "\t\t# conc version that built the corpus\n",
    "\t\tself.conc_version = None\n",
    "\t\t\n",
    "\t\t# paths\n",
    "\t\tself.corpus_path = None\n",
    "\t\tself.source_path = None\n",
    "\n",
    "\t\t# settings\n",
    "\t\tself.EOF_TOKEN = None\n",
    "\n",
    "\t\t# special token ids\n",
    "\t\tself.punct_tokens = None\n",
    "\t\tself.space_tokens = None\n",
    "\n",
    "\t\t# metadata for corpus\n",
    "\t\tself.document_count = None\n",
    "\t\tself.token_count = None\n",
    "\t\tself.unique_tokens = None\n",
    "\n",
    "\t\tself.word_token_count = None\n",
    "\t\tself.unique_word_tokens = None\n",
    "\n",
    "\t\t# token data\n",
    "\t\tself.orth_index = None\n",
    "\t\tself.lower_index = None\n",
    "\n",
    "\t\t# lookup mapping doc_id to every token in doc\n",
    "\t\tself.token2doc_index = None\n",
    "\n",
    "\t\t# lookups to get token string or frequency \n",
    "\t\tself.vocab = None\n",
    "\t\tself.frequency_lookup = None\n",
    "\n",
    "\t\t# offsets for each document in token data\n",
    "\t\tself.offsets = None\n",
    "\n",
    "\t\t# punct and space positions in token data\n",
    "\t\tself.punct_positions = None\n",
    "\t\tself.space_positions = None\n",
    "\n",
    "\t\t# metadata for each document\n",
    "\t\tself.metadata = []\n",
    "\n",
    "\t\t# lookups to get spacy tokenizer or internal ids\n",
    "\t\tself.original_to_new = None\n",
    "\t\tself.new_to_original = None\n",
    "\t\t\n",
    "\t\t# temporary data used when processing text, not saved to disk permanently on save\n",
    "\t\tself.frequency_table = None\n",
    "\t\tself.ngram_index = {}\n",
    "\t\tself.results_cache = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, load and save a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load(self: Corpus, \n",
    "\t\t corpus_path: str # path to load corpus\n",
    "\t\t ):\n",
    "\t\"\"\" Load corpus from disk. \"\"\"\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tif not os.path.isfile(corpus_path):\n",
    "\t\traise FileNotFoundError(f\"Path '{corpus_path}' is not a file\")\n",
    "\tnpz = np.load(corpus_path)\n",
    "\tdata = pickle.loads(npz['corpus'])\n",
    "\tfor k, v in data.items():\n",
    "\t\tsetattr(self, k, v)\n",
    "\tself.orth_index = npz['orth_index']\n",
    "\tself.lower_index = npz['lower_index']\n",
    "\tself.token2doc_index = npz['token2doc_index']\n",
    "\tself.offsets = npz['offsets']\n",
    "\n",
    "\tself.punct_tokens = npz['punct_tokens']\n",
    "\tself.space_tokens = npz['space_tokens']\n",
    "\tself.punct_positions = npz['punct_positions']\n",
    "\tself.space_positions = npz['space_positions']\n",
    "\n",
    "\tself.corpus_path = corpus_path\n",
    "\tlogger.info(f'Load time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save(self: Corpus, \n",
    "\t\t corpus_path: str # path to save corpus\n",
    "\t\t ):\n",
    "\t\"\"\" Save corpus to disk. \"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tif not os.path.isdir(os.path.dirname(corpus_path)):\n",
    "\t\tos.makedirs(os.path.dirname(corpus_path))\n",
    "\tself.corpus_path = corpus_path\n",
    "\tcorpus_bytes = pickle.dumps({k: getattr(self, k) for k in ['name', 'description', 'slug', 'conc_version', 'metadata', 'vocab', 'frequency_lookup', 'original_to_new', 'new_to_original', 'document_count', 'token_count', 'unique_tokens', 'word_token_count', 'unique_word_tokens', 'source_path', 'EOF_TOKEN']})\n",
    "\twith open(corpus_path, 'wb') as f:\n",
    "\t\tnp.savez_compressed(f, corpus=corpus_bytes, orth_index=self.orth_index, lower_index=self.lower_index, token2doc_index=self.token2doc_index, offsets=self.offsets, punct_tokens=self.punct_tokens, space_tokens=self.space_tokens, punct_positions=self.punct_positions, space_positions=self.space_positions)\n",
    "\tlogger.info(f'Save time: {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_build_process(self:Corpus, \n",
    "                        build_process_path\n",
    "                        ):# -> tuple[zarr.Array, zarr.Array, zarr.Array]:\n",
    "    \"\"\" Create the Zarr disk store for build process. \"\"\"\n",
    "    if not os.path.isdir(build_process_path):\n",
    "        os.makedirs(build_process_path)\n",
    "    # create new dir {self.slug}\n",
    "    if not os.path.isdir(f'{build_process_path}/{self.slug}'):\n",
    "        os.makedirs(f'{build_process_path}/{self.slug}')\n",
    "    # self.working_identifier = slugify(self.name)\n",
    "    # orth_store = zarr.create_array(f'{build_process_path}/{working_identifier}_orth_index.zarr', overwrite=True, shape=(1,), chunks=(1000000,), dtype=np.uint64, compressors=zarr.codecs.BloscCodec(cname='zstd', clevel=3, shuffle=zarr.codecs.BloscShuffle.shuffle))\n",
    "    # lower_store = zarr.create_array(f'{build_process_path}/{working_identifier}_lower_index.zarr', overwrite=True, shape=(1,), chunks=(1000000,), dtype=np.uint64, compressors=zarr.codecs.BloscCodec(cname='zstd', clevel=3, shuffle=zarr.codecs.BloscShuffle.shuffle))\n",
    "    # token2doc_store = zarr.create_array(f'{build_process_path}/{working_identifier}_token2doc_index.zarr', overwrite=True, shape=(1,), chunks=(1000000,), dtype=np.int32, compressors=zarr.codecs.BloscCodec(cname='zstd', clevel=3, shuffle=zarr.codecs.BloscShuffle.shuffle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _update_build_process(self: Corpus, \n",
    "                           build_process_path: str, # path to build process\n",
    "                           orth_index: list[np.ndarray],\n",
    "                           lower_index: list[np.ndarray],\n",
    "                           token2doc_index: list[np.ndarray],\n",
    "                           store_pos: int # current store pos\n",
    "                        #    orth_store: zarr.Array,\n",
    "                        #    lower_store: zarr.Array,\n",
    "                        #    token2doc_store: zarr.Array\n",
    "                           ) -> int: # next store pos\n",
    "    \"\"\" Write build data to Zarr disk store. \"\"\"\n",
    "\n",
    "    # orth_index = np.concatenate(orth_index)\n",
    "    # lower_index = np.concatenate(lower_index)\n",
    "    # token2doc_index = np.concatenate(token2doc_index)\n",
    "    # chunk_length = len(lower_index)\n",
    "\n",
    "    # orth_store.resize((store_length + chunk_length,))\n",
    "    # lower_store.resize((store_length + chunk_length,))\n",
    "    # token2doc_store.resize((store_length + chunk_length,))\n",
    "\n",
    "    # orth_store[store_length:] = orth_index\n",
    "    # lower_store[store_length:] = lower_index\n",
    "    # token2doc_store[store_length:] = token2doc_index\n",
    "\n",
    "    # store_length += chunk_length\n",
    "\n",
    "    pl.DataFrame([np.concatenate(orth_index), np.concatenate(lower_index), np.concatenate(token2doc_index)], schema = [('orth_index', pl.UInt64), ('lower_index', pl.UInt64), ('token2doc_index', pl.Int32)] ).write_parquet(f'{build_process_path}/{self.slug}/build_{store_pos}.parquet')\n",
    "\n",
    "    #logger.info(f'Progress: {doc_order} documents with {store_length} index length written to disk')\n",
    "    return store_pos + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _process_punct_positions(self: Corpus):\n",
    "\t\"\"\" Process punct positions in token data. \"\"\"\n",
    "\tself.punct_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip(string.punctuation) == ''}.keys()))\n",
    "\t# faster to retrieve with isin than where\n",
    "\tpunct_mask = np.isin(self.lower_index, self.punct_tokens) \n",
    "\t# storing this as smaller\n",
    "\tself.punct_positions = np.nonzero(punct_mask)[0]\n",
    "\n",
    "# Spacy includes space tokens in the vocab for non-destructive tokenisation, storing positions so can filter them out \n",
    "# for processing and analysis.\n",
    "\n",
    "@patch\n",
    "def _process_space_positions(self: Corpus):\n",
    "\t\"\"\" Process space positions in token data. \"\"\"\n",
    "\tself.space_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip() == ''}.keys()))\n",
    "\t# faster to retrieve with isin than where\n",
    "\tspace_mask = np.isin(self.lower_index, self.space_tokens) \n",
    "\t# storing this as smaller\n",
    "\tself.space_positions = np.nonzero(space_mask)[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _complete_build_process(self: Corpus, \n",
    "                            build_process_path: str # path to build process\n",
    "                            ):\n",
    "\t\"\"\" File-based build to create internal representation of the corpus for faster analysis and efficient representation on disk. \"\"\"\n",
    "\thandler = 'psutil'\n",
    "\tlogger.memory_usage('init', init=True)\n",
    "\tinput_df = pl.scan_parquet(f'{build_process_path}/{self.slug}/build_*.parquet')\n",
    "\t# combining indexes to reindex\n",
    "\tcombined_df = pl.concat([input_df.select(pl.col('orth_index').alias('index')), input_df.select(pl.col('lower_index').alias('index'))])\n",
    "\tvocab_df  = combined_df.select(pl.col('index').unique().sort().alias('source_id')).with_row_index('token_id', offset=1).collect()\n",
    "\tlogger.memory_usage('collected vocab')\n",
    "\tcombined_df = (combined_df.with_columns(pl.col('index').replace(vocab_df.select(pl.col('source_id'))['source_id'], vocab_df.select(pl.col('token_id'))['token_id']).cast(pl.UInt32)))\n",
    "\tcombined_df = combined_df.with_columns(pl.col('index').cast(pl.UInt32))\n",
    "\tinput_length = input_df.select(pl.col('lower_index')).count().collect().item()\n",
    "\tlogger.memory_usage(f'got input length {input_length}')\n",
    "\ttokens_df = pl.concat(\n",
    "\t\t\t\t\t\t\t\t\t[combined_df.select(pl.col('index').alias('orth_index')).slice(0, input_length), \n",
    "\t\t\t\t\t\t\t\t\tcombined_df.select(pl.col('index').alias('lower_index')).slice(input_length),\n",
    "\t\t\t\t\t\t\t\t\tinput_df.select(pl.col('token2doc_index'))], how='horizontal'\n",
    "\t\t\t\t\t\t\t)\n",
    "\tdel combined_df\n",
    "\tdel input_df\n",
    "\tlogger.memory_usage('freed up combined_df and input_df')\n",
    "\n",
    "\tvocab = {k:nlp.vocab[k].text for k in vocab_df['source_id'].to_list()}\n",
    "\ttoken_strs = list(vocab.values())\n",
    "\tvocab_df = vocab_df.with_columns(pl.Series(token_strs).alias('token'))\n",
    "\tlogger.memory_usage('added vocab strings')\n",
    "\n",
    "\tself.EOF_TOKEN = vocab_df.filter(pl.col('source_id') == EOF_TOKEN)['token_id'][0] \n",
    "\n",
    "\t# lower_index = tokens_df.select(pl.col('lower_index')).collect().to_numpy() # improve to not use lower_index and build lazily\n",
    "\t# logger.memory_usage('collected lower index')\n",
    "\t\n",
    "\t# self.punct_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip(string.punctuation) == '']\n",
    "\t# self.space_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip() == '']\n",
    "\t# self.punct_positions = np.nonzero(np.isin(lower_index, self.punct_tokens))[0] # improve to not use lower_index and build lazily\n",
    "\t# self.space_positions = np.nonzero(np.isin(lower_index, self.space_tokens))[0] # improve to not use lower_index\n",
    "\t# del lower_index\n",
    "\t# logger.memory_usage('freed up lower index')\n",
    "\n",
    "\tself.punct_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip(string.punctuation) == '']\n",
    "\tself.space_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip() == '']\n",
    "\n",
    "\tlower_index_lf = tokens_df.select(pl.col('lower_index'))\n",
    "\tlogger.memory_usage('built lower index')\n",
    "\t# Create LazyFrames for punct_positions and space_positions\n",
    "\tself.punct_positions = lower_index_lf.with_row_index('position').filter(pl.col('lower_index').is_in(self.punct_tokens)).select('position').collect().to_numpy().flatten()\n",
    "\tlogger.memory_usage('got punct positions' + str(self.punct_positions.shape))\n",
    "\tself.space_positions = lower_index_lf.with_row_index('position').filter(pl.col('lower_index').is_in(self.space_tokens)).select('position').collect().to_numpy().flatten()\n",
    "\tlogger.memory_usage('got space positions' + str(self.space_positions.shape))\n",
    "\tdel lower_index_lf\n",
    "\tlogger.memory_usage('freed up lower index')\n",
    "\n",
    "\t# get counts from tokens_df\n",
    "\tfrequency_lower = tokens_df.filter(pl.col('lower_index') != self.EOF_TOKEN).select(pl.col('lower_index')).group_by('lower_index').agg(pl.count('lower_index').alias('frequency_lower')).collect()\n",
    "\tself.unique_tokens = len(frequency_lower) # could move this to query to reduce memory - dont need frequency_lower, frequency_orth - abstract?\n",
    "\tfrequency_orth = tokens_df.filter(pl.col('orth_index') != self.EOF_TOKEN).select(pl.col('orth_index')).group_by('orth_index').agg(pl.count('orth_index').alias('frequency_orth')).collect()\n",
    "\tvocab_df = vocab_df.join(frequency_lower, left_on = 'token_id', right_on = 'lower_index', how='left').join(frequency_orth, left_on = 'token_id', right_on = 'orth_index', how='left')\n",
    "\tlogger.memory_usage('added frequency to vocab')\n",
    "\n",
    "\t# add column for is_punct and is_space based on punct_tokens and space_tokens and token_id\n",
    "\tvocab_df = vocab_df.with_columns(pl.Series(np.isin(vocab_df['token_id'], self.punct_tokens)).alias('is_punct'))\n",
    "\tvocab_df = vocab_df.with_columns(pl.Series(np.isin(vocab_df['token_id'], self.space_tokens)).alias('is_space'))\n",
    "\tlogger.memory_usage('added is_punct is_space to vocab')\n",
    "\n",
    "\ttokens_df.collect().write_parquet(f'{build_process_path}/{self.slug}/tokens.parquet')\n",
    "\tvocab_df.write_parquet(f'{build_process_path}/{self.slug}/vocab.parquet')\n",
    "\tlogger.memory_usage('wrote tokens and vocab to disk')\n",
    "\n",
    "\t# reduce memory usage of this line ...\n",
    "\tself.document_count = tokens_df.select(pl.col('token2doc_index').filter(pl.col('token2doc_index') != NOT_DOC_TOKEN).unique().count()).collect().item()\n",
    "\tlogger.memory_usage(f'got doc count {self.document_count}')\n",
    "\t# adjusting for text breaks and jeaders at start and end of index\n",
    "\tself.token_count = input_length - self.document_count - INDEX_HEADER_LENGTH - INDEX_HEADER_LENGTH \n",
    "\tlogger.memory_usage('got token count')\n",
    "\n",
    "\tself.word_token_count = self.token_count - len(self.punct_positions) - len(self.space_positions)\n",
    "\tself.unique_word_tokens = self.unique_tokens - len(self.punct_tokens) - len(self.space_tokens)\n",
    "\n",
    "\tdel tokens_df\n",
    "\tlogger.memory_usage('freed tokens_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_process_path = '../conc-build-process'\n",
    "# slug = 'us-congressional-speeches-subset-100k'\n",
    "\n",
    "# input_df = pl.scan_parquet(f'{build_process_path}/{slug}/build_*.parquet')\n",
    "\n",
    "# # combining indexes to reindex\n",
    "# combined_df = pl.concat([input_df.select(pl.col('orth_index').alias('index')), input_df.select(pl.col('lower_index').alias('index'))])\n",
    "\n",
    "# vocab_df  = combined_df.select(pl.col('index').unique().sort().alias('source_id')).with_row_index('token_id', offset=1).collect()\n",
    "\n",
    "# combined_df = (combined_df.with_columns(pl.col('index').replace(vocab_df.select(pl.col('source_id'))['source_id'], vocab_df.select(pl.col('token_id'))['token_id']).cast(pl.UInt32)))\n",
    "# combined_df = combined_df.with_columns(pl.col('index').cast(pl.UInt32))\n",
    "\n",
    "# split_point = input_df.collect().height\n",
    "\n",
    "# tokens_df = pl.concat(\n",
    "#                                 [combined_df.select(pl.col('index').alias('orth_index')).slice(0, split_point), \n",
    "#                                 combined_df.select(pl.col('index').alias('lower_index')).slice(split_point),\n",
    "#                                 input_df.select(pl.col('token2doc_index'))], how='horizontal'\n",
    "#                         )\n",
    "\n",
    "# del combined_df\n",
    "# del input_df\n",
    "\n",
    "# vocab = {k:nlp.vocab[k].text for k in vocab_df['source_id'].to_list()}\n",
    "# token_strs = list(vocab.values())\n",
    "# vocab_df = vocab_df.with_columns(pl.Series(token_strs).alias('token'))\n",
    "\n",
    "# corpus_eof_token = vocab_df.filter(pl.col('source_id') == EOF_TOKEN)['token_id'][0] # TODO change var name\n",
    "# print(corpus_eof_token)\n",
    "\n",
    "# tokens_df.collect().write_parquet(f'{build_process_path}/{slug}/tokens.parquet')\n",
    "# lower_index = tokens_df.select(pl.col('lower_index')).collect().to_numpy()\n",
    "# punct_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip(string.punctuation) == ''] # TODO change var name\n",
    "# space_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip() == ''] # TODO change var name\n",
    "# punct_mask = np.isin(lower_index, punct_tokens)\n",
    "# punct_positions = np.nonzero(punct_mask)[0] # TODO change var name\n",
    "# space_mask = np.isin(lower_index, space_tokens)\n",
    "# space_positions = np.nonzero(space_mask)[0] # TODO change var name\n",
    "\n",
    "# # get counts from tokens_df\n",
    "# frequency_lower = tokens_df.filter(pl.col('lower_index') != corpus_eof_token).select(pl.col('lower_index')).group_by('lower_index').agg(pl.count('lower_index').alias('frequency_lower')).collect()\n",
    "# frequency_orth = tokens_df.filter(pl.col('orth_index') != corpus_eof_token).select(pl.col('orth_index')).group_by('orth_index').agg(pl.count('orth_index').alias('frequency_orth')).collect()\n",
    "# vocab_df = vocab_df.join(frequency_lower, left_on = 'token_id', right_on = 'lower_index', how='left').join(frequency_orth, left_on = 'token_id', right_on = 'orth_index', how='left')\n",
    "\n",
    "# # add column for is_punct and is_space based on punct_tokens and space_tokens and token_id\n",
    "# vocab_df = vocab_df.with_columns(pl.Series(np.isin(vocab_df['token_id'], punct_tokens)).alias('is_punct'))\n",
    "# vocab_df = vocab_df.with_columns(pl.Series(np.isin(vocab_df['token_id'], space_tokens)).alias('is_space'))\n",
    "\n",
    "# display(vocab_df.filter(pl.col('token') == '.'))\n",
    "\n",
    "# vocab_df.write_parquet(f'{build_process_path}/{slug}/vocab.parquet')\n",
    "\n",
    "# # get lines where frequncy_lower not null\n",
    "# #display(vocab_df.filter(pl.col('frequency_lower').is_not_null()))\n",
    "\n",
    "# del lower_index, tokens_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = corpora['us-congressional-speeches-subset-100k']\n",
    "# print(corpus.token_to_id('-'))\n",
    "# print(corpus.frequency_lookup[corpus.token_to_id('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = Corpus('us-congressional-speeches-subset-10k')\n",
    "# corpus.slug = slugify(corpus.name)\n",
    "# corpus._complete_build_process('../conc-build-process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _create_indices(self: Corpus, \n",
    "\t\t\t\t   orth_index: list[np.ndarray], # list of np arrays of orth token ids \n",
    "\t\t\t\t   lower_index: list[np.ndarray], # list of np arrays of lower token ids\n",
    "\t\t\t\t   token2doc_index: list[np.ndarray] # list of np arrays of doc ids\n",
    "\t\t\t\t   ):\n",
    "\t\"\"\" Use Numpy to create internal representation of the corpus for faster analysis and efficient representation on disk. \"\"\"\n",
    "\n",
    "\tself.token2doc_index = np.concatenate(token2doc_index)\n",
    "\tunique_values, inverse = np.unique(np.concatenate(orth_index + lower_index), return_inverse=True)\n",
    "\n",
    "\t# add a dummy value at the 0 index to avoid 0 being used as a token id\n",
    "\tunique_values = np.insert(unique_values, 0, 0)\n",
    "\tinverse += 1\n",
    "\tnew_values = np.arange(len(unique_values), dtype=np.uint32)\n",
    "\tself.original_to_new = dict(zip(unique_values, new_values))\n",
    "\tself.new_to_original = dict(zip(new_values, unique_values))\n",
    "\n",
    "\tself.orth_index = np.array(np.split(inverse, 2)[0], dtype=np.uint32)\n",
    "\tself.lower_index = np.array(np.split(inverse, 2)[1], dtype=np.uint32)\n",
    "\tdel inverse\n",
    "\n",
    "\tvocab = {k:nlp.vocab.strings[k] for k in unique_values}\n",
    "\tvocab[0] = 'ERROR: not a token'\n",
    "\n",
    "\tself.vocab = {**{k:vocab[self.new_to_original[k]] for k in new_values}}\n",
    "\n",
    "\tself.EOF_TOKEN = self.original_to_new[EOF_TOKEN]\n",
    "\n",
    "\tself._process_punct_positions()\n",
    "\tself._process_space_positions()\n",
    "\n",
    "\tself.frequency_lookup = dict(zip(*np.unique(self.lower_index, return_counts=True)))\n",
    "\tdel self.frequency_lookup[self.EOF_TOKEN]\n",
    "\tdel unique_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build(self: Corpus, \n",
    "\t\t  iterator: iter, # iterator of texts\n",
    "\t\t  batch_size:int=1000, # batch size for spacy tokenizer\n",
    "\t\t  build_process_path:str|None=None, # path to save an in-progress build to disk to reduce memory usage, default of None disables \n",
    "\t\t  build_process_batch_size:int=5000 # save in-progress build to disk every n docs\n",
    "\t\t  ):\n",
    "\t\"\"\"Build a corpus from an iterator of texts.\"\"\"\n",
    "\t\n",
    "\t# TODO - add a progress indicator\n",
    "\tself.conc_version = __version__\n",
    "\tself.slug = slugify(self.name)\n",
    "\tlogger.memory_usage('init', init=True)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\teof_arr = np.array([EOF_TOKEN], dtype=np.uint64)\n",
    "\tnot_doc_arr = np.array([NOT_DOC_TOKEN], dtype=np.int16)\n",
    "\tindex_header_arr = np.array([EOF_TOKEN] * INDEX_HEADER_LENGTH, dtype=np.uint64) # this is added to start and end of index to prevent out of bound issues on searches\n",
    "\n",
    "\torth_index = [index_header_arr]\n",
    "\tlower_index = [index_header_arr]\n",
    "\ttoken2doc_index = [np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32)]\n",
    "\n",
    "\toffset = INDEX_HEADER_LENGTH\n",
    "\tself.offsets = [] # TODO - check that this is being used  - consider removing\n",
    "\n",
    "\tif build_process_path is not None:\n",
    "\t\tself._init_build_process(build_process_path)\n",
    "\t\t\n",
    "\tstore_pos = 0\n",
    "\n",
    "\tdoc_order = 0\n",
    "\tfor doc in nlp.tokenizer.pipe(iterator, batch_size=batch_size): # test varying this TODO\n",
    "\t\t#TODO  - as corpus size increases memory requirements will increase - consider buffering orth_index, lower_index, token2doc_index and writing to disk periodically\n",
    "\t\torth_index.append(doc.to_array(ORTH))\n",
    "\t\torth_index.append(eof_arr)\n",
    "\n",
    "\t\tlower_index_tmp = doc.to_array(LOWER)\n",
    "\t\tlower_index.append(lower_index_tmp)\n",
    "\t\tlower_index.append(eof_arr)\n",
    "\n",
    "\t\ttoken2doc_index.append(np.array([doc_order] * len(lower_index_tmp), dtype=np.int32))\n",
    "\t\ttoken2doc_index.append(not_doc_arr)\n",
    "\n",
    "\t\tself.offsets.append(offset) \n",
    "\t\toffset = offset + len(lower_index_tmp) + 1\n",
    "\t\tdoc_order += 1\n",
    "\n",
    "\t\t# update store every build_process_batch_size docs\n",
    "\t\tif build_process_path is not None and doc_order % build_process_batch_size == 0:\n",
    "\t\t\tstore_pos = self._update_build_process(build_process_path, orth_index, lower_index, token2doc_index, store_pos)\n",
    "\t\t\tlower_index, orth_index, token2doc_index = [], [], []\n",
    "\t\t\n",
    "\t\tif doc_order % 5000 == 0:\n",
    "\t\t\tlogger.memory_usage(f'processed {doc_order} documents - pre gc')\n",
    "\t\t\tgc.collect()\n",
    "\t\t\tlogger.memory_usage(f'processed {doc_order} documents - post gc')\n",
    "\t\t\t\n",
    "\tdel iterator\n",
    "\torth_index.append(index_header_arr)\n",
    "\tlower_index.append(index_header_arr)\n",
    "\ttoken2doc_index.append(np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32))\n",
    "\n",
    "\tlogger.memory_usage(f'pre-complete-build-process')\n",
    "\tif build_process_path is not None:\n",
    "\t\tstore_pos = self._update_build_process(build_process_path, orth_index, lower_index, token2doc_index, store_pos)\n",
    "\t\t# orth_index = [orth_store[:]]\n",
    "\t\t# lower_index = [lower_store[:]]\n",
    "\t\t# token2doc_index = [token2doc_store[:]]\n",
    "\n",
    "\t\t# del lower_store\n",
    "\t\t# del orth_store\n",
    "\t\t# del token2doc_store\n",
    "\t\tself._complete_build_process(build_process_path)\n",
    "\telse:\n",
    "\t\tself._create_indices(orth_index, lower_index, token2doc_index)\n",
    "\t\tself.document_count = len(self.offsets)\n",
    "\t\t# adjusting for text breaks and jeaders at start and end of index\n",
    "\t\tself.token_count = self.lower_index.shape[0] - self.document_count - len(index_header_arr) - len(index_header_arr) \n",
    "\t\tself.unique_tokens = len(self.frequency_lookup)\n",
    "\n",
    "\t\tself.word_token_count = self.token_count - len(self.punct_positions) - len(self.space_positions)\n",
    "\t\tself.unique_word_tokens = len(self.frequency_lookup) - len(self.punct_tokens) - len(self.space_tokens)\n",
    "\tlogger.memory_usage(f'post-complete-build-process')\n",
    "\n",
    "\tdel orth_index\n",
    "\tdel lower_index\n",
    "\tdel token2doc_index\n",
    "\tlogger.memory_usage(f'freed memory')\n",
    "\n",
    "\tlogger.info(f'Build time: {(time.time() - start_time):.3f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _prepare_files(self: Corpus, \n",
    "\t\t\t\t\tsource_path: str, # path to folder with text files \n",
    "\t\t\t\t\tfile_mask:str='*.txt', # mask to select files \n",
    "\t\t\t\t\tmetadata_file: str|None=None, # path to a CSV with metadata\n",
    "\t\t\t\t\tmetadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from metadata\n",
    "\t\t\t\t\tencoding:str='utf8' # encoding of text files\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Prepare text files and metadata for building a corpus. Returns an iterator to get file text for processing.\"\"\"\n",
    "\n",
    "\t# allowing import from zip and tar files\n",
    "\tif os.path.isdir(source_path):\n",
    "\t\tfiles = glob.glob(os.path.join(source_path, file_mask))\n",
    "\t\ttype = 'folder'\n",
    "\telif os.path.isfile(source_path):\n",
    "\t\timport fnmatch\n",
    "\t\tif source_path.endswith('.zip'):\n",
    "\t\t\timport zipfile\n",
    "\t\t\twith zipfile.ZipFile(source_path, 'r') as z:\n",
    "\t\t\t\tfiles = []\n",
    "\t\t\t\tfor f in z.namelist():\n",
    "\t\t\t\t\tif fnmatch.fnmatch(f, file_mask):\n",
    "\t\t\t\t\t\tfiles.append(f)\n",
    "\t\t\t\tif len(files) > 0:\n",
    "\t\t\t\t\ttype = 'zip'\n",
    "\t\telif source_path.endswith('.tar') or source_path.endswith('.tar.gz'):\n",
    "\t\t\timport tarfile\n",
    "\t\t\twith tarfile.open(source_path, 'r') as t:\n",
    "\t\t\t\tfiles = []\n",
    "\t\t\t\tfor f in t.getnames():\n",
    "\t\t\t\t\tif fnmatch.fnmatch(f, file_mask):\n",
    "\t\t\t\t\t\tfiles.append(f)\n",
    "\t\t\t\tif len(files) > 0:\n",
    "\t\t\t\t\ttype = 'tar'\n",
    "\t\telse:\n",
    "\t\t\traise FileNotFoundError(f\"Path '{source_path}' is not a directory, zip or tar file\")\n",
    "\t\n",
    "\tif not files:\n",
    "\t\traise FileNotFoundError(f\"No files matching {file_mask} found in '{source_path}'\")\n",
    "\n",
    "\torder = pl.DataFrame({metadata_file_column: [os.path.basename(p) for p in files]})\n",
    "\n",
    "\tif metadata_file:\n",
    "\t\tif not os.path.isfile(metadata_file):\n",
    "\t\t\traise FileNotFoundError(f\"Metadata file '{metadata_file}' not found\")\n",
    "\t\ttry:\n",
    "\t\t\tmetadata_columns = set([metadata_file_column] + metadata_columns)\n",
    "\t\t\t\n",
    "\t\t\t# ordering metadata based on order of files so token data and metadata aligned\n",
    "\t\t\tmetadata = pl.read_csv(metadata_file).select(metadata_columns)\n",
    "\t\t\tself.metadata = order.join(metadata, on=metadata_file_column, how='left')\n",
    "\t\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\t\traise\n",
    "\telse:\n",
    "\t\tself.metadata = order\n",
    "\n",
    "\tself.source_path = source_path\n",
    "\n",
    "\tif type == 'folder':\n",
    "\t\tfor p in files:\n",
    "\t\t\tyield open(p, \"rb\").read().decode(encoding)\n",
    "\telif type == 'zip':\n",
    "\t\twith zipfile.ZipFile(source_path, 'r') as z:\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tyield z.read(f).decode(encoding)\n",
    "\telif type == 'tar':\n",
    "\t\twith tarfile.open(source_path, 'r') as t:\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tyield t.extractfile(f).read().decode(encoding)\t\t\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_from_files(self: Corpus,\n",
    "\t\t\t\t\tsource_path: str, # path to folder with text files \n",
    "\t\t\t\t\tfile_mask:str='*.txt', # mask to select files \n",
    "\t\t\t\t\tmetadata_file: str|None=None, # path to a CSV with metadata\n",
    "\t\t\t\t\tmetadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from metadata\n",
    "\t\t\t\t\tencoding:str='utf-8', # encoding of text files\n",
    "\t\t\t\t\tbatch_size:int=1000, # batch size for spacy tokenizer\n",
    "\t\t\t\t\tbuild_process_path:str='../conc-build-process', # path to save an in-progress build to disk to reduce memory usage\n",
    "\t\t\t\t\tbuild_process_batch_size:int=5000 # save in-progress build to disk every n docs\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Build a corpus from text files in a folder.\"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\titerator = self._prepare_files(source_path, file_mask, metadata_file, metadata_file_column, metadata_columns, encoding)\n",
    "\tself.build(iterator, batch_size, build_process_path, build_process_batch_size)\n",
    "\tlogger.info(f'Build from files time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test = Corpus('test')\n",
    "texts = []\n",
    "for text in test._prepare_files('../test-corpora/source/toy', file_mask='*1.txt'):\n",
    "\ttexts.append(text)\n",
    "assert len(texts) == 1\n",
    "assert texts[0] == 'The cat sat on the mat.'\n",
    "\n",
    "texts = []\n",
    "for text in test._prepare_files('../test-corpora/source/toy', file_mask='*.txt', metadata_file='../test-corpora/source/toy.csv', metadata_file_column = 'source', metadata_columns=['category']):\n",
    "\ttexts.append(text)\n",
    "\n",
    "assert len(texts) == 6\n",
    "assert 'The cat sat on the mat.' in texts\n",
    "assert test.metadata.shape[0] == 6\n",
    "assert test.metadata.columns == ['source', 'category']\n",
    "\n",
    "cat_sat_index = texts.index('The cat sat on the mat.') \n",
    "assert test.metadata['source'][cat_sat_index] == '1.txt'\n",
    "assert test.metadata['category'][cat_sat_index] == 'feline'\n",
    "\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _prepare_csv(self: Corpus, \n",
    "\t\t\t\t\tsource_path:str, # path to csv file\n",
    "\t\t\t\t\ttext_column:str='text', # column in csv with text\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from csv\n",
    "\t\t\t\t\tencoding:str='utf8' # encoding of csv passed to Polars read_csv, see their documentation\n",
    "\t\t\t\t\t) -> iter: # iterator to return rows for processing\n",
    "\t\"\"\"Prepare to import from CSV, including metadata. Returns an iterator to process the text column.\"\"\"\n",
    "\n",
    "\tif not os.path.isfile(source_path):\n",
    "\t\traise FileNotFoundError(f'Path ({source_path}) is not a file')\n",
    "\t\n",
    "\ttry:\n",
    "\t\tdf = pl.scan_csv(source_path, encoding = encoding).select([text_column] + metadata_columns)\n",
    "\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\traise\n",
    "\n",
    "\tself.source_path = source_path\n",
    "\tself.metadata = df.select(metadata_columns).collect()\n",
    "\n",
    "\tfor slice_df in df.collect().iter_slices(n_rows=5000):  \n",
    "\t\tfor row in slice_df.iter_rows():\n",
    "\t\t\tyield row[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_from_csv(self: Corpus, \n",
    "\t\t\t\t   source_path:str, # path to csv file\n",
    "\t\t\t\t   text_column:str='text', # column in csv with text\n",
    "\t\t\t\t   metadata_columns:list[str]=[], # list of column names to import from csv\n",
    "\t\t\t\t   encoding:str='utf8', # encoding of csv passed to Polars read_csv, see their documentation\n",
    "\t\t\t\t   batch_size:int=1000, # batch size for Spacy tokenizer\n",
    "\t\t\t\t   build_process_path:str=None, # path to save an in-progress build to disk to reduce memory usage\n",
    "\t\t\t\t   build_process_batch_size:int=5000 # save in-progress build to disk every n docs\n",
    "\t\t\t\t   ):\n",
    "\t\"\"\"Build a corpus from a csv file.\"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\titerator = self._prepare_csv(source_path, text_column, metadata_columns, encoding)\n",
    "\tself.build(iterator, batch_size, build_process_path, build_process_batch_size)\n",
    "\tlogger.info(f'Build from csv time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test = Corpus('test')\n",
    "texts = []\n",
    "\n",
    "for text in test._prepare_csv('../test-corpora/source/toy.csv', text_column='text', metadata_columns=['source', 'category']):\n",
    "\ttexts.append(text)\n",
    "\n",
    "assert len(texts) == 6\n",
    "cat_sat_index = 0\n",
    "assert texts[cat_sat_index] == 'The cat sat on the mat.'\n",
    "assert test.metadata.shape[0] == 6\n",
    "assert test.metadata.columns == ['source', 'category']\n",
    "assert test.metadata['source'][cat_sat_index] == '1.txt'\n",
    "assert test.metadata['category'][cat_sat_index] == 'feline'\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = '../test-corpora/source/'\n",
    "save_path = '../test-corpora/saved/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 15:53:38 - INFO - load - Load time: 0.001 seconds\n",
      "2025-03-19 15:53:38 - INFO - load - Load time: 0.143 seconds\n",
      "2025-03-19 15:53:38 - INFO - load - Load time: 0.140 seconds\n",
      "2025-03-19 15:53:38 - INFO - load - Load time: 0.139 seconds\n",
      "2025-03-19 15:53:38 - INFO - load - Load time: 0.010 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "corpora = {}\n",
    "corpora['toy'] = {'name': 'Toy Corpus', 'description': 'Toy corpus for testing', 'extension': '.csv.gz'}\n",
    "corpora['brown'] = {'name': 'Brown Corpus', 'description': 'A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html', 'extension': '.csv.gz'}\n",
    "corpora['reuters'] = {'name': 'Reuters Corpus', 'description': 'From NLTK TODO', 'extension': '.csv.gz'}\n",
    "corpora['gutenberg'] = {'name': 'Gutenberg Corpus', 'description': 'From NLTK TODO', 'extension': '.csv.gz'}\n",
    "corpora['garden-party-corpus'] = {'name': 'Garden Party Corpus', 'description': 'https://github.com/ucdh/scraping-garden-party', 'extension': '.zip'}\n",
    "\n",
    "set_logger_state('verbose')\n",
    "for corpus_name, corpus_details in corpora.items():\n",
    "\ttry:\n",
    "\t\tcorpus = Corpus().load(f'{save_path}{corpus_name}.corpus')\n",
    "\texcept FileNotFoundError:\n",
    "\t\tif 'csv' in corpus_details['extension']:\n",
    "\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_csv(f'{source_path}{corpus_name}.csv.gz', text_column='text', metadata_columns=['source'])\n",
    "\t\telse:\n",
    "\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_files(f'{source_path}{corpus_name}{corpus_details[\"extension\"]}')\n",
    "\t\tcorpus.save(f'{save_path}{corpus_name}.corpus')\n",
    "\texcept Exception as e:\n",
    "\t\traise e\n",
    "set_logger_state('quiet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add tests for build and save and load\n",
    "if os.path.isfile(f'{save_path}/toy.corpus'):\n",
    "\tos.remove(f'{save_path}/toy.corpus')\n",
    "\n",
    "try:\n",
    "\ttoy = Corpus().load(f'{save_path}/toy.corpus')\n",
    "except FileNotFoundError:\n",
    "\ttoy = Corpus(name = corpora['toy']['name'], description = corpora['toy']['description']).build_from_csv(f'{source_path}toy.csv.gz', text_column='text', metadata_columns=['source'])\n",
    "\ttoy.save(f'{save_path}/toy.corpus')\n",
    "except Exception as e:\n",
    "\traise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add tests for build and save and load\n",
    "if os.path.isfile(f'{save_path}/brown.corpus'):\n",
    "\tos.remove(f'{save_path}/brown.corpus')\n",
    "\n",
    "try:\n",
    "\tbrown = Corpus().load(f'{save_path}/brown.corpus')\n",
    "except FileNotFoundError:\n",
    "\tbrown = Corpus(name = corpora['brown']['name'], description = corpora['brown']['description']).build_from_csv(f'{source_path}/brown.csv.gz', text_column='text', metadata_columns=['source'])\n",
    "\tbrown.save(f'{save_path}/brown.corpus')\n",
    "except Exception as e:\n",
    "\traise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 15:53:43 - INFO - memory_usage - init, memory usage: 514.70703125 MB\n",
      "2025-03-19 15:53:45 - INFO - memory_usage - processed 5000 documents - pre gc, memory usage: 532.0703125 MB, difference: 17.36328125 MB\n",
      "2025-03-19 15:53:45 - INFO - memory_usage - processed 5000 documents - post gc, memory usage: 529.453125 MB, difference: -2.6171875 MB\n",
      "2025-03-19 15:53:46 - INFO - memory_usage - processed 10000 documents - pre gc, memory usage: 532.54296875 MB, difference: 3.08984375 MB\n",
      "2025-03-19 15:53:46 - INFO - memory_usage - processed 10000 documents - post gc, memory usage: 532.54296875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:46 - INFO - memory_usage - pre-complete-build-process, memory usage: 532.54296875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:46 - INFO - memory_usage - init, memory usage: 532.54296875 MB\n",
      "2025-03-19 15:53:46 - INFO - memory_usage - collected vocab, memory usage: 581.11328125 MB, difference: 48.5703125 MB\n",
      "2025-03-19 15:53:46 - INFO - memory_usage - got input length 1975172, memory usage: 592.05859375 MB, difference: 10.9453125 MB\n",
      "2025-03-19 15:53:46 - INFO - memory_usage - freed up combined_df and input_df, memory usage: 592.05859375 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - added vocab strings, memory usage: 594.6484375 MB, difference: 2.58984375 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - built lower index, memory usage: 594.6484375 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - got punct positions(187068,), memory usage: 701.63671875 MB, difference: 106.98828125 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - got space positions(10000,), memory usage: 732.19921875 MB, difference: 30.5625 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - freed up lower index, memory usage: 732.19921875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - added frequency to vocab, memory usage: 778.80859375 MB, difference: 46.609375 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - added is_punct is_space to vocab, memory usage: 778.80859375 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - wrote tokens and vocab to disk, memory usage: 817.234375 MB, difference: 38.42578125 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - got doc count 10000, memory usage: 842.828125 MB, difference: 25.59375 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - got token count, memory usage: 842.828125 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - freed tokens_df, memory usage: 842.828125 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - post-complete-build-process, memory usage: 842.828125 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - freed memory, memory usage: 842.828125 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:47 - INFO - build - Build time: 4.056 seconds\n",
      "2025-03-19 15:53:47 - INFO - build_from_csv - Build from csv time: 4.057 seconds\n",
      "2025-03-19 15:53:47 - INFO - memory_usage - init, memory usage: 842.828125 MB\n",
      "2025-03-19 15:53:49 - INFO - memory_usage - processed 5000 documents - pre gc, memory usage: 856.6796875 MB, difference: 13.8515625 MB\n",
      "2025-03-19 15:53:49 - INFO - memory_usage - processed 5000 documents - post gc, memory usage: 856.6796875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:50 - INFO - memory_usage - processed 10000 documents - pre gc, memory usage: 857.578125 MB, difference: 0.8984375 MB\n",
      "2025-03-19 15:53:50 - INFO - memory_usage - processed 10000 documents - post gc, memory usage: 857.578125 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:52 - INFO - memory_usage - processed 15000 documents - pre gc, memory usage: 859.53125 MB, difference: 1.953125 MB\n",
      "2025-03-19 15:53:52 - INFO - memory_usage - processed 15000 documents - post gc, memory usage: 859.53125 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:54 - INFO - memory_usage - processed 20000 documents - pre gc, memory usage: 862.1640625 MB, difference: 2.6328125 MB\n",
      "2025-03-19 15:53:54 - INFO - memory_usage - processed 20000 documents - post gc, memory usage: 862.1640625 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:55 - INFO - memory_usage - processed 25000 documents - pre gc, memory usage: 862.09765625 MB, difference: -0.06640625 MB\n",
      "2025-03-19 15:53:55 - INFO - memory_usage - processed 25000 documents - post gc, memory usage: 862.09765625 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:57 - INFO - memory_usage - processed 30000 documents - pre gc, memory usage: 869.66796875 MB, difference: 7.5703125 MB\n",
      "2025-03-19 15:53:57 - INFO - memory_usage - processed 30000 documents - post gc, memory usage: 869.66796875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:53:59 - INFO - memory_usage - processed 35000 documents - pre gc, memory usage: 889.640625 MB, difference: 19.97265625 MB\n",
      "2025-03-19 15:53:59 - INFO - memory_usage - processed 35000 documents - post gc, memory usage: 889.640625 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:00 - INFO - memory_usage - processed 40000 documents - pre gc, memory usage: 890.12890625 MB, difference: 0.48828125 MB\n",
      "2025-03-19 15:54:00 - INFO - memory_usage - processed 40000 documents - post gc, memory usage: 890.12890625 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:02 - INFO - memory_usage - processed 45000 documents - pre gc, memory usage: 892.1328125 MB, difference: 2.00390625 MB\n",
      "2025-03-19 15:54:02 - INFO - memory_usage - processed 45000 documents - post gc, memory usage: 892.1328125 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:04 - INFO - memory_usage - processed 50000 documents - pre gc, memory usage: 894.21484375 MB, difference: 2.08203125 MB\n",
      "2025-03-19 15:54:04 - INFO - memory_usage - processed 50000 documents - post gc, memory usage: 894.21484375 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:05 - INFO - memory_usage - processed 55000 documents - pre gc, memory usage: 896.49609375 MB, difference: 2.28125 MB\n",
      "2025-03-19 15:54:05 - INFO - memory_usage - processed 55000 documents - post gc, memory usage: 896.49609375 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:07 - INFO - memory_usage - processed 60000 documents - pre gc, memory usage: 900.3515625 MB, difference: 3.85546875 MB\n",
      "2025-03-19 15:54:07 - INFO - memory_usage - processed 60000 documents - post gc, memory usage: 900.3515625 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:09 - INFO - memory_usage - processed 65000 documents - pre gc, memory usage: 902.12109375 MB, difference: 1.76953125 MB\n",
      "2025-03-19 15:54:09 - INFO - memory_usage - processed 65000 documents - post gc, memory usage: 902.12109375 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:10 - INFO - memory_usage - processed 70000 documents - pre gc, memory usage: 911.9765625 MB, difference: 9.85546875 MB\n",
      "2025-03-19 15:54:10 - INFO - memory_usage - processed 70000 documents - post gc, memory usage: 911.9765625 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:12 - INFO - memory_usage - processed 75000 documents - pre gc, memory usage: 913.78125 MB, difference: 1.8046875 MB\n",
      "2025-03-19 15:54:12 - INFO - memory_usage - processed 75000 documents - post gc, memory usage: 913.78125 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:14 - INFO - memory_usage - processed 80000 documents - pre gc, memory usage: 915.5859375 MB, difference: 1.8046875 MB\n",
      "2025-03-19 15:54:14 - INFO - memory_usage - processed 80000 documents - post gc, memory usage: 915.5859375 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:15 - INFO - memory_usage - processed 85000 documents - pre gc, memory usage: 917.390625 MB, difference: 1.8046875 MB\n",
      "2025-03-19 15:54:15 - INFO - memory_usage - processed 85000 documents - post gc, memory usage: 917.390625 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:17 - INFO - memory_usage - processed 90000 documents - pre gc, memory usage: 919.1953125 MB, difference: 1.8046875 MB\n",
      "2025-03-19 15:54:17 - INFO - memory_usage - processed 90000 documents - post gc, memory usage: 919.1953125 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:19 - INFO - memory_usage - processed 95000 documents - pre gc, memory usage: 920.7421875 MB, difference: 1.546875 MB\n",
      "2025-03-19 15:54:19 - INFO - memory_usage - processed 95000 documents - post gc, memory usage: 920.7421875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:20 - INFO - memory_usage - processed 100000 documents - pre gc, memory usage: 922.546875 MB, difference: 1.8046875 MB\n",
      "2025-03-19 15:54:20 - INFO - memory_usage - processed 100000 documents - post gc, memory usage: 922.546875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:20 - INFO - memory_usage - pre-complete-build-process, memory usage: 922.546875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:20 - INFO - memory_usage - init, memory usage: 922.546875 MB\n",
      "2025-03-19 15:54:21 - INFO - memory_usage - collected vocab, memory usage: 1319.83203125 MB, difference: 397.28515625 MB\n",
      "2025-03-19 15:54:21 - INFO - memory_usage - got input length 20127441, memory usage: 1319.4921875 MB, difference: -0.33984375 MB\n",
      "2025-03-19 15:54:21 - INFO - memory_usage - freed up combined_df and input_df, memory usage: 1319.4921875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:21 - INFO - memory_usage - added vocab strings, memory usage: 1371.6171875 MB, difference: 52.125 MB\n",
      "2025-03-19 15:54:21 - INFO - memory_usage - built lower index, memory usage: 1371.6171875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:22 - INFO - memory_usage - got punct positions(1906472,), memory usage: 1986.2578125 MB, difference: 614.640625 MB\n",
      "2025-03-19 15:54:22 - INFO - memory_usage - got space positions(100000,), memory usage: 2206.37890625 MB, difference: 220.12109375 MB\n",
      "2025-03-19 15:54:22 - INFO - memory_usage - freed up lower index, memory usage: 2206.37890625 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:23 - INFO - memory_usage - added frequency to vocab, memory usage: 2403.42578125 MB, difference: 197.046875 MB\n",
      "2025-03-19 15:54:23 - INFO - memory_usage - added is_punct is_space to vocab, memory usage: 2403.42578125 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:24 - INFO - memory_usage - wrote tokens and vocab to disk, memory usage: 2478.09375 MB, difference: 74.66796875 MB\n",
      "2025-03-19 15:54:25 - INFO - memory_usage - got doc count 100000, memory usage: 2578.1796875 MB, difference: 100.0859375 MB\n",
      "2025-03-19 15:54:25 - INFO - memory_usage - got token count, memory usage: 2578.1796875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:25 - INFO - memory_usage - freed tokens_df, memory usage: 2578.1796875 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:25 - INFO - memory_usage - post-complete-build-process, memory usage: 2568.3359375 MB, difference: -9.84375 MB\n",
      "2025-03-19 15:54:25 - INFO - memory_usage - freed memory, memory usage: 2568.3359375 MB, difference: 0.0 MB\n",
      "2025-03-19 15:54:25 - INFO - build - Build time: 37.718 seconds\n",
      "2025-03-19 15:54:25 - INFO - build_from_csv - Build from csv time: 37.719 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  types |   # objects |   total size\n",
      "======================= | =========== | ============\n",
      "                   dict |       49448 |     54.81 MB\n",
      "                    str |      280494 |     27.81 MB\n",
      "          numpy.ndarray |          64 |     22.52 MB\n",
      "                   code |       34250 |     12.85 MB\n",
      "                   type |        5018 |      6.08 MB\n",
      "                   list |       30779 |      4.52 MB\n",
      "                    int |      133965 |      3.62 MB\n",
      "                  tuple |       41347 |      2.58 MB\n",
      "                    set |        1893 |    951.30 KB\n",
      "            abc.ABCMeta |         370 |    564.92 KB\n",
      "  weakref.ReferenceType |        7033 |    549.45 KB\n",
      "              frozenset |         736 |    473.00 KB\n",
      "             re.Pattern |         467 |    437.02 KB\n",
      "    function (__init__) |        2528 |    375.25 KB\n",
      "                 method |        5620 |    351.25 KB\n"
     ]
    }
   ],
   "source": [
    "test_corpora_names = ['us-congressional-speeches-subset-10k',\n",
    "                      'us-congressional-speeches-subset-100k'\n",
    "\t\t\t\t\t  ]\n",
    "\n",
    "\n",
    "from pympler import tracker\n",
    "from pympler import muppy, summary\n",
    "# tr = tracker.SummaryTracker()\n",
    "\n",
    "corpora = {}\n",
    "for name in test_corpora_names:\n",
    "\tset_logger_state('verbose')\n",
    "\t# try:\n",
    "\t# \tcorpora[name] = Corpus().load(f'{save_path}{name}.corpus')\n",
    "\t# except FileNotFoundError:\n",
    "\t# \tlogger.info(f'No file for {name}')\n",
    "\ttry:\n",
    "\t\tcorpora[name] = Corpus(name).build_from_csv(f'{source_path}{name}.csv.gz', text_column='text', build_process_path = '../conc-build-process')\n",
    "\texcept Exception as e:\n",
    "\t\traise e\n",
    "\tset_logger_state('quiet')\n",
    "\n",
    "# tr.print_diff()\n",
    "\n",
    "all_objects = muppy.get_objects()\n",
    "\n",
    "# my_types = muppy.filter(all_objects, Type=dict)\n",
    "# for t in my_types:\n",
    "# \tprint (t)\n",
    "\n",
    "sum1 = summary.summarize(all_objects)\n",
    "summary.print_(sum1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def info(self: Corpus, \n",
    "\t\t include_memory_usage:bool = False, # include memory usage in output\n",
    "\t\t formatted:bool = True # return formatted output\n",
    "\t\t ) -> str: # formatted information about the corpus\n",
    "\t\"\"\" Return information about the corpus. \"\"\"\n",
    "\t\n",
    "\tresult = []\n",
    "\tattributes = ['name', 'description', 'conc_version', 'corpus_path', 'source_path', 'document_count', 'token_count', 'unique_tokens', 'word_token_count', 'unique_word_tokens']\n",
    "\tfor attr in attributes:\n",
    "\t\tvalue = getattr(self, attr)\n",
    "\t\tif isinstance(value, bool):\n",
    "\t\t\tresult.append('True' if value else 'False')\n",
    "\t\telif isinstance(value, int):\n",
    "\t\t\tresult.append(f'{value:,}')\n",
    "\t\telse:\n",
    "\t\t\tresult.append(str(value))\n",
    "\n",
    "\tif include_memory_usage:\n",
    "\t\tsize_attributes = ['orth_index', 'lower_index', 'token2doc_index', 'vocab', 'frequency_lookup', 'offsets', 'metadata', 'original_to_new', 'new_to_original', 'results_cache', 'ngram_index', 'frequency_table']\n",
    "\t\tfor attr in size_attributes:\n",
    "\t\t\tsize = sys.getsizeof(getattr(self, attr))\n",
    "\t\t\tattributes.append(attr + ' (MB)')\n",
    "\t\t\tresult.append(f'{size/1024/1024:.3f}')\n",
    "\t\n",
    "\tif formatted:\n",
    "\t\tattributes = [attr.replace('_', ' ').title() for attr in attributes]\n",
    "\n",
    "\treturn pl.DataFrame({'Attribute': attributes, 'Value': result})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def summary(self: Corpus, \n",
    "\t\t\tinclude_memory_usage:bool = False # include memory usage in output\n",
    "\t\t\t):\n",
    "\t\"\"\" Print information about the corpus in a formatted table. \"\"\"\n",
    "\tresult = Result('summary', self.info(include_memory_usage), 'Corpus Summary', '', {}, [])\n",
    "\tresult.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def __str__(self: Corpus):\n",
    "\t\"\"\" Formatted information about the corpus. \"\"\"\n",
    "\t\n",
    "\treturn str(self.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _index_name(self: Corpus, index):\n",
    "\t\"\"\"Get name of index from spacy.\"\"\"\n",
    "\n",
    "\treturn list(spacy.attrs.IDS.keys())[list(spacy.attrs.IDS.values()).index(index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get summary information on your corpus, including the number of documents, the token count and the number of unique tokens as a dataframe using the `info` method. You can also just print the corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Attribute           Value                           \n",
      "\n",
      " Name                Brown Corpus                    \n",
      " Description         A Standard Corpus of Present-D \n",
      " Conc Version        0.0.1                           \n",
      " Corpus Path         ../test-corpora/saved//brown.c \n",
      " Source Path         ../test-corpora/source//brown. \n",
      " Document Count      500                             \n",
      " Token Count         1,140,905                       \n",
      " Unique Tokens       42,937                          \n",
      " Word Token Count    980,844                         \n",
      " Unique Word Tokens  42,907                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Attribute           Value                           \n",
      "\n",
      " Name                Toy Corpus                      \n",
      " Description         Toy corpus for testing          \n",
      " Conc Version        0.0.1                           \n",
      " Corpus Path         ../test-corpora/saved//toy.cor \n",
      " Source Path         ../test-corpora/source/toy.csv \n",
      " Document Count      6                               \n",
      " Token Count         38                              \n",
      " Unique Tokens       15                              \n",
      " Word Token Count    238                             \n",
      " Unique Word Tokens  14                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "print(toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the same information in a nicer format by using the `summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"pyoiiffzno\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#pyoiiffzno table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#pyoiiffzno thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#pyoiiffzno p { margin: 0; padding: 0; }\n",
       " #pyoiiffzno .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #pyoiiffzno .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #pyoiiffzno .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #pyoiiffzno .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #pyoiiffzno .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #pyoiiffzno .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #pyoiiffzno .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #pyoiiffzno .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #pyoiiffzno .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #pyoiiffzno .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #pyoiiffzno .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #pyoiiffzno .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #pyoiiffzno .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #pyoiiffzno .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #pyoiiffzno .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #pyoiiffzno .gt_from_md> :first-child { margin-top: 0; }\n",
       " #pyoiiffzno .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #pyoiiffzno .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #pyoiiffzno .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #pyoiiffzno .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #pyoiiffzno .gt_row_group_first td { border-top-width: 2px; }\n",
       " #pyoiiffzno .gt_row_group_first th { border-top-width: 2px; }\n",
       " #pyoiiffzno .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #pyoiiffzno .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #pyoiiffzno .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #pyoiiffzno .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #pyoiiffzno .gt_left { text-align: left; }\n",
       " #pyoiiffzno .gt_center { text-align: center; }\n",
       " #pyoiiffzno .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #pyoiiffzno .gt_font_normal { font-weight: normal; }\n",
       " #pyoiiffzno .gt_font_bold { font-weight: bold; }\n",
       " #pyoiiffzno .gt_font_italic { font-style: italic; }\n",
       " #pyoiiffzno .gt_super { font-size: 65%; }\n",
       " #pyoiiffzno .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #pyoiiffzno .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"2\" class=\"gt_heading gt_title gt_font_normal\">Corpus Summary</td>\n",
       "  </tr>\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"2\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\"></td>\n",
       "  </tr>\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Attribute\">Attribute</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Value\">Value</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Name</td>\n",
       "    <td class=\"gt_row gt_left\">Brown Corpus</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Description</td>\n",
       "    <td class=\"gt_row gt_left\">A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Conc Version</td>\n",
       "    <td class=\"gt_row gt_left\">0.0.1</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Corpus Path</td>\n",
       "    <td class=\"gt_row gt_left\">../test-corpora/saved//brown.corpus</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Source Path</td>\n",
       "    <td class=\"gt_row gt_left\">../test-corpora/source//brown.csv.gz</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Document Count</td>\n",
       "    <td class=\"gt_row gt_left\">500</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Token Count</td>\n",
       "    <td class=\"gt_row gt_left\">1,140,905</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Unique Tokens</td>\n",
       "    <td class=\"gt_row gt_left\">42,937</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Word Token Count</td>\n",
       "    <td class=\"gt_row gt_left\">980,844</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Unique Word Tokens</td>\n",
       "    <td class=\"gt_row gt_left\">42,907</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "brown.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a corpus\n",
    "\n",
    "Explain the various indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_frequency_table(self: Corpus):\n",
    "\t\"\"\" Prepare the frequency table for the corpus. \"\"\"\n",
    "\t# TODO work out case sensitivity issues - currently if do token lookup for The - not there\n",
    "\tif self.frequency_table is None:\n",
    "\t\t# note: don't sort this - leave in order of token_id - sorts can be done when required\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.frequency_table = pl.DataFrame({'token_id': list(self.frequency_lookup.keys()), 'frequency': list(self.frequency_lookup.values())})  \n",
    "\t\tself.frequency_table = self.frequency_table.join(pl.DataFrame({'token_id': list(self.vocab.keys()), 'token': list(self.vocab.values())}), on='token_id', how='left')\n",
    "\t\tself.frequency_table = self.frequency_table.with_columns(self.frequency_table['token_id'].is_in(self.punct_tokens).alias('is_punct')).with_columns(self.frequency_table['token_id'].is_in(self.space_tokens).alias('is_space'))\t\n",
    "\t\tself.frequency_table = self.frequency_table.with_row_index(name='rank', offset=1)\n",
    "\t\tlogger.info(f'Frequency table created in {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _mask_from_positions(self: Corpus, \n",
    "\t\t\t\t\t\t positions # positions to create mask from\n",
    "\t\t\t\t\t\t ):\n",
    "\t\"\"\" Convert positions to mask \"\"\"\n",
    "\tmask_from_positions = np.zeros(self.lower_index.shape, dtype=bool)\n",
    "\tmask_from_positions[positions] = True\n",
    "\treturn mask_from_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_tokens_array(self: Corpus):\n",
    "\t\"\"\" Prepare the tokens array for the corpus. \"\"\"\n",
    "\tif 'tokens_array' not in self.results_cache:\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.results_cache['tokens_array'] = np.array(list(self.vocab.values()))\n",
    "\t\tlogger.info(f'Create tokens_array in {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_tokens_sort_order(self: Corpus):\n",
    "\t\"\"\" Prepare the tokens sort order for the corpus. \"\"\"\n",
    "\tif 'tokens_sort_order' not in self.results_cache:\n",
    "\t\tself._init_tokens_array()\n",
    "\t\t# lowercasing then sorting ...\n",
    "\t\ttokens_array_lower = np.strings.lower(self.results_cache['tokens_array'])\n",
    "\t\tself.results_cache['tokens_sort_order'] = np.argsort(np.argsort(tokens_array_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "# TODO maybe convert to using tokens_array rather than frequency_table\n",
    "def token_to_id(self: Corpus, \n",
    "\t\t\t\ttoken: str # token to get id for\n",
    "\t\t\t\t) -> int|bool: # return token id or False if not found in the corpus\n",
    "\t\"\"\" Get the id for a token string. \"\"\"\n",
    "\n",
    "\tself._init_frequency_table()\n",
    "\ttoken = self.frequency_table.filter(pl.col('token') == token)['token_id']\n",
    "\tif token.shape[0] == 0:\n",
    "\t\treturn False\n",
    "\telse:\n",
    "\t\ttoken = token[0]\n",
    "\treturn token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ID of the token 'dog' like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23289"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.token_to_id('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_ids_to_tokens(self: Corpus, \n",
    "\t\t\t\t\t\ttoken_ids: np.ndarray|list # token ids to retrieve as tokens\n",
    "\t\t\t\t\t\t) -> np.ndarray: # return token strings for token ids\n",
    "\t\"\"\" Get token strings for a list of token ids. \"\"\" \n",
    "\n",
    "\tself._init_tokens_array()\n",
    "\tif isinstance(token_ids, list):\n",
    "\t\ttoken_ids = np.array(token_ids)\n",
    "\treturn self.results_cache['tokens_array'][token_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, conc uses Numpy vector operations where possible. A list or numpy array of Token IDs can be converted to a numpy array of token strings like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['acid', '395,000', 'mckinney'], dtype='<U30')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = [23288, 24576, 47803]\n",
    "brown.token_ids_to_tokens(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_ids_to_sort_order(self: Corpus, \n",
    "\t\t\t\t\t\t\ttoken_ids: np.ndarray # token ids to get rank \n",
    "\t\t\t\t\t\t\t) -> np.ndarray: # rank of token ids\n",
    "\t\"\"\" Get the rank of token ids in the frequency table. \"\"\"\n",
    "\t#TODO document that this is a rank\n",
    "\tself._init_tokens_sort_order()\t\n",
    "\n",
    "\treturn self.results_cache['tokens_sort_order'][token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22848, 23289, 18808]\n",
      "['the' 'dog' 'went']\n",
      "[50087 15848 54497]\n"
     ]
    }
   ],
   "source": [
    "test_token_ids = [\n",
    "brown.token_to_id('the'),\n",
    "brown.token_to_id('dog'),\n",
    "brown.token_to_id('went'),\n",
    "]\n",
    "\n",
    "print(test_token_ids)\n",
    "print(brown.token_ids_to_tokens(test_token_ids))\n",
    "print(brown.token_ids_to_sort_order(test_token_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def frequency_of(self: Corpus, \n",
    "\t\t\t\t token:str|int # token id or string to get frequency for\n",
    "\t\t\t\t ) -> int|bool: # return frequency of token or False if not found\n",
    "\t\"\"\" Get the frequency of a specific token. \"\"\"\n",
    "\t# TODO - make work with case insensitive tokens\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tself._init_frequency_table()\n",
    "\t\n",
    "\tif type(token) == str:\n",
    "\t\ttoken = self.token_to_id(token)\n",
    "\t\tif token == False:\n",
    "\t\t\treturn False\n",
    "\n",
    "\tlogger.info(f'Token frequency retrieval time: {(time.time() - start_time):.5f} seconds')\n",
    "\n",
    "\tif token in self.frequency_lookup:\n",
    "\t\treturn int(self.frequency_lookup[token])\n",
    "\telse:\n",
    "\t\treturn False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token [id=24577, go] occurs 625 times.\n",
      "Token [go] occurs 625 times.\n"
     ]
    }
   ],
   "source": [
    "token = 'go'\n",
    "token_id = brown.token_to_id(token)\n",
    "print(f'Token [id={token_id}, {token}] occurs {brown.frequency_of(token_id)} times.')\n",
    "print(f'Token [{token}] occurs {brown.frequency_of(token)} times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# lower_without_punct = test.lower_index[~(test._mask_from_positions(test.punct_positions))]\n",
    "# lower_without_space = test.lower_index[~(test._mask_from_positions(test.space_positions))]\n",
    "# lower_without_space_punct = test.lower_index[~(test._mask_from_positions(test.space_positions) | test._mask_from_positions(test.punct_positions))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def tokenize(self: Corpus, \n",
    "\t\t\t string:str, # string to tokenize \n",
    "\t\t\t return_doc = False, # return doc object\n",
    "\t\t\t simple_indexing = False # use simple indexing\n",
    "             ): # return tokenized string\n",
    "\t\"\"\" Tokenize a string using the Spacy tokenizer. \"\"\"\n",
    "\t# TODO implement case insensitive tokenization\n",
    "\t# TODO implement wildcard search and multiple strings\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tplaceholder_string = 'zzxxzzplaceholderzzxxzz' # so doesn't split tokens\n",
    "\tis_wildcard_search = False\n",
    "\tif simple_indexing == True:\n",
    "\t\tindex_id = LOWER\n",
    "\t\tstrings_to_tokenize = [string.strip()]\n",
    "\telse:\n",
    "\t\traise('only simple_indexing implemented')\n",
    "\t\t# TODO rework\n",
    "\t\t# if '*' in string:\n",
    "\t\t# \tis_wildcard_search = True\n",
    "\t\t# \tstring = string.replace('*',placeholder_string)\n",
    "\t\t# if string.islower() == True:\n",
    "\t\t# \tindex_id = LOWER\n",
    "\t\t# else:\n",
    "\t\t# \tindex_id = ORTH\n",
    "\t\t# if '|' in string:\n",
    "\t\t# \tstrings_to_tokenize = string.split('|')\n",
    "\t\t# else:\n",
    "\t\t# \tstrings_to_tokenize = [string.strip()]\n",
    "\ttoken_sequences = []\n",
    "\tfor doc in nlp.tokenizer.pipe(strings_to_tokenize):\n",
    "\t\ttoken_sequences.append(tuple(doc.to_array(index_id)))\n",
    "\t# if is_wildcard_search == True:\n",
    "\t# \ttmp_token_sequence = []\n",
    "\t# \tsequence_count = 1\n",
    "\t# \tfor token in doc:\n",
    "\t# \t\ttmp_token_sequence.append([])\n",
    "\t# \t\tif placeholder_string in token.text:\n",
    "\t# \t\t\tchunked_string = token.text.split(placeholder_string)\n",
    "\t# \t\t\tif len(chunked_string) > 2 or (len(chunked_string) == 2 and chunked_string[0] != '' and chunked_string[1] != ''):\n",
    "\t# \t\t\t\t# use regex\n",
    "\t# \t\t\t\tapproach = 'regex'\n",
    "\t# \t\t\t\tregex = re.compile('.*'.join(chunked_string))\n",
    "\t# \t\t\telif chunked_string[0] == '':\n",
    "\t# \t\t\t\tapproach = 'endswith'\n",
    "\t# \t\t\telse:\n",
    "\t# \t\t\t\tapproach = 'startswith'\n",
    "\t# \t\t\tfor token_id in loaded_corpora[corpus_name]['frequency_lookup']:\n",
    "\t# \t\t\t\tpossible_word = False\n",
    "\t# \t\t\t\tword = loaded_corpora[corpus_name]['vocab'][token_id]\n",
    "\t# \t\t\t\tif approach == 'regex':\n",
    "\t# \t\t\t\t\tif regex.match(word):\n",
    "\t# \t\t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\telif getattr(word,approach)(''.join(chunked_string)):\n",
    "\t# \t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\tif possible_word != False:\n",
    "\t# \t\t\t\t\ttmp_token_sequence[token.i].append(loaded_corpora[corpus_name]['vocab'][possible_word])\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttmp_token_sequence[token.i].append(token.orth)\n",
    "\t# \t\tsequence_count *= len(tmp_token_sequence[token.i])\n",
    "\t# \trotated_token_sequence = []\n",
    "\t# \ttoken_repeat = sequence_count\n",
    "\t# \tfor pos in range(len(tmp_token_sequence)):\n",
    "\t# \t\trotated_token_sequence.append([])\n",
    "\t# \t\tif len(tmp_token_sequence[pos]) == 1:\n",
    "\t# \t\t\trotated_token_sequence[pos] += sequence_count * [tmp_token_sequence[pos][0]]\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttoken_repeat = token_repeat // len(tmp_token_sequence[pos])\n",
    "\t# \t\t\twhile len(rotated_token_sequence[pos]) < sequence_count:\n",
    "\t# \t\t\t\tfor token in tmp_token_sequence[pos]:\n",
    "\t# \t\t\t\t\trotated_token_sequence[pos] += token_repeat * [token]\n",
    "\t# \ttoken_sequences = list(zip(*rotated_token_sequence))\n",
    "\t# \t#for tokens in tmp_token_sequence:\n",
    "\t# \t#    for token in tokens:\n",
    "\t# covert token_sequences to reindexed tokens using original_to_new\n",
    "\ttoken_sequences = [tuple([self.original_to_new[token] for token in sequence]) for sequence in token_sequences]\n",
    "\tlogger.info(f'Tokenization time: {(time.time() - start_time):.5f} seconds')\n",
    "\tif return_doc == True:\n",
    "\t\treturn token_sequences, index_id, doc\n",
    "\telse:\n",
    "\t\treturn token_sequences, index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.uint32(23289),)] LOWER\n"
     ]
    }
   ],
   "source": [
    "token_str = 'dog'\n",
    "brown_token_sequence, brown_index_id = brown.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "print(brown_token_sequence, brown._index_name(brown_index_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find positions of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_token_index(self: Corpus, \n",
    "\t\t\t\t\ttoken_sequence: list[np.ndarray], # token sequence to get index for \n",
    "\t\t\t\t\tindex_id: int # index to search (i.e. ORTH, LOWER)\n",
    "\t\t\t\t\t) -> np.ndarray: # positions of token sequence\n",
    "\t\"\"\" Get the positions of a token sequence in the corpus. \"\"\"\n",
    "\t\n",
    "\t#TODO - refactor token_sequence?\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tresults = []\n",
    "\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\tvariants_len = len(token_sequence)\n",
    "\n",
    "\tif index_id == ORTH:\n",
    "\t\tindex = 'orth_index'\n",
    "\telse:\n",
    "\t\tindex = 'lower_index'\n",
    "\n",
    "\tif (index, sequence_len) not in self.ngram_index:\n",
    "\t\tslices = [] # TODO adjust so not just lower below - so need a var to pass to this function with whether islower\n",
    "\t\t[slices.append(np.roll(getattr(self, index), shift)) for shift in -np.arange(sequence_len)]\n",
    "\t\tseq = np.vstack(slices).T\n",
    "\t\tself.ngram_index[(index, sequence_len)] = seq\n",
    "\n",
    "\tif variants_len == 1:\n",
    "\t\tresults.append(np.where(np.all(self.ngram_index[(index, sequence_len)] == token_sequence[0], axis=1))[0])\n",
    "\telse:\n",
    "\t\tcondition_list = []\n",
    "\t\tchoice_list = variants_len * [True]\n",
    "\t\tfor seq in token_sequence:\n",
    "\t\t\tcondition_list.append(self.ngram_index[(index, sequence_len)] == seq)\n",
    "\t\tresults.append(np.where(np.all(np.select(condition_list, choice_list),axis=1))[0])\n",
    "\n",
    "\tlogger.info(f'Token indexing ({len(results[0])}) time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  18944,   18981,   18992,   19062,   19069,   37777,   89076,\n",
      "        125511,  137608,  138261,  138296,  138305,  138349,  144502,\n",
      "        189104,  249691,  249831,  250054,  250067,  250093,  250161,\n",
      "        250187,  250247,  250275,  250386,  251335,  251354,  251414,\n",
      "        251473,  251505,  251559,  251569,  251894,  253602,  254562,\n",
      "        256120,  256224,  256397,  331441,  360984,  439241,  439245,\n",
      "        439300,  439305,  464727,  464756,  464778,  522492,  649908,\n",
      "        695780,  695829,  695989,  696181,  696460,  696839,  696916,\n",
      "        697014,  863902,  863909,  865540,  865558,  877577,  877619,\n",
      "        877706,  889653,  997085, 1014338, 1030313, 1052840, 1052849,\n",
      "       1054274, 1077178, 1087042, 1088300, 1088332, 1088919, 1107306,\n",
      "       1130649, 1139762])]\n"
     ]
    }
   ],
   "source": [
    "token_str = 'dog'\n",
    "brown_token_sequence, brown_token_id = brown.tokenize(token_str, simple_indexing=True)\n",
    "brown_token_index = brown.get_token_index(brown_token_sequence, brown_index_id)\n",
    "print(brown_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
