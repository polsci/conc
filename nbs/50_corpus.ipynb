{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus\n",
    "\n",
    "> Create a conc corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# requirements - numpy pandas polars spacy nltk great_tables\n",
    "# dev requirements - nbdev, jupyterlab, memory_profiler\n",
    "# TODO check\n",
    "\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from great_tables import GT\n",
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "from spacy.attrs import ORTH, LOWER # TODO - add ENT_TYPE, ENT_IOB? from spacy.attrs import SPACY, POS, TAG, SENT_START, LEMMA\n",
    "import sys\n",
    "import string\n",
    "from fastcore.basics import patch\n",
    "import time\n",
    "from slugify import slugify\n",
    "import msgspec # tested against orjson - with validation was faster, without around the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from memory_profiler import memory_usage\n",
    "# import pickle\n",
    "# import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from conc import __version__\n",
    "from conc.core import logger, set_logger_state, PAGE_SIZE, EOF_TOKEN_STR, REPOSITORY_URL, DOCUMENTATION_URL, CITATION_STR\n",
    "from conc.result import Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "polars_conf = pl.Config.set_tbl_hide_column_data_types(True)\n",
    "polars_conf = pl.Config.set_tbl_hide_dataframe_shape(True)\n",
    "polars_conf = pl.Config.set_tbl_rows(50)\n",
    "polars_conf = pl.Config.set_tbl_width_chars(300)\n",
    "polars_conf = pl.Config.set_fmt_str_lengths(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "_RE_PUNCT = re.compile(r\"^[^\\s^\\w^\\d]$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "NOT_DOC_TOKEN = -1\n",
    "INDEX_HEADER_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus metadata validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CorpusMetadata(msgspec.Struct): \n",
    "    \"\"\" JSON validation schema for corpus metadata \"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    slug: str\n",
    "    conc_version: str\n",
    "    document_count: int\n",
    "    token_count: int\n",
    "    word_token_count: int\n",
    "    punct_token_count: int\n",
    "    space_token_count: int\n",
    "    unique_tokens: int\n",
    "    unique_word_tokens: int\n",
    "    date_created: str\n",
    "    #source_path: str\n",
    "    EOF_TOKEN: int\n",
    "    SPACY_EOF_TOKEN: int\n",
    "    SPACY_MODEL: str\n",
    "    SPACY_MODEL_VERSION: str\n",
    "    punct_tokens: list[int]\n",
    "    space_tokens: list[int]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Corpus:\n",
    "\t\"\"\"Represention of text corpus, with methods to build, load and save a corpus from a variety of formats and to work with the corpus data.\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, \n",
    "\t\t\t\tname: str = '', # name of corpus\n",
    "\t\t\t\tdescription: str = '' # description of corpus\n",
    "\t\t\t\t):\n",
    "\t\t# information about corpus\n",
    "\t\tself.name = name\n",
    "\t\tself.description = description\n",
    "\t\tself.slug = None\n",
    "\n",
    "\t\t# conc version that built the corpus\n",
    "\t\tself.conc_version = None\n",
    "\t\t\n",
    "\t\t# paths\n",
    "\t\tself.corpus_path = None\n",
    "\t\tself.source_path = None\n",
    "\n",
    "\t\t# settings\n",
    "\t\tself.SPACY_MODEL = None\n",
    "\t\tself.SPACY_MODEL_VERSION = None\n",
    "\t\tself.SPACY_EOF_TOKEN = None # set below as nlp.vocab[EOF_TOKEN_STR].orth in build or through load  - EOF_TOKEN_STR starts with space so eof_token can't match anything from corpus\n",
    "\t\tself.EOF_TOKEN = None\n",
    "\n",
    "\t\t# special token ids\n",
    "\t\tself.punct_tokens = None\n",
    "\t\tself.space_tokens = None\n",
    "\n",
    "\t\t# metadata for corpus\n",
    "\t\tself.document_count = None\n",
    "\t\tself.token_count = None\n",
    "\t\tself.unique_tokens = None\n",
    "\n",
    "\t\tself.word_token_count = None\n",
    "\t\tself.unique_word_tokens = None\n",
    "\n",
    "\t\tself.date_created = None\n",
    "\n",
    "\t\t# token data\n",
    "\t\tself.orth_index = None\n",
    "\t\tself.lower_index = None\n",
    "\n",
    "\t\t# lookup mapping doc_id to every token in doc\n",
    "\t\tself.token2doc_index = None\n",
    "\n",
    "\t\t# lookups to get token string or frequency \n",
    "\t\tself.vocab = None\n",
    "\t\tself.frequency_lookup = None\n",
    "\n",
    "\t\t# offsets for each document in token data\n",
    "\t\tself.offsets = None\n",
    "\n",
    "\t\t# punct and space positions in token data\n",
    "\t\tself.punct_positions = None\n",
    "\t\tself.space_positions = None\n",
    "\n",
    "\t\t# metadata for each document\n",
    "\t\tself.metadata = []\n",
    "\n",
    "\t\t# lookups to get spacy tokenizer or internal ids\n",
    "\t\tself.original_to_new = None\n",
    "\t\tself.new_to_original = None\n",
    "\t\t\n",
    "\t\t# temporary data used when processing text, not \n",
    "\t\t# \n",
    "\t\t# \n",
    "\t\t# \n",
    "\t\t# d to disk permanently on save\n",
    "\t\tself.frequency_table = None\n",
    "\t\tself.ngram_index = {}\n",
    "\t\tself.results_cache = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and save a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_spacy_model(self: Corpus,\n",
    "                model: str = 'en_core_web_sm', # spacy model to use for tokenization\n",
    "\t\t\t\tversion: str|None = None # version of spacy model expected, if mismatch will raise a warning\n",
    "\t\t\t\t):\n",
    "\ttry:\n",
    "\t\tself._nlp = spacy.load(model)\n",
    "\t\tself._nlp.disable_pipes(['parser', 'ner', 'lemmatizer', 'tagger', 'senter', 'tok2vec', 'attribute_ruler'])\n",
    "\t\tself._nlp.max_length = 10_000_000 # set max length to a large number to avoid issues with long documents\n",
    "\texcept OSError as e:\n",
    "\t\tlogger.error(f'Error loading model {model}. You need to run python -m spacy download YOURMODEL to download the model. See https://spacy.io/models for available models.')\n",
    "\t\traise e\n",
    "\t\n",
    "\tif version is not None:\n",
    "\t\tif self._nlp.meta['version'] != version:\n",
    "\t\t\tlogger.warning(f'Spacy model version mismatch: expecting {version}, got {self._nlp.meta[\"version\"]}. This may cause issues with tokenization.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _process_punct_positions(self: Corpus):\n",
    "\t\"\"\" Process punctuation positions in token data and populates punct_tokens and punct_positions. \"\"\"\n",
    "\n",
    "\tself.punct_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip(string.punctuation) == ''}.keys()))\n",
    "\tpunct_mask = np.isin(self.lower_index, self.punct_tokens) # faster to retrieve with isin than where\n",
    "\tself.punct_positions = np.nonzero(punct_mask)[0] # storing this as smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation tokens are defined using Python `string.punctuation` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def _process_space_positions(self: Corpus):\n",
    "\t\"\"\" Process whitespace positions in token data and populates space_tokens and space_positions. \"\"\"\n",
    "\n",
    "\tself.space_tokens = np.array(list({k: v for k, v in self.vocab.items() if v.strip() == ''}.keys()))\n",
    "\tspace_mask = np.isin(self.lower_index, self.space_tokens) \t# faster to retrieve with isin than where\n",
    "\tself.space_positions = np.nonzero(space_mask)[0] # storing this as smaller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy includes space tokens in the vocab for non-destructive tokenisation. Positions of space tokens are stored so they can be filtered out for analysis and reporting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "# reminder of string.punctuation characters\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens consisting of only punctuation are defined as punctuation tokens. These can be removed or included in analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_build_process(self:Corpus,\n",
    "\t\t\t\t\t\tsave_path: str, # path to save corpus data \n",
    "\t\t\t\t\t\t):\n",
    "\t\"\"\" Create slug, corpus_path, and create directory if needed. \"\"\"\n",
    "\n",
    "\tself.conc_version = __version__\n",
    "\tself.slug = slugify(self.name, stopwords=['corpus'])\n",
    "\tself.corpus_path = f'{save_path}/{self.slug}.corpus'\n",
    "\n",
    "\tif not os.path.isdir(self.corpus_path):\n",
    "\t\tos.makedirs(self.corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _update_build_process(self: Corpus, \n",
    "                           orth_index: list[np.ndarray], # orthographic token ids\n",
    "                           lower_index: list[np.ndarray], # lower case token ids\n",
    "                           token2doc_index: list[np.ndarray], # token to document mapping\n",
    "                           store_pos: int # current store pos\n",
    "                           ) -> int: # next store pos\n",
    "    \"\"\" Write in-progress build data to Parquet disk store. \"\"\"\n",
    "\n",
    "    pl.DataFrame([np.concatenate(orth_index), np.concatenate(lower_index), np.concatenate(token2doc_index)], schema = [('orth_index', pl.UInt64), ('lower_index', pl.UInt64), ('token2doc_index', pl.Int32)] ).write_parquet(f'{self.corpus_path}/build_{store_pos}.parquet')\n",
    "    return store_pos + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _complete_build_process(self: Corpus, \n",
    "\t\t\t\t\t\t\tbuild_process_cleanup: bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t\t\t\t\t\t):\n",
    "\t\"\"\" Complete the disk-based build to create representation of the corpus. \"\"\"\n",
    "\n",
    "\tlogger.memory_usage('init', init=True)\n",
    "\tinput_df = pl.scan_parquet(f'{self.corpus_path}/build_*.parquet')\n",
    "\t# combining indexes to reindex\n",
    "\tcombined_df = pl.concat([input_df.select(pl.col('orth_index').alias('index')), input_df.select(pl.col('lower_index').alias('index'))])\n",
    "\n",
    "\tinput_length = input_df.select(pl.len()).collect(engine='streaming').item() # tested vs count - len seems to have slight memory overhead, but more correct (i.e. count only counts non-null)\n",
    "\tlogger.memory_usage(f'got input length {input_length}')\n",
    "\n",
    "\t# get unique vocab ids (combining orth and lower) and create new index\n",
    "\tvocab_df  = combined_df.select(pl.col('index').unique().sort().alias('source_id')).with_row_index('token_id', offset=1) #.collect(engine='streaming')\n",
    "\tlogger.memory_usage('collected vocab')\n",
    "\n",
    "\t# combined_df = (combined_df.with_columns(pl.col('index').replace(vocab_df.select(pl.col('source_id'))['source_id'], vocab_df.select(pl.col('token_id'))['token_id']).cast(pl.UInt32)))\n",
    "\t# combined_df = combined_df.with_columns(pl.col('index').cast(pl.UInt32))\n",
    "\n",
    "\tcombined_df = (\n",
    "\t\tcombined_df\n",
    "\t\t.join(vocab_df, left_on=\"index\", right_on=\"source_id\", how=\"left\", maintain_order=\"left\")\n",
    "\t\t.drop(\"index\")\n",
    "\t\t.rename({\"token_id\": \"index\"})\n",
    "\t\t.with_columns(pl.col(\"index\").cast(pl.UInt32).alias(\"index\"))\n",
    "\t)\n",
    "\n",
    "\ttokens_df = pl.concat(\n",
    "\t\t\t\t\t\t\t\t\t[combined_df.select(pl.col('index').alias('orth_index')).slice(0, input_length), \n",
    "\t\t\t\t\t\t\t\t\tcombined_df.select(pl.col('index').alias('lower_index')).slice(input_length),\n",
    "\t\t\t\t\t\t\t\t\tinput_df.select(pl.col('token2doc_index'))], how='horizontal'\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\n",
    "\tdel combined_df\n",
    "\tdel input_df\n",
    "\tlogger.memory_usage('freed up combined_df and input_df')\n",
    "\n",
    "\tvocab_query = vocab_df.select(pl.col('source_id')).collect(engine='streaming').to_numpy().flatten() # get vocab ids as numpy array for faster processing\n",
    "\n",
    "\tvocab = {k:self._nlp.vocab[k].text for k in vocab_query} # get vocab strings from spacy vocab\n",
    "\ttoken_strs = list(vocab.values())\n",
    "\tlogger.memory_usage('got vocab strings')\n",
    "\tvocab_df = vocab_df.with_columns(pl.Series(token_strs).alias('token'))\n",
    "\tlogger.memory_usage('added vocab strings')\n",
    "\n",
    "\tself.EOF_TOKEN = vocab_df.filter(pl.col('source_id') == self.SPACY_EOF_TOKEN).select(pl.col('token_id')).collect(engine='streaming').item() # casting to int for storage\n",
    "\t\n",
    "\tself.punct_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip(string.punctuation) == '']\n",
    "\tlogger.memory_usage(f'got punct tokens')\n",
    "\tself.space_tokens = [(k + 1) for k, v in enumerate(token_strs) if v.strip() == '']\n",
    "\tlogger.memory_usage(f'got space tokens')\n",
    "\n",
    "\tdel token_strs\n",
    "\n",
    "\t# Create LazyFrames for punct_positions and space_positions\n",
    "\ttokens_df.select(pl.col('lower_index')).with_row_index('position').filter(pl.col('lower_index').is_in(self.punct_tokens)).select('position').sink_parquet(f'{self.corpus_path}/puncts.parquet') #.collect(engine='streaming').to_numpy().flatten()\n",
    "\tlogger.memory_usage('saved punct positions')\n",
    "\ttokens_df.select(pl.col('lower_index')).with_row_index('position').filter(pl.col('lower_index').is_in(self.space_tokens)).select('position').sink_parquet(f'{self.corpus_path}/spaces.parquet') #.collect(engine='streaming').to_numpy().flatten()\n",
    "\tlogger.memory_usage('saved space positions')\n",
    "\n",
    "\t# get counts from tokens_df\n",
    "\tfrequency_lower = tokens_df.filter(pl.col('lower_index') != self.EOF_TOKEN).select(pl.col('lower_index')).group_by('lower_index').agg(pl.count('lower_index').alias('frequency_lower')) #.collect(engine='streaming')\n",
    "\tfrequency_orth = tokens_df.filter(pl.col('orth_index') != self.EOF_TOKEN).select(pl.col('orth_index')).group_by('orth_index').agg(pl.count('orth_index').alias('frequency_orth')) #.collect(engine='streaming')\n",
    "\tvocab_df = vocab_df.join(frequency_lower, left_on = 'token_id', right_on = 'lower_index', how='left', maintain_order=\"left\").join(frequency_orth, left_on = 'token_id', right_on = 'orth_index', how='left', maintain_order=\"left\")\n",
    "\tlogger.memory_usage('added frequency to vocab')\n",
    "\n",
    "\tself.unique_tokens = frequency_lower.select(pl.len()).collect(engine='streaming').item() # was len(frequency_lower) before used polars streaming # TODO - validate correct - make sure that EOF_TOKEN not included\n",
    "\tlogger.memory_usage(f'got unique tokens {self.document_count}')\n",
    "\n",
    "\t# add column for is_punct and is_space based on punct_tokens and space_tokens and token_id\n",
    "\tvocab_df = vocab_df.with_columns((pl.col(\"token_id\").is_in(self.punct_tokens)).alias(\"is_punct\"))\n",
    "\tvocab_df = vocab_df.with_columns((pl.col(\"token_id\").is_in(self.space_tokens)).alias(\"is_space\"))\n",
    "\tlogger.memory_usage('added is_punct is_space to vocab')\n",
    "\n",
    "\tvocab_df.sink_parquet(f'{self.corpus_path}/vocab.parquet')\n",
    "\tlogger.memory_usage('wrote vocab to disk')\n",
    "\ttokens_df.sink_parquet(f'{self.corpus_path}/tokens.parquet')\n",
    "\tlogger.memory_usage('wrote tokens to disk')\n",
    "\n",
    "\tself.document_count = tokens_df.select(pl.col('token2doc_index').filter(pl.col('token2doc_index') != NOT_DOC_TOKEN).unique().count()).collect(engine='streaming').item()\n",
    "\tlogger.memory_usage(f'got doc count {self.document_count}')\n",
    "\t# adjusting for text breaks and headers at start and end of index\n",
    "\tself.token_count = input_length - self.document_count - INDEX_HEADER_LENGTH - INDEX_HEADER_LENGTH \n",
    "\tlogger.memory_usage('got token count')\n",
    "\n",
    "\tdel tokens_df\n",
    "\n",
    "\tself.punct_token_count = pl.scan_parquet(f'{self.corpus_path}/puncts.parquet').select(pl.len()).collect(engine='streaming').item() # TODO - may be more efficient to do this prior to disk write\n",
    "\tlogger.memory_usage('got punct token count')\n",
    "\tself.space_token_count = pl.scan_parquet(f'{self.corpus_path}/spaces.parquet').select(pl.len()).collect(engine='streaming').item() # TODO - may be more efficient to do this prior to disk write\n",
    "\tlogger.memory_usage('got space token count')\n",
    "\tself.word_token_count = self.token_count - self.punct_token_count - self.space_token_count # TODO - validate correct - make sure that EOF_TOKEN not included\n",
    "\tself.unique_word_tokens = self.unique_tokens - len(self.punct_tokens) - len(self.space_tokens) # TODO - validate correct - make sure that EOF_TOKEN not included\n",
    "\t\n",
    "\tself.date_created = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "\tif build_process_cleanup:\n",
    "\t\tfor f in glob.glob(f'{self.corpus_path}/build_*.parquet'):\n",
    "\t\t\tos.remove(f)\n",
    "\t\tlogger.memory_usage('removed build files')\n",
    "\t\n",
    "\tlogger.memory_usage('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _create_indices(self: Corpus, \n",
    "\t\t\t\t   orth_index: list[np.ndarray], # list of np arrays of orth token ids \n",
    "\t\t\t\t   lower_index: list[np.ndarray], # list of np arrays of lower token ids\n",
    "\t\t\t\t   token2doc_index: list[np.ndarray] # list of np arrays of doc ids\n",
    "\t\t\t\t   ):\n",
    "\t\"\"\" (Depreciated) Use Numpy to create internal representation of the corpus for faster analysis and efficient representation on disk. Only used when the disk-based build process is not used. \"\"\"\n",
    "\n",
    "\tself.token2doc_index = np.concatenate(token2doc_index)\n",
    "\tunique_values, inverse = np.unique(np.concatenate(orth_index + lower_index), return_inverse=True)\n",
    "\n",
    "\t# adding a dummy value at the 0 index to avoid 0 being used as a token id\n",
    "\tunique_values = np.insert(unique_values, 0, 0)\n",
    "\tinverse += 1\n",
    "\tnew_values = np.arange(len(unique_values), dtype=np.uint32)\n",
    "\tself.original_to_new = dict(zip(unique_values, new_values))\n",
    "\tself.new_to_original = dict(zip(new_values, unique_values))\n",
    "\n",
    "\tself.orth_index = np.array(np.split(inverse, 2)[0], dtype=np.uint32)\n",
    "\tself.lower_index = np.array(np.split(inverse, 2)[1], dtype=np.uint32)\n",
    "\tdel inverse\n",
    "\n",
    "\tvocab = {k:self._nlp.vocab.strings[k] for k in unique_values}\n",
    "\tvocab[0] = 'ERROR: not a token'\n",
    "\n",
    "\tself.vocab = {**{k:vocab[self.new_to_original[k]] for k in new_values}}\n",
    "\n",
    "\tself.EOF_TOKEN = self.original_to_new[self.SPACY_EOF_TOKEN]\n",
    "\n",
    "\tself._process_punct_positions()\n",
    "\tself._process_space_positions()\n",
    "\n",
    "\tself.frequency_lookup = dict(zip(*np.unique(self.lower_index, return_counts=True)))\n",
    "\tdel self.frequency_lookup[self.EOF_TOKEN]\n",
    "\tdel unique_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_corpus_metadata(self: Corpus, \n",
    "\t\t ):\n",
    "\t\"\"\" Save corpus metadata. \"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tjson_bytes = msgspec.json.encode(CorpusMetadata(**{k: getattr(self, k) for k in ['name', 'description', 'slug', 'conc_version', 'document_count', 'token_count', 'word_token_count', 'punct_token_count', 'space_token_count', 'unique_tokens', 'unique_word_tokens', 'date_created', 'EOF_TOKEN', 'SPACY_EOF_TOKEN', 'SPACY_MODEL', 'SPACY_MODEL_VERSION', 'punct_tokens', 'space_tokens']}))\n",
    "\n",
    "\twith open(f'{self.corpus_path}/corpus.json', 'wb') as f:\n",
    "\t\tf.write(json_bytes)\n",
    "\n",
    "\twith open(f'{self.corpus_path}/README.md', 'w', encoding='utf-8') as f:\n",
    "\t\tf.write(f'# {self.name}\\n\\n{self.description}\\n\\n## About\\n\\nThis directory contains a corpus created by [Conc]({REPOSITORY_URL}) (version {self.conc_version}) on {self.date_created}. \\n\\nIt was created using spaCy model: {self.SPACY_MODEL} (version {self.SPACY_MODEL_VERSION})\\n\\n')\n",
    "\t\tf.write(f'## Corpus Information\\n\\nDocument count: {self.document_count}  \\nToken count: {self.token_count}  \\nWord token count: {self.word_token_count}  \\nUnique tokens: {self.unique_tokens}  \\nUnique word tokens: {self.unique_word_tokens}\\n\\n')\n",
    "\t\tf.write(f'## Using this corpus\\n\\nConc can be installed with pip using:  \\n\\nDocumentation with tutorials to get you started is available at {DOCUMENTATION_URL} \\n\\n')\n",
    "\t\tf.write(f'## Cite Conc\\n\\n{CITATION_STR}')\n",
    "\t\t\n",
    "\tlogger.info(f'Saved corpus metadata time: {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build(self: Corpus, \n",
    "\t\t  save_path:str, # directory where corpus will be created, a subdirectory will be automatically created with the corpus content\n",
    "\t\t  iterator: iter, # iterator of texts\n",
    "\t\t  model: str='en_core_web_sm', # spacy model to use for tokenisation\n",
    "\t\t  spacy_batch_size:int=500, # batch size for spacy tokenizer\n",
    "\t\t  #build_process_path:str|None=None, # path to save an in-progress build to disk to reduce memory usage, default of None disables \n",
    "\t\t  build_process_batch_size:int=5000, # save in-progress build to disk every n docs\n",
    "\t\t  build_process_cleanup:bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t  ):\n",
    "\t\"\"\"Build a corpus from an iterator of texts.\"\"\"\n",
    "\n",
    "\tself._init_spacy_model(model)\n",
    "\t\n",
    "\tself.SPACY_MODEL = model\n",
    "\tself.SPACY_MODEL_VERSION = self._nlp.meta['version']\n",
    "\tself.SPACY_EOF_TOKEN = self._nlp.vocab[EOF_TOKEN_STR].orth\n",
    "\t\n",
    "\tif self.corpus_path is None: # leaving for testing ... this should already be set if build has been initiated in standard way via build_from_csv, build_from_files or whatever other methods are implemented to handle build/imports in future\n",
    "\t\tself._init_build_process(save_path)\n",
    "\t\n",
    "\tlogger.memory_usage('init', init=True)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\teof_arr = np.array([self.SPACY_EOF_TOKEN], dtype=np.uint64)\n",
    "\tnot_doc_arr = np.array([NOT_DOC_TOKEN], dtype=np.int16)\n",
    "\tindex_header_arr = np.array([self.SPACY_EOF_TOKEN] * INDEX_HEADER_LENGTH, dtype=np.uint64) # this is added to start and end of index to prevent out of bound issues on searches\n",
    "\n",
    "\torth_index = [index_header_arr]\n",
    "\tlower_index = [index_header_arr]\n",
    "\ttoken2doc_index = [np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32)]\n",
    "\n",
    "\toffset = INDEX_HEADER_LENGTH\n",
    "\tself.offsets = [] # TODO - check that this is being used  - consider removing\n",
    "\n",
    "\tstore_pos = 0\n",
    "\n",
    "\tdoc_order = 0\n",
    "\tfor doc in self._nlp.pipe(iterator, batch_size = spacy_batch_size): # was previously using self._nlp.tokenizer.pipe(iterator, batch_size=batch_size): but this is faster, test other options at some point\n",
    "\t\torth_index.append(doc.to_array(ORTH))\n",
    "\t\torth_index.append(eof_arr)\n",
    "\n",
    "\t\tlower_index_tmp = doc.to_array(LOWER)\n",
    "\t\tlower_index.append(lower_index_tmp)\n",
    "\t\tlower_index.append(eof_arr)\n",
    "\n",
    "\t\ttoken2doc_index.append(np.array([doc_order] * len(lower_index_tmp), dtype=np.int32))\n",
    "\t\ttoken2doc_index.append(not_doc_arr)\n",
    "\n",
    "\t\tself.offsets.append(offset) \n",
    "\t\toffset = offset + len(lower_index_tmp) + 1\n",
    "\t\tdoc_order += 1\n",
    "\n",
    "\t\t# update store every build_process_batch_size docs\n",
    "\t\tif doc_order % build_process_batch_size == 0:\n",
    "\t\t\t#was based on condition build_process_path is not None before disk-based build process\n",
    "\t\t\tstore_pos = self._update_build_process(orth_index, lower_index, token2doc_index, store_pos)\n",
    "\t\t\tlower_index, orth_index, token2doc_index = [], [], []\n",
    "\t\t\tlogger.memory_usage(f'processed {doc_order} documents')\n",
    "\t\t\t\n",
    "\tdel iterator\n",
    "\torth_index.append(index_header_arr)\n",
    "\tlower_index.append(index_header_arr)\n",
    "\ttoken2doc_index.append(np.array([NOT_DOC_TOKEN] * len(index_header_arr), dtype=np.int32))\n",
    "\n",
    "\tlogger.memory_usage(f'Completing build process')\n",
    "\tif save_path is not None:\n",
    "\t\tstore_pos = self._update_build_process(orth_index, lower_index, token2doc_index, store_pos)\n",
    "\t\tself._complete_build_process(build_process_cleanup = build_process_cleanup)\n",
    "\telse:\n",
    "\t\t# depreciated - leaving for now\n",
    "\t\tself._create_indices(orth_index, lower_index, token2doc_index)\n",
    "\t\tself.document_count = len(self.offsets)\n",
    "\n",
    "\t\tself.token_count = self.lower_index.shape[0] - self.document_count - len(index_header_arr) - len(index_header_arr) \n",
    "\t\tself.unique_tokens = len(self.frequency_lookup)\n",
    "\n",
    "\t\tself.word_token_count = self.token_count - len(self.punct_positions) - len(self.space_positions)\n",
    "\t\tself.unique_word_tokens = len(self.frequency_lookup) - len(self.punct_tokens) - len(self.space_tokens)\n",
    "\n",
    "\tdel orth_index\n",
    "\tdel lower_index\n",
    "\tdel token2doc_index\n",
    "\t\n",
    "\tlogger.memory_usage(f'Completed build process')\n",
    "\n",
    "\t# save corpus metadata\n",
    "\tself.save_corpus_metadata()\n",
    "\n",
    "\tlogger.info(f'Build time: {(time.time() - start_time):.3f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _prepare_files(self: Corpus, \n",
    "\t\t\t\t\tsource_path: str, # path to folder with text files, path can be a directory, zip or tar/tar.gz file\n",
    "\t\t\t\t\tfile_mask:str='*.txt', # mask to select files \n",
    "\t\t\t\t\tmetadata_file: str|None=None, # path to a CSV with metadata\n",
    "\t\t\t\t\tmetadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from metadata\n",
    "\t\t\t\t\tencoding:str='utf8' # encoding of text files\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Prepare text files and metadata for building a corpus. Returns an iterator to get file text for processing.\"\"\"\n",
    "\n",
    "\t# allowing import from zip and tar files\n",
    "\tif os.path.isdir(source_path):\n",
    "\t\tfiles = glob.glob(os.path.join(source_path, file_mask))\n",
    "\t\ttype = 'folder'\n",
    "\telif os.path.isfile(source_path):\n",
    "\t\timport fnmatch\n",
    "\t\tif source_path.endswith('.zip'):\n",
    "\t\t\timport zipfile\n",
    "\t\t\twith zipfile.ZipFile(source_path, 'r') as z:\n",
    "\t\t\t\tfiles = []\n",
    "\t\t\t\tfor f in z.namelist():\n",
    "\t\t\t\t\tif fnmatch.fnmatch(f, file_mask):\n",
    "\t\t\t\t\t\tfiles.append(f)\n",
    "\t\t\t\tif len(files) > 0:\n",
    "\t\t\t\t\ttype = 'zip'\n",
    "\t\telif source_path.endswith('.tar') or source_path.endswith('.tar.gz'):\n",
    "\t\t\timport tarfile\n",
    "\t\t\twith tarfile.open(source_path, 'r') as t:\n",
    "\t\t\t\tfiles = []\n",
    "\t\t\t\tfor f in t.getnames():\n",
    "\t\t\t\t\tif fnmatch.fnmatch(f, file_mask):\n",
    "\t\t\t\t\t\tfiles.append(f)\n",
    "\t\t\t\tif len(files) > 0:\n",
    "\t\t\t\t\ttype = 'tar'\n",
    "\t\telse:\n",
    "\t\t\traise FileNotFoundError(f\"Path '{source_path}' is not a directory, zip or tar file\")\n",
    "\t\n",
    "\tif not files:\n",
    "\t\traise FileNotFoundError(f\"No files matching {file_mask} found in '{source_path}'\")\n",
    "\n",
    "\tmetadata = pl.LazyFrame({metadata_file_column: [os.path.basename(p) for p in files]})\n",
    "\n",
    "\tif metadata_file:\n",
    "\t\tif not os.path.isfile(metadata_file):\n",
    "\t\t\traise FileNotFoundError(f\"Metadata file '{metadata_file}' not found\")\n",
    "\t\ttry:\n",
    "\t\t\tmetadata_columns = set([metadata_file_column] + metadata_columns)\n",
    "\t\t\t\n",
    "\t\t\t# ordering metadata based on order of files so token data and metadata aligned\n",
    "\t\t\tmetadata = metadata.join(pl.scan_csv(metadata_file).select(metadata_columns), on=metadata_file_column, how='left')\n",
    "\t\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\t\traise\n",
    "\t\n",
    "\tmetadata.sink_parquet(f'{self.corpus_path}/metadata.parquet')\n",
    "\n",
    "\tself.source_path = source_path\n",
    "\n",
    "\tif type == 'folder':\n",
    "\t\tfor p in files:\n",
    "\t\t\tyield open(p, \"rb\").read().decode(encoding)\n",
    "\telif type == 'zip':\n",
    "\t\twith zipfile.ZipFile(source_path, 'r') as z:\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tyield z.read(f).decode(encoding)\n",
    "\telif type == 'tar':\n",
    "\t\twith tarfile.open(source_path, 'r') as t:\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tyield t.extractfile(f).read().decode(encoding)\t\t\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_from_files(self: Corpus,\n",
    "\t\t\t\t\tsource_path: str, # path to folder with text files \n",
    "\t\t\t\t\tsave_path: str, # path to save corpus\n",
    "\t\t\t\t\tfile_mask:str='*.txt', # mask to select files \n",
    "\t\t\t\t\tmetadata_file: str|None=None, # path to a CSV with metadata\n",
    "\t\t\t\t\tmetadata_file_column:str = 'file', # column in metadata file with file names to align texts with metadata\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from metadata\n",
    "\t\t\t\t\tencoding:str='utf-8', # encoding of text files\n",
    "\t\t\t\t\tmodel:str='en_core_web_sm', # spacy model to use for tokenisation\n",
    "\t\t\t\t\tspacy_batch_size:int=1000, # batch size for spacy tokenizer\n",
    "\t\t\t\t\t#build_process_path:str=None, # path to save an in-progress build to disk to reduce memory usage\n",
    "\t\t\t\t\tbuild_process_batch_size:int=5000, # save in-progress build to disk every n docs\n",
    "\t\t\t\t\tbuild_process_cleanup:bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Build a corpus from text files in a folder.\"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tself._init_build_process(save_path)\n",
    "\titerator = self._prepare_files(source_path, file_mask, metadata_file, metadata_file_column, metadata_columns, encoding) #, build_process_path=build_process_path\n",
    "\tself.build(save_path = save_path, iterator = iterator, model = model, spacy_batch_size = spacy_batch_size, build_process_batch_size = build_process_batch_size, build_process_cleanup = build_process_cleanup) #build_process_path = build_process_path, \n",
    "\tlogger.info(f'Build from files time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# test = Corpus('test')\n",
    "# texts = []\n",
    "# for text in test._prepare_files('../test-corpora/source/toy', file_mask='*1.txt'):\n",
    "# \ttexts.append(text)\n",
    "# assert len(texts) == 1\n",
    "# assert texts[0] == 'The cat sat on the mat.'\n",
    "\n",
    "# texts = []\n",
    "# for text in test._prepare_files('../test-corpora/source/toy', file_mask='*.txt', metadata_file='../test-corpora/source/toy.csv', metadata_file_column = 'source', metadata_columns=['category']):\n",
    "# \ttexts.append(text)\n",
    "\n",
    "# assert len(texts) == 6\n",
    "# assert 'The cat sat on the mat.' in texts\n",
    "# assert test.metadata.shape[0] == 6\n",
    "# assert test.metadata.columns == ['source', 'category']\n",
    "\n",
    "# cat_sat_index = texts.index('The cat sat on the mat.') \n",
    "# assert test.metadata['source'][cat_sat_index] == '1.txt'\n",
    "# assert test.metadata['category'][cat_sat_index] == 'feline'\n",
    "\n",
    "# del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _prepare_csv(self: Corpus, \n",
    "\t\t\t\t\tsource_path:str, # path to csv file\n",
    "\t\t\t\t\ttext_column:str='text', # column in csv with text\n",
    "\t\t\t\t\tmetadata_columns:list[str]=[], # list of column names to import from csv\n",
    "\t\t\t\t\tencoding:str='utf8', # encoding of csv passed to Polars read_csv, see their documentation\n",
    "\t\t\t\t\tbuild_process_batch_size:int=5000 # save in-progress build to disk every n rows\n",
    "\t\t\t\t\t) -> iter: # iterator to return rows for processing\n",
    "\t\"\"\"Prepare to import from CSV, including metadata. Returns an iterator to process the text column.\"\"\"\n",
    "\n",
    "\tif not os.path.isfile(source_path):\n",
    "\t\traise FileNotFoundError(f'Path ({source_path}) is not a file')\n",
    "\t\n",
    "\ttry:\n",
    "\t\tdf = pl.scan_csv(source_path, encoding = encoding).select([text_column] + metadata_columns)\n",
    "\texcept pl.exceptions.ColumnNotFoundError as e:\n",
    "\t\traise\n",
    "\n",
    "\tself.source_path = source_path\n",
    "\t\n",
    "\tdf.select(metadata_columns).sink_parquet(f'{self.corpus_path}/metadata.parquet')\n",
    "\n",
    "\tfor slice_df in df.collect(engine='streaming').iter_slices(n_rows=build_process_batch_size):  \n",
    "\t\tfor row in slice_df.iter_rows():\n",
    "\t\t\tyield row[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_from_csv(self: Corpus, \n",
    "\t\t\t\t   source_path:str, # path to csv file\n",
    "\t\t\t\t   save_path: str, # path to save corpus\n",
    "\t\t\t\t   text_column:str='text', # column in csv with text\n",
    "\t\t\t\t   metadata_columns:list[str]=[], # list of column names to import from csv\n",
    "\t\t\t\t   encoding:str='utf8', # encoding of csv passed to Polars read_csv, see their documentation\n",
    "\t\t\t\t   model:str='en_core_web_sm', # spacy model to use for tokenisation\n",
    "\t\t\t\t   spacy_batch_size:int=1000, # batch size for Spacy tokenizer\n",
    "\t\t\t\t   #build_process_path:str=None, # path to save an in-progress build to disk to reduce memory usage\n",
    "\t\t\t\t   build_process_batch_size:int=5000, # save in-progress build to disk every n docs\n",
    "\t\t\t\t   build_process_cleanup:bool = True # Remove the build files after build is complete, retained for development and testing purposes\n",
    "\t\t\t\t   ):\n",
    "\t\"\"\"Build a corpus from a csv file.\"\"\"\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\tself._init_build_process(save_path)\n",
    "\titerator = self._prepare_csv(source_path = source_path, text_column = text_column, metadata_columns = metadata_columns, encoding = encoding, build_process_batch_size = build_process_batch_size)\n",
    "\tself.build(save_path = save_path, iterator = iterator, model = model, spacy_batch_size = spacy_batch_size, build_process_batch_size = build_process_batch_size, build_process_cleanup = build_process_cleanup)\n",
    "\tlogger.info(f'Build from csv time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# test = Corpus('test')\n",
    "# texts = []\n",
    "\n",
    "# for text in test._prepare_csv('../test-corpora/source/toy.csv', text_column='text', metadata_columns=['source', 'category']):\n",
    "# \ttexts.append(text)\n",
    "\n",
    "# assert len(texts) == 6\n",
    "# cat_sat_index = 0\n",
    "# assert texts[cat_sat_index] == 'The cat sat on the mat.'\n",
    "# assert test.metadata.shape[0] == 6\n",
    "# assert test.metadata.columns == ['source', 'category']\n",
    "# assert test.metadata['source'][cat_sat_index] == '1.txt'\n",
    "# assert test.metadata['category'][cat_sat_index] == 'feline'\n",
    "# del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "source_path = '../test-corpora/source/'\n",
    "save_path = '../test-corpora/saved/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load(self: Corpus, \n",
    "\t\t corpus_path: str # path to load corpus\n",
    "\t\t ):\n",
    "\t\"\"\" Load corpus from disk and load the corresponding spaCy model. \"\"\"\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tif not os.path.isdir(corpus_path):\n",
    "\t\traise FileNotFoundError(f\"Path '{corpus_path}' is not a directory\")\n",
    "\t\n",
    "\texpected_files = ['corpus.json', 'vocab.parquet', 'tokens.parquet', 'puncts.parquet', 'spaces.parquet']\n",
    "\tif not all(os.path.isfile(os.path.join(corpus_path, f)) for f in expected_files):\n",
    "\t\traise FileNotFoundError(f\"Path '{corpus_path}' does not contain all expected files: {expected_files}\")\n",
    "\n",
    "\tself.corpus_path = corpus_path\n",
    "\n",
    "\twith open(f'{self.corpus_path}/corpus.json', 'rb') as f:\n",
    "\t\tdata = msgspec.json.decode(f.read(), type=CorpusMetadata)\n",
    "\n",
    "\tfor k in data.__slots__:\n",
    "\t\tsetattr(self, k, getattr(data, k))\n",
    "\n",
    "\tself._init_spacy_model(self.SPACY_MODEL, version = self.SPACY_MODEL_VERSION)\n",
    "\n",
    "\tlogger.info(f'Load time: {(time.time() - start_time):.3f} seconds')\n",
    "\n",
    "\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 14:34:12 - INFO - <module> - Loading toy corpus\n",
      "2025-05-27 14:34:12 - INFO - load - Load time: 0.228 seconds\n",
      "2025-05-27 14:34:12 - INFO - <module> - Loading brown corpus\n",
      "2025-05-27 14:34:12 - INFO - load - Load time: 0.281 seconds\n",
      "2025-05-27 14:34:12 - INFO - <module> - Loading reuters corpus\n",
      "2025-05-27 14:34:12 - INFO - load - Load time: 0.207 seconds\n",
      "2025-05-27 14:34:12 - INFO - <module> - Loading gutenberg corpus\n",
      "2025-05-27 14:34:13 - INFO - load - Load time: 0.200 seconds\n",
      "2025-05-27 14:34:13 - INFO - <module> - Loading garden-party-corpus corpus\n",
      "2025-05-27 14:34:13 - INFO - load - Load time: 0.196 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "corpora = {}\n",
    "corpora['toy'] = {'name': 'Toy Corpus', 'slug': 'toy', 'description': 'Toy corpus for testing', 'extension': '.csv.gz'}\n",
    "corpora['brown'] = {'name': 'Brown Corpus', 'slug': 'brown', 'description': 'A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html', 'extension': '.csv.gz'}\n",
    "corpora['reuters'] = {'name': 'Reuters Corpus', 'slug': 'reuters', 'description': 'From NLTK TODO', 'extension': '.csv.gz'}\n",
    "corpora['gutenberg'] = {'name': 'Gutenberg Corpus', 'slug': 'gutenberg', 'description': 'From NLTK TODO', 'extension': '.csv.gz'}\n",
    "corpora['garden-party-corpus'] = {'name': 'Garden Party Corpus', 'slug': 'garden-party', 'description': 'https://github.com/ucdh/scraping-garden-party', 'extension': '.zip'}\n",
    "\n",
    "set_logger_state('verbose')\n",
    "for corpus_name, corpus_details in corpora.items():\n",
    "\tlogger.info(f'Loading {corpus_name} corpus')\n",
    "\ttry:\n",
    "\t\tcorpus = Corpus().load(f\"{save_path}{corpus_details['slug']}.corpus\")\n",
    "\texcept FileNotFoundError:\n",
    "\t\tif 'csv' in corpus_details['extension']:\n",
    "\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_csv(source_path = f'{source_path}{corpus_name}.csv.gz', text_column='text', metadata_columns=['source'], save_path = save_path)\n",
    "\t\telse:\n",
    "\t\t\tcorpus = Corpus(name = corpus_details['name'], description = corpus_details['description']).build_from_files(source_path = f'{source_path}{corpus_name}{corpus_details[\"extension\"]}', save_path = save_path)\n",
    "\texcept Exception as e:\n",
    "\t\traise e\n",
    "\tdel corpus\n",
    "set_logger_state('quiet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 14:34:13 - INFO - memory_usage - init, memory usage: 1335.890625 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - Completing build process, memory usage: 1337.81640625 MB, difference: 1.92578125 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - init, memory usage: 1337.81640625 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - got input length 244, memory usage: 1339.75390625 MB, difference: 1.9375 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - collected vocab, memory usage: 1339.75390625 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - freed up combined_df and input_df, memory usage: 1339.75390625 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - got vocab strings, memory usage: 1340.50390625 MB, difference: 0.75 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - added vocab strings, memory usage: 1340.50390625 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - got punct tokens, memory usage: 1341.25390625 MB, difference: 0.75 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - got space tokens, memory usage: 1341.25390625 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - saved punct positions, memory usage: 1347.4921875 MB, difference: 6.23828125 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - saved space positions, memory usage: 1348.3046875 MB, difference: 0.8125 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - added frequency to vocab, memory usage: 1348.3046875 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - got unique tokens None, memory usage: 1353.51953125 MB, difference: 5.21484375 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - added is_punct is_space to vocab, memory usage: 1353.51953125 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - wrote vocab to disk, memory usage: 1364.12890625 MB, difference: 10.609375 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - wrote tokens to disk, memory usage: 1368.82421875 MB, difference: 4.6953125 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - got doc count 6, memory usage: 1370.765625 MB, difference: 1.94140625 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - got token count, memory usage: 1370.765625 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - got punct token count, memory usage: 1370.765625 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - got space token count, memory usage: 1370.765625 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - removed build files, memory usage: 1370.765625 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - done, memory usage: 1370.765625 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - memory_usage - Completed build process, memory usage: 1370.765625 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:13 - INFO - save_corpus_metadata - Saved corpus metadata time: 0.000 seconds\n",
      "2025-05-27 14:34:13 - INFO - build - Build time: 0.139 seconds\n",
      "2025-05-27 14:34:13 - INFO - build_from_csv - Build from csv time: 0.347 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "set_logger_state('verbose')\n",
    "# TODO - add tests for build and save and load\n",
    "if os.path.isdir(f'{save_path}/toy.corpus'):\n",
    "\tshutil.rmtree(f'{save_path}/toy.corpus')\n",
    "\t\n",
    "try:\n",
    "\ttoy = Corpus().load(f'{save_path}/toy.corpus')\n",
    "except FileNotFoundError:\n",
    "\ttoy = Corpus(name = corpora['toy']['name'], description = corpora['toy']['description']).build_from_csv(f'{source_path}toy.csv.gz', save_path = save_path, text_column='text', metadata_columns=['source'])\n",
    "except Exception as e:\n",
    "\traise e\n",
    "del toy\n",
    "set_logger_state('quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 14:34:13 - INFO - memory_usage - init, memory usage: 1370.765625 MB\n",
      "2025-05-27 14:34:15 - INFO - memory_usage - Completing build process, memory usage: 1360.5859375 MB, difference: -10.1796875 MB\n",
      "2025-05-27 14:34:15 - INFO - memory_usage - init, memory usage: 1360.5859375 MB\n",
      "2025-05-27 14:34:15 - INFO - memory_usage - got input length 1141605, memory usage: 1360.4609375 MB, difference: -0.125 MB\n",
      "2025-05-27 14:34:15 - INFO - memory_usage - collected vocab, memory usage: 1360.4609375 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:15 - INFO - memory_usage - freed up combined_df and input_df, memory usage: 1360.4609375 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - got vocab strings, memory usage: 1359.11328125 MB, difference: -1.34765625 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - added vocab strings, memory usage: 1359.11328125 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - got punct tokens, memory usage: 1354.328125 MB, difference: -4.78515625 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - got space tokens, memory usage: 1354.328125 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - saved punct positions, memory usage: 1343.63671875 MB, difference: -10.69140625 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - saved space positions, memory usage: 1339.25 MB, difference: -4.38671875 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - added frequency to vocab, memory usage: 1339.25 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - got unique tokens None, memory usage: 1330.828125 MB, difference: -8.421875 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - added is_punct is_space to vocab, memory usage: 1330.828125 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - wrote vocab to disk, memory usage: 1322.93359375 MB, difference: -7.89453125 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - wrote tokens to disk, memory usage: 1330.71484375 MB, difference: 7.78125 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - got doc count 500, memory usage: 1327.39453125 MB, difference: -3.3203125 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - got token count, memory usage: 1327.39453125 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - got punct token count, memory usage: 1327.6328125 MB, difference: 0.23828125 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - got space token count, memory usage: 1328.5703125 MB, difference: 0.9375 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - removed build files, memory usage: 1328.5703125 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - done, memory usage: 1328.5703125 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:16 - INFO - memory_usage - Completed build process, memory usage: 1328.5703125 MB, difference: 0.0 MB\n",
      "2025-05-27 14:34:16 - INFO - save_corpus_metadata - Saved corpus metadata time: 0.000 seconds\n",
      "2025-05-27 14:34:16 - INFO - build - Build time: 2.476 seconds\n",
      "2025-05-27 14:34:16 - INFO - build_from_csv - Build from csv time: 2.746 seconds\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "set_logger_state('verbose')\n",
    "# TODO - add tests for build and save and load\n",
    "if os.path.isdir(f'{save_path}/brown.corpus'):\n",
    "\tshutil.rmtree(f'{save_path}/brown.corpus')\n",
    "\n",
    "try:\n",
    "\tbrown = Corpus().load(f'{save_path}/brown.corpus')\n",
    "except FileNotFoundError:\n",
    "\tbrown = Corpus(name = corpora['brown']['name'], description = corpora['brown']['description']).build_from_csv(f'{source_path}/brown.csv.gz', save_path = save_path, text_column='text', metadata_columns=['source'])\n",
    "except Exception as e:\n",
    "\traise e\n",
    "del brown\n",
    "set_logger_state('quiet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def list_corpora(\n",
    "\t\tpath: str # path to load corpus\n",
    "\t\t) -> pl.DataFrame: # Dataframe with path, corpus, corpus name, document count, token count\n",
    "\t\"\"\" Scan a directory for available corpora \"\"\"\n",
    "\t\n",
    "\tavailable_corpora = {'path': path, 'corpus': [], 'name': [], 'date_created': [], 'document_count': [], 'token_count': []}\n",
    "\tfor dir in os.listdir(path):\n",
    "\t\tif os.path.isdir(os.path.join(path, dir)) and os.path.isfile( os.path.join(path, dir, 'corpus.json')):\n",
    "\t\t\twith open(os.path.join(path, dir, 'corpus.json'), 'rb') as f:\n",
    "\t\t\t\tdata = msgspec.json.decode(f.read(), type=CorpusMetadata)\n",
    "\n",
    "\t\t\tavailable_corpora['corpus'].append(dir)\n",
    "\t\t\tfor k in ['name', 'document_count', 'token_count', 'date_created']:\n",
    "\t\t\t\tattr = getattr(data, k)\n",
    "\t\t\t\tif isinstance(attr, int):\n",
    "\t\t\t\t\tattr = f'{attr:,}'\n",
    "\t\t\t\tavailable_corpora[k].append(attr)\n",
    "\n",
    "\treturn pl.DataFrame(available_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " path                    corpus               name                 date_created         document_count  token_count \n",
      "\n",
      " ../test-corpora/saved/  gutenberg.corpus     Gutenberg Corpus     2025-05-27 14:30:06  18              2,777,046   \n",
      " ../test-corpora/saved/  garden-party.corpus  Garden Party Corpus  2025-05-27 14:30:06  15              79,940      \n",
      " ../test-corpora/saved/  brown.corpus         Brown Corpus         2025-05-27 14:34:16  500             1,140,905   \n",
      " ../test-corpora/saved/  toy.corpus           Toy Corpus           2025-05-27 14:34:13  6               38          \n",
      " ../test-corpora/saved/  reuters.corpus       Reuters Corpus       2025-05-27 14:29:56  10,788          1,726,826   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list_corpora(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def info(self: Corpus, \n",
    "\t\t include_disk_usage:bool = False, # include information of size on disk in output\n",
    "\t\t formatted:bool = True # return formatted output\n",
    "\t\t ) -> str: # formatted information about the corpus\n",
    "\t\"\"\" Return information about the corpus. \"\"\"\n",
    "\t\n",
    "\tresult = []\n",
    "\tattributes = ['name', 'description', 'date_created', 'conc_version', 'corpus_path', 'document_count', 'token_count', 'word_token_count', 'unique_tokens', 'unique_word_tokens']\n",
    "\tfor attr in attributes:\n",
    "\t\tvalue = getattr(self, attr)\n",
    "\t\tif isinstance(value, bool):\n",
    "\t\t\tresult.append('True' if value else 'False')\n",
    "\t\telif isinstance(value, int):\n",
    "\t\t\tresult.append(f'{value:,}')\n",
    "\t\telse:\n",
    "\t\t\tresult.append(str(value))\n",
    "\n",
    "\tif include_disk_usage:\n",
    "\t\tfiles = {'corpus.json': 'Corpus Metadata', 'metadata.parquet': 'Document Metadata', 'tokens.parquet': 'Tokens', 'vocab.parquet': 'Vocab', 'puncts.parquet': 'Punctuation positions', 'spaces.parquet': 'Space positions'}\n",
    "\t\tfor file, file_descriptor in files.items():\n",
    "\t\t\tsize = os.path.getsize(f'{self.corpus_path}/{file}')\n",
    "\t\t\tattributes.append(file_descriptor + ' (MB)')\n",
    "\t\t\tresult.append(f'{size/1024/1024:.3f}')\n",
    "\n",
    "\t# maybe add in status of these: 'results_cache', 'ngram_index', 'frequency_table'\n",
    "\t# size = sys.getsizeof(getattr(self, attr))\n",
    "\t\n",
    "\tif formatted:\n",
    "\t\tattributes = [attr.replace('_', ' ').title() for attr in attributes]\n",
    "\n",
    "\treturn pl.DataFrame({'Attribute': attributes, 'Value': result})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def summary(self: Corpus, \n",
    "\t\t\tinclude_memory_usage:bool = False # include memory usage in output\n",
    "\t\t\t):\n",
    "\t\"\"\" Print information about the corpus in a formatted table. \"\"\"\n",
    "\tresult = Result('summary', self.info(include_memory_usage), 'Corpus Summary', '', {}, [])\n",
    "\tresult.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def __str__(self: Corpus):\n",
    "\t\"\"\" Formatted information about the corpus. \"\"\"\n",
    "\t\n",
    "\treturn str(self.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _index_name(self: Corpus, index):\n",
    "\t\"\"\"Get name of index from spacy.\"\"\"\n",
    "\n",
    "\treturn list(spacy.attrs.IDS.keys())[list(spacy.attrs.IDS.values()).index(index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get summary information on your corpus, including the number of documents, the token count and the number of unique tokens as a dataframe using the `info` method. You can also just print the corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Attribute           Value                                                                                                                                                                                                                                              \n",
      "\n",
      " Name                Brown Corpus                                                                                                                                                                                                                                       \n",
      " Description         A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 \n",
      "                     http://www.hit.uib.no/icame/brown/bcm.html                                                                                                                                                                                                         \n",
      " Date Created        2025-05-27 14:34:16                                                                                                                                                                                                                                \n",
      " Conc Version        0.0.1                                                                                                                                                                                                                                              \n",
      " Corpus Path         ../test-corpora/saved//brown.corpus                                                                                                                                                                                                                \n",
      " Document Count      500                                                                                                                                                                                                                                                \n",
      " Token Count         1,140,905                                                                                                                                                                                                                                          \n",
      " Word Token Count    980,144                                                                                                                                                                                                                                            \n",
      " Unique Tokens       42,937                                                                                                                                                                                                                                             \n",
      " Unique Word Tokens  42,907                                                                                                                                                                                                                                             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "brown = Corpus().load(f'{save_path}/brown.corpus')\n",
    "print(brown) # equivalent to print(brown.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info` method can also provide information on the disk usage of the corpus setting the `include_disk_usage` parameter to `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Attribute                   Value                                                                                                                                                                                                                                              \n",
      "\n",
      " Name                        Brown Corpus                                                                                                                                                                                                                                       \n",
      " Description                 A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 \n",
      "                             http://www.hit.uib.no/icame/brown/bcm.html                                                                                                                                                                                                         \n",
      " Date Created                2025-05-27 14:34:16                                                                                                                                                                                                                                \n",
      " Conc Version                0.0.1                                                                                                                                                                                                                                              \n",
      " Corpus Path                 ../test-corpora/saved//brown.corpus                                                                                                                                                                                                                \n",
      " Document Count              500                                                                                                                                                                                                                                                \n",
      " Token Count                 1,140,905                                                                                                                                                                                                                                          \n",
      " Word Token Count            980,144                                                                                                                                                                                                                                            \n",
      " Unique Tokens               42,937                                                                                                                                                                                                                                             \n",
      " Unique Word Tokens          42,907                                                                                                                                                                                                                                             \n",
      " Corpus Metadata (Mb)        0.001                                                                                                                                                                                                                                              \n",
      " Document Metadata (Mb)      0.001                                                                                                                                                                                                                                              \n",
      " Tokens (Mb)                 4.429                                                                                                                                                                                                                                              \n",
      " Vocab (Mb)                  0.909                                                                                                                                                                                                                                              \n",
      " Punctuation Positions (Mb)  0.426                                                                                                                                                                                                                                              \n",
      " Space Positions (Mb)        0.007                                                                                                                                                                                                                                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(brown.info(include_disk_usage=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the same information in a nicer format by using the `summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"qpbhpqhzay\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#qpbhpqhzay table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#qpbhpqhzay thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#qpbhpqhzay p { margin: 0; padding: 0; }\n",
       " #qpbhpqhzay .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: 0; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #qpbhpqhzay .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #qpbhpqhzay .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #qpbhpqhzay .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #qpbhpqhzay .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #qpbhpqhzay .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #qpbhpqhzay .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #qpbhpqhzay .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #qpbhpqhzay .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #qpbhpqhzay .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #qpbhpqhzay .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #qpbhpqhzay .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #qpbhpqhzay .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #qpbhpqhzay .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #qpbhpqhzay .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #qpbhpqhzay .gt_from_md> :first-child { margin-top: 0; }\n",
       " #qpbhpqhzay .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #qpbhpqhzay .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #qpbhpqhzay .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #qpbhpqhzay .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #qpbhpqhzay .gt_row_group_first td { border-top-width: 2px; }\n",
       " #qpbhpqhzay .gt_row_group_first th { border-top-width: 2px; }\n",
       " #qpbhpqhzay .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #qpbhpqhzay .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #qpbhpqhzay .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #qpbhpqhzay .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #qpbhpqhzay .gt_left { text-align: left; }\n",
       " #qpbhpqhzay .gt_center { text-align: center; }\n",
       " #qpbhpqhzay .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #qpbhpqhzay .gt_font_normal { font-weight: normal; }\n",
       " #qpbhpqhzay .gt_font_bold { font-weight: bold; }\n",
       " #qpbhpqhzay .gt_font_italic { font-style: italic; }\n",
       " #qpbhpqhzay .gt_super { font-size: 65%; }\n",
       " #qpbhpqhzay .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #qpbhpqhzay .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"2\" class=\"gt_heading gt_title gt_font_normal\">Corpus Summary</td>\n",
       "  </tr>\n",
       "  <tr class=\"gt_heading\">\n",
       "    <td colspan=\"2\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\"></td>\n",
       "  </tr>\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Attribute\">Attribute</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Value\">Value</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Name</td>\n",
       "    <td class=\"gt_row gt_left\">Brown Corpus</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Description</td>\n",
       "    <td class=\"gt_row gt_left\">A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Date Created</td>\n",
       "    <td class=\"gt_row gt_left\">2025-05-27 14:34:16</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Conc Version</td>\n",
       "    <td class=\"gt_row gt_left\">0.0.1</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Corpus Path</td>\n",
       "    <td class=\"gt_row gt_left\">../test-corpora/saved//brown.corpus</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Document Count</td>\n",
       "    <td class=\"gt_row gt_left\">500</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Token Count</td>\n",
       "    <td class=\"gt_row gt_left\">1,140,905</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Word Token Count</td>\n",
       "    <td class=\"gt_row gt_left\">980,144</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Unique Tokens</td>\n",
       "    <td class=\"gt_row gt_left\">42,937</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">Unique Word Tokens</td>\n",
       "    <td class=\"gt_row gt_left\">42,907</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "brown.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a corpus\n",
    "\n",
    "A Conc corpus is a directory containing specific files as follows:\n",
    "\n",
    "```\n",
    "corpus-name.corpus/\n",
    "\tREADME.md - Human readable information about the corpus to aide distribution\n",
    "\tcorpus.json - Machine readable information about the corpus, including name, description, various summary statistics, and models used to build the corpus\n",
    "\tvocab.parquet - A table mapping token strings to token IDs and frequency information\n",
    "\ttokens.parquet - A table with indices based on token positions used to query the corpus with tokens represented by numeric IDs\n",
    "\tmetadata.parquet - A table with metadata for each document (if there is any)\n",
    "```\n",
    "\n",
    "Note: by default the library creates a directory with the `.corpus` suffix. This is not necessary, but this makes corpora on your filesystem easier to find or identify.\n",
    "\n",
    "To distribute a corpus, send a zip of the directory for others to extract or just share the directory as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### README.md\n",
    "\n",
    "Below is an example of the README.md file generated by the Conc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div class=\"alert alert-block alert-success\">\n",
       "\n",
       "## Brown Corpus\n",
       "\n",
       "A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA Revised 1971, Revised and Amplified 1979 http://www.hit.uib.no/icame/brown/bcm.html\n",
       "\n",
       "### About\n",
       "\n",
       "This directory contains a corpus created by [Conc](https://github.com/polsci/conc) (version 0.0.1) on 2025-05-27 14:34:16. \n",
       "\n",
       "It was created using spaCy model: en_core_web_sm (version 3.8.0)\n",
       "\n",
       "### Corpus Information\n",
       "\n",
       "Document count: 500  \n",
       "Token count: 1140905  \n",
       "Word token count: 980144  \n",
       "Unique tokens: 42937  \n",
       "Unique word tokens: 42907\n",
       "\n",
       "### Using this corpus\n",
       "\n",
       "Conc can be installed with pip using:  \n",
       "\n",
       "Documentation with tutorials to get you started is available at https://geoffford.nz/conc \n",
       "\n",
       "### Cite Conc\n",
       "\n",
       "If you use Conc in your work, please cite it as follows:\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: true\n",
    "with open(f'{brown.corpus_path}/README.md', 'rb') as f:\n",
    "    markdown = '<div class=\"alert alert-block alert-success\">\\n\\n' + f.read().decode('utf-8') + '\\n'\n",
    "    markdown = markdown.replace('\\n#', '\\n##') # making headings smaller for display\n",
    "    markdown += '</div>'\n",
    "    display(Markdown(markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus.json file\n",
    "\n",
    "Below is the schema for the `corpus.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': {'type': 'string'},\n",
       " 'description': {'type': 'string'},\n",
       " 'slug': {'type': 'string'},\n",
       " 'conc_version': {'type': 'string'},\n",
       " 'document_count': {'type': 'integer'},\n",
       " 'token_count': {'type': 'integer'},\n",
       " 'word_token_count': {'type': 'integer'},\n",
       " 'punct_token_count': {'type': 'integer'},\n",
       " 'space_token_count': {'type': 'integer'},\n",
       " 'unique_tokens': {'type': 'integer'},\n",
       " 'unique_word_tokens': {'type': 'integer'},\n",
       " 'date_created': {'type': 'string'},\n",
       " 'EOF_TOKEN': {'type': 'integer'},\n",
       " 'SPACY_EOF_TOKEN': {'type': 'integer'},\n",
       " 'SPACY_MODEL': {'type': 'string'},\n",
       " 'SPACY_MODEL_VERSION': {'type': 'string'},\n",
       " 'punct_tokens': {'type': 'array', 'items': {'type': 'integer'}},\n",
       " 'space_tokens': {'type': 'array', 'items': {'type': 'integer'}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: true\n",
    "properties = msgspec.json.schema(CorpusMetadata)['$defs']['CorpusMetadata']['properties']\n",
    "display(properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocab.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>token_id</th><th>source_id</th><th>token</th><th>frequency_lower</th><th>frequency_orth</th><th>is_punct</th><th>is_space</th></tr></thead><tbody><tr><td>22848</td><td>7425985699627899538</td><td>&quot;the&quot;</td><td>63516</td><td>62473</td><td>false</td><td>false</td></tr><tr><td>8128</td><td>2593208677638477497</td><td>&quot;,&quot;</td><td>58331</td><td>58331</td><td>true</td><td>false</td></tr><tr><td>38309</td><td>12646065887601541794</td><td>&quot;.&quot;</td><td>49907</td><td>49907</td><td>true</td><td>false</td></tr><tr><td>2739</td><td>886050111519832510</td><td>&quot;of&quot;</td><td>36321</td><td>36122</td><td>false</td><td>false</td></tr><tr><td>7126</td><td>2283656566040971221</td><td>&quot;and&quot;</td><td>27787</td><td>27633</td><td>false</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "\n",
       " token_id  source_id             token  frequency_lower  frequency_orth  is_punct  is_space \n",
       "\n",
       " 22848     7425985699627899538   the    63516            62473           false     false    \n",
       " 8128      2593208677638477497   ,      58331            58331           true      false    \n",
       " 38309     12646065887601541794  .      49907            49907           true      false    \n",
       " 2739      886050111519832510    of     36321            36122           false     false    \n",
       " 7126      2283656566040971221   and    27787            27633           false     false    \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: true\n",
    "display(pl.scan_parquet(f'{brown.corpus_path}/vocab.parquet').filter(pl.col('frequency_lower') > 0).sort(by = pl.col('frequency_lower'), descending = True).head(5).collect(engine='streaming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how frequency stored - i.e. with different word forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>token_id</th><th>source_id</th><th>token</th><th>frequency_lower</th><th>frequency_orth</th><th>is_punct</th><th>is_space</th></tr></thead><tbody><tr><td>15682</td><td>5059648917813135842</td><td>&quot;The&quot;</td><td>null</td><td>1043</td><td>false</td><td>false</td></tr><tr><td>22848</td><td>7425985699627899538</td><td>&quot;the&quot;</td><td>63516</td><td>62473</td><td>false</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "\n",
       " token_id  source_id            token  frequency_lower  frequency_orth  is_punct  is_space \n",
       "\n",
       " 15682     5059648917813135842  The    null             1043            false     false    \n",
       " 22848     7425985699627899538  the    63516            62473           false     false    \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: true\n",
    "display(pl.scan_parquet(f'{brown.corpus_path}/vocab.parquet').filter(pl.col('token').str.to_lowercase() == 'the').head(5).collect(engine='streaming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokens.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>position</th><th>orth_index</th><th>lower_index</th><th>token2doc_index</th></tr></thead><tbody><tr><td>99</td><td>46333</td><td>46333</td><td>-1</td></tr><tr><td>100</td><td>27276</td><td>27276</td><td>0</td></tr><tr><td>101</td><td>15682</td><td>22848</td><td>0</td></tr><tr><td>102</td><td>4361</td><td>41672</td><td>0</td></tr><tr><td>103</td><td>14610</td><td>29725</td><td>0</td></tr><tr><td>104</td><td>54713</td><td>49998</td><td>0</td></tr><tr><td>105</td><td>45742</td><td>19078</td><td>0</td></tr><tr><td>106</td><td>53250</td><td>53250</td><td>0</td></tr><tr><td>107</td><td>8699</td><td>35796</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "\n",
       " position  orth_index  lower_index  token2doc_index \n",
       "\n",
       " 99        46333       46333        -1              \n",
       " 100       27276       27276        0               \n",
       " 101       15682       22848        0               \n",
       " 102       4361        41672        0               \n",
       " 103       14610       29725        0               \n",
       " 104       54713       49998        0               \n",
       " 105       45742       19078        0               \n",
       " 106       53250       53250        0               \n",
       " 107       8699        35796        0               \n",
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: true\n",
    "pl.scan_parquet(f'{brown.corpus_path}/tokens.parquet').with_row_index('position').filter(pl.col('position').is_between(99, 107)).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain this token2doc_index -1 above and various other fields mapped below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>position</th><th>orth_index</th><th>lower_index</th><th>token2doc_index</th><th>token</th></tr></thead><tbody><tr><td>99</td><td>46333</td><td>46333</td><td>-1</td><td>&quot; conc-end-of-file-token&quot;</td></tr><tr><td>100</td><td>27276</td><td>27276</td><td>0</td><td>&quot;\n",
       "\n",
       "\t&quot;</td></tr><tr><td>101</td><td>15682</td><td>22848</td><td>0</td><td>&quot;The&quot;</td></tr><tr><td>102</td><td>4361</td><td>41672</td><td>0</td><td>&quot;Fulton&quot;</td></tr><tr><td>103</td><td>14610</td><td>29725</td><td>0</td><td>&quot;County&quot;</td></tr><tr><td>104</td><td>54713</td><td>49998</td><td>0</td><td>&quot;Grand&quot;</td></tr><tr><td>105</td><td>45742</td><td>19078</td><td>0</td><td>&quot;Jury&quot;</td></tr><tr><td>106</td><td>53250</td><td>53250</td><td>0</td><td>&quot;said&quot;</td></tr><tr><td>107</td><td>8699</td><td>35796</td><td>0</td><td>&quot;Friday&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "\n",
       " position  orth_index  lower_index  token2doc_index  token                   \n",
       "\n",
       " 99        46333       46333        -1                conc-end-of-file-token \n",
       " 100       27276       27276        0                                        \n",
       "                                                                             \n",
       "                                                     \t                       \n",
       " 101       15682       22848        0                The                     \n",
       " 102       4361        41672        0                Fulton                  \n",
       " 103       14610       29725        0                County                  \n",
       " 104       54713       49998        0                Grand                   \n",
       " 105       45742       19078        0                Jury                    \n",
       " 106       53250       53250        0                said                    \n",
       " 107       8699        35796        0                Friday                  \n",
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: true\n",
    "pl.scan_parquet(f'{brown.corpus_path}/tokens.parquet').with_row_index('position').filter(pl.col('position').is_between(99, 107)).join(\n",
    "    pl.scan_parquet(f'{brown.corpus_path}/vocab.parquet').select(pl.col('token_id'), pl.col('token')),\n",
    "    left_on='orth_index', right_on='token_id', how='left', maintain_order='left').collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaces.parquet and puncts.parquet\n",
    "\n",
    "The format of spaces.parquet and puncts.parquet are the same. Each table contains one field, namely `position`, which indexes the position of punctuation or space tokens in the corpus. Here are the first three rows of a `puncts.parquet` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>position</th></tr></thead><tbody><tr><td>117</td></tr><tr><td>118</td></tr><tr><td>121</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "\n",
       " position \n",
       "\n",
       " 117      \n",
       " 118      \n",
       " 121      \n",
       ""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: true\n",
    "pl.scan_parquet(f'{brown.corpus_path}/puncts.parquet').head(3).collect(engine='streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metadata.parquet\n",
    "\n",
    "The `metadata.parquet` should not be confused with the metadata of the corpus itself, which is accessible in `corpus.jon`.\n",
    "\n",
    "If populated, the `metadata.parquet` file contains metadata for each document in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>speech_id</th><th>date</th><th>speaker</th><th>chamber</th><th>state</th></tr></thead><tbody><tr><td>530182158</td><td>&quot;1895-01-10T00:00:00.000000&quot;</td><td>&quot;Mr. COCKRELL&quot;</td><td>&quot;S&quot;</td><td>&quot;Unknown&quot;</td></tr><tr><td>890274849</td><td>&quot;1966-08-31T00:00:00.000000&quot;</td><td>&quot;Mr. LONG of Louisiana&quot;</td><td>&quot;S&quot;</td><td>&quot;Louisiana&quot;</td></tr><tr><td>880088363</td><td>&quot;1963-09-11T00:00:00.000000&quot;</td><td>&quot;Mr. FULBRIGHT&quot;</td><td>&quot;S&quot;</td><td>&quot;Unknown&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "\n",
       " speech_id  date                        speaker                chamber  state     \n",
       "\n",
       " 530182158  1895-01-10T00:00:00.000000  Mr. COCKRELL           S        Unknown   \n",
       " 890274849  1966-08-31T00:00:00.000000  Mr. LONG of Louisiana  S        Louisiana \n",
       " 880088363  1963-09-11T00:00:00.000000  Mr. FULBRIGHT          S        Unknown   \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: true\n",
    "corpus = Corpus().load(f'{save_path}/us-congressional-speeches-subset-10k.corpus')\n",
    "display(pl.scan_parquet(f'{corpus.corpus_path}/metadata.parquet').head(3).collect(engine='streaming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For corpora created from files, there will always be a field for the source file at the time of creation. This is in the same order as documents are represented in the `tokens.parquet` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th>file</th></tr></thead><tbody><tr><td>&quot;an-ideal-family.txt&quot;</td></tr><tr><td>&quot;at-the-bay.txt&quot;</td></tr><tr><td>&quot;bank-holiday.txt&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "\n",
       " file                \n",
       "\n",
       " an-ideal-family.txt \n",
       " at-the-bay.txt      \n",
       " bank-holiday.txt    \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: true\n",
    "corpus = Corpus().load(f'{save_path}/garden-party.corpus')\n",
    "display(pl.scan_parquet(f'{corpus.corpus_path}/metadata.parquet').head(3).collect(engine='streaming'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_frequency_table(self: Corpus):\n",
    "\t\"\"\" Prepare the frequency table for the corpus. \"\"\"\n",
    "\t# TODO work out case sensitivity issues - currently if do token lookup for The - not there\n",
    "\tif self.frequency_table is None:\n",
    "\t\t# note: don't sort this - leave in order of token_id - sorts can be done when required\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.frequency_table = pl.DataFrame({'token_id': list(self.frequency_lookup.keys()), 'frequency': list(self.frequency_lookup.values())})  \n",
    "\t\tself.frequency_table = self.frequency_table.join(pl.DataFrame({'token_id': list(self.vocab.keys()), 'token': list(self.vocab.values())}), on='token_id', how='left')\n",
    "\t\tself.frequency_table = self.frequency_table.with_columns(self.frequency_table['token_id'].is_in(self.punct_tokens).alias('is_punct')).with_columns(self.frequency_table['token_id'].is_in(self.space_tokens).alias('is_space'))\t\n",
    "\t\tself.frequency_table = self.frequency_table.with_row_index(name='rank', offset=1)\n",
    "\t\tlogger.info(f'Frequency table created in {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _mask_from_positions(self: Corpus, \n",
    "\t\t\t\t\t\t positions # positions to create mask from\n",
    "\t\t\t\t\t\t ):\n",
    "\t\"\"\" Convert positions to mask \"\"\"\n",
    "\tmask_from_positions = np.zeros(self.lower_index.shape, dtype=bool)\n",
    "\tmask_from_positions[positions] = True\n",
    "\treturn mask_from_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_tokens_array(self: Corpus):\n",
    "\t\"\"\" Prepare the tokens array for the corpus. \"\"\"\n",
    "\tif 'tokens_array' not in self.results_cache:\n",
    "\t\tstart_time = time.time()\n",
    "\t\tself.results_cache['tokens_array'] = np.array(list(self.vocab.values()))\n",
    "\t\tlogger.info(f'Create tokens_array in {(time.time() - start_time):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch\n",
    "def _init_tokens_sort_order(self: Corpus):\n",
    "\t\"\"\" Prepare the tokens sort order for the corpus. \"\"\"\n",
    "\tif 'tokens_sort_order' not in self.results_cache:\n",
    "\t\tself._init_tokens_array()\n",
    "\t\t# lowercasing then sorting ...\n",
    "\t\ttokens_array_lower = np.strings.lower(self.results_cache['tokens_array'])\n",
    "\t\tself.results_cache['tokens_sort_order'] = np.argsort(np.argsort(tokens_array_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "# TODO maybe convert to using tokens_array rather than frequency_table\n",
    "def token_to_id(self: Corpus, \n",
    "\t\t\t\ttoken: str # token to get id for\n",
    "\t\t\t\t) -> int|bool: # return token id or False if not found in the corpus\n",
    "\t\"\"\" Get the id for a token string. \"\"\"\n",
    "\n",
    "\tself._init_frequency_table()\n",
    "\ttoken = self.frequency_table.filter(pl.col('token') == token)['token_id']\n",
    "\tif token.shape[0] == 0:\n",
    "\t\treturn False\n",
    "\telse:\n",
    "\t\ttoken = token[0]\n",
    "\treturn token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ID of the token 'dog' like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23289"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.token_to_id('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_ids_to_tokens(self: Corpus, \n",
    "\t\t\t\t\t\ttoken_ids: np.ndarray|list # token ids to retrieve as tokens\n",
    "\t\t\t\t\t\t) -> np.ndarray: # return token strings for token ids\n",
    "\t\"\"\" Get token strings for a list of token ids. \"\"\" \n",
    "\n",
    "\tself._init_tokens_array()\n",
    "\tif isinstance(token_ids, list):\n",
    "\t\ttoken_ids = np.array(token_ids)\n",
    "\treturn self.results_cache['tokens_array'][token_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, conc uses Numpy vector operations where possible. A list or numpy array of Token IDs can be converted to a numpy array of token strings like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['acid', '395,000', 'mckinney'], dtype='<U30')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = [23288, 24576, 47803]\n",
    "brown.token_ids_to_tokens(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def token_ids_to_sort_order(self: Corpus, \n",
    "\t\t\t\t\t\t\ttoken_ids: np.ndarray # token ids to get rank \n",
    "\t\t\t\t\t\t\t) -> np.ndarray: # rank of token ids\n",
    "\t\"\"\" Get the rank of token ids in the frequency table. \"\"\"\n",
    "\t#TODO document that this is a rank\n",
    "\tself._init_tokens_sort_order()\t\n",
    "\n",
    "\treturn self.results_cache['tokens_sort_order'][token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22848, 23289, 18808]\n",
      "['the' 'dog' 'went']\n",
      "[50087 15848 54497]\n"
     ]
    }
   ],
   "source": [
    "test_token_ids = [\n",
    "brown.token_to_id('the'),\n",
    "brown.token_to_id('dog'),\n",
    "brown.token_to_id('went'),\n",
    "]\n",
    "\n",
    "print(test_token_ids)\n",
    "print(brown.token_ids_to_tokens(test_token_ids))\n",
    "print(brown.token_ids_to_sort_order(test_token_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def frequency_of(self: Corpus, \n",
    "\t\t\t\t token:str|int # token id or string to get frequency for\n",
    "\t\t\t\t ) -> int|bool: # return frequency of token or False if not found\n",
    "\t\"\"\" Get the frequency of a specific token. \"\"\"\n",
    "\t# TODO - make work with case insensitive tokens\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tself._init_frequency_table()\n",
    "\t\n",
    "\tif type(token) == str:\n",
    "\t\ttoken = self.token_to_id(token)\n",
    "\t\tif token == False:\n",
    "\t\t\treturn False\n",
    "\n",
    "\tlogger.info(f'Token frequency retrieval time: {(time.time() - start_time):.5f} seconds')\n",
    "\n",
    "\tif token in self.frequency_lookup:\n",
    "\t\treturn int(self.frequency_lookup[token])\n",
    "\telse:\n",
    "\t\treturn False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token [id=24577, go] occurs 625 times.\n",
      "Token [go] occurs 625 times.\n"
     ]
    }
   ],
   "source": [
    "token = 'go'\n",
    "token_id = brown.token_to_id(token)\n",
    "print(f'Token [id={token_id}, {token}] occurs {brown.frequency_of(token_id)} times.')\n",
    "print(f'Token [{token}] occurs {brown.frequency_of(token)} times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# lower_without_punct = test.lower_index[~(test._mask_from_positions(test.punct_positions))]\n",
    "# lower_without_space = test.lower_index[~(test._mask_from_positions(test.space_positions))]\n",
    "# lower_without_space_punct = test.lower_index[~(test._mask_from_positions(test.space_positions) | test._mask_from_positions(test.punct_positions))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def tokenize(self: Corpus, \n",
    "\t\t\t string:str, # string to tokenize \n",
    "\t\t\t return_doc = False, # return doc object\n",
    "\t\t\t simple_indexing = False # use simple indexing\n",
    "             ): # return tokenized string\n",
    "\t\"\"\" Tokenize a string using the Spacy tokenizer. \"\"\"\n",
    "\t# TODO implement case insensitive tokenization\n",
    "\t# TODO implement wildcard search and multiple strings\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tplaceholder_string = 'zzxxzzplaceholderzzxxzz' # so doesn't split tokens\n",
    "\tis_wildcard_search = False\n",
    "\tif simple_indexing == True:\n",
    "\t\tindex_id = LOWER\n",
    "\t\tstrings_to_tokenize = [string.strip()]\n",
    "\telse:\n",
    "\t\traise('only simple_indexing implemented')\n",
    "\t\t# TODO rework\n",
    "\t\t# if '*' in string:\n",
    "\t\t# \tis_wildcard_search = True\n",
    "\t\t# \tstring = string.replace('*',placeholder_string)\n",
    "\t\t# if string.islower() == True:\n",
    "\t\t# \tindex_id = LOWER\n",
    "\t\t# else:\n",
    "\t\t# \tindex_id = ORTH\n",
    "\t\t# if '|' in string:\n",
    "\t\t# \tstrings_to_tokenize = string.split('|')\n",
    "\t\t# else:\n",
    "\t\t# \tstrings_to_tokenize = [string.strip()]\n",
    "\ttoken_sequences = []\n",
    "\tfor doc in self._nlp.tokenizer.pipe(strings_to_tokenize):\n",
    "\t\ttoken_sequences.append(tuple(doc.to_array(index_id)))\n",
    "\t# if is_wildcard_search == True:\n",
    "\t# \ttmp_token_sequence = []\n",
    "\t# \tsequence_count = 1\n",
    "\t# \tfor token in doc:\n",
    "\t# \t\ttmp_token_sequence.append([])\n",
    "\t# \t\tif placeholder_string in token.text:\n",
    "\t# \t\t\tchunked_string = token.text.split(placeholder_string)\n",
    "\t# \t\t\tif len(chunked_string) > 2 or (len(chunked_string) == 2 and chunked_string[0] != '' and chunked_string[1] != ''):\n",
    "\t# \t\t\t\t# use regex\n",
    "\t# \t\t\t\tapproach = 'regex'\n",
    "\t# \t\t\t\tregex = re.compile('.*'.join(chunked_string))\n",
    "\t# \t\t\telif chunked_string[0] == '':\n",
    "\t# \t\t\t\tapproach = 'endswith'\n",
    "\t# \t\t\telse:\n",
    "\t# \t\t\t\tapproach = 'startswith'\n",
    "\t# \t\t\tfor token_id in loaded_corpora[corpus_name]['frequency_lookup']:\n",
    "\t# \t\t\t\tpossible_word = False\n",
    "\t# \t\t\t\tword = loaded_corpora[corpus_name]['vocab'][token_id]\n",
    "\t# \t\t\t\tif approach == 'regex':\n",
    "\t# \t\t\t\t\tif regex.match(word):\n",
    "\t# \t\t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\telif getattr(word,approach)(''.join(chunked_string)):\n",
    "\t# \t\t\t\t\tpossible_word = word\n",
    "\t# \t\t\t\tif possible_word != False:\n",
    "\t# \t\t\t\t\ttmp_token_sequence[token.i].append(loaded_corpora[corpus_name]['vocab'][possible_word])\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttmp_token_sequence[token.i].append(token.orth)\n",
    "\t# \t\tsequence_count *= len(tmp_token_sequence[token.i])\n",
    "\t# \trotated_token_sequence = []\n",
    "\t# \ttoken_repeat = sequence_count\n",
    "\t# \tfor pos in range(len(tmp_token_sequence)):\n",
    "\t# \t\trotated_token_sequence.append([])\n",
    "\t# \t\tif len(tmp_token_sequence[pos]) == 1:\n",
    "\t# \t\t\trotated_token_sequence[pos] += sequence_count * [tmp_token_sequence[pos][0]]\n",
    "\t# \t\telse:\n",
    "\t# \t\t\ttoken_repeat = token_repeat // len(tmp_token_sequence[pos])\n",
    "\t# \t\t\twhile len(rotated_token_sequence[pos]) < sequence_count:\n",
    "\t# \t\t\t\tfor token in tmp_token_sequence[pos]:\n",
    "\t# \t\t\t\t\trotated_token_sequence[pos] += token_repeat * [token]\n",
    "\t# \ttoken_sequences = list(zip(*rotated_token_sequence))\n",
    "\t# \t#for tokens in tmp_token_sequence:\n",
    "\t# \t#    for token in tokens:\n",
    "\t# covert token_sequences to reindexed tokens using original_to_new\n",
    "\ttoken_sequences = [tuple([self.original_to_new[token] for token in sequence]) for sequence in token_sequences] # TODO - check as may not be portable\n",
    "\tlogger.info(f'Tokenization time: {(time.time() - start_time):.5f} seconds')\n",
    "\tif return_doc == True:\n",
    "\t\treturn token_sequences, index_id, doc\n",
    "\telse:\n",
    "\t\treturn token_sequences, index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_str = 'dog'\n",
    "brown_token_sequence, brown_index_id = brown.tokenize(token_str, simple_indexing=True)\n",
    "\n",
    "print(brown_token_sequence, brown._index_name(brown_index_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find positions of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_token_index(self: Corpus, \n",
    "\t\t\t\t\ttoken_sequence: list[np.ndarray], # token sequence to get index for \n",
    "\t\t\t\t\tindex_id: int # index to search (i.e. ORTH, LOWER)\n",
    "\t\t\t\t\t) -> np.ndarray: # positions of token sequence\n",
    "\t\"\"\" Get the positions of a token sequence in the corpus. \"\"\"\n",
    "\t\n",
    "\t#TODO - refactor token_sequence?\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tresults = []\n",
    "\n",
    "\tsequence_len = len(token_sequence[0])\n",
    "\tvariants_len = len(token_sequence)\n",
    "\n",
    "\tif index_id == ORTH:\n",
    "\t\tindex = 'orth_index'\n",
    "\telse:\n",
    "\t\tindex = 'lower_index'\n",
    "\n",
    "\tif (index, sequence_len) not in self.ngram_index:\n",
    "\t\tslices = [] # TODO adjust so not just lower below - so need a var to pass to this function with whether islower\n",
    "\t\t[slices.append(np.roll(getattr(self, index), shift)) for shift in -np.arange(sequence_len)]\n",
    "\t\tseq = np.vstack(slices).T\n",
    "\t\tself.ngram_index[(index, sequence_len)] = seq\n",
    "\n",
    "\tif variants_len == 1:\n",
    "\t\tresults.append(np.where(np.all(self.ngram_index[(index, sequence_len)] == token_sequence[0], axis=1))[0])\n",
    "\telse:\n",
    "\t\tcondition_list = []\n",
    "\t\tchoice_list = variants_len * [True]\n",
    "\t\tfor seq in token_sequence:\n",
    "\t\t\tcondition_list.append(self.ngram_index[(index, sequence_len)] == seq)\n",
    "\t\tresults.append(np.where(np.all(np.select(condition_list, choice_list),axis=1))[0])\n",
    "\n",
    "\tlogger.info(f'Token indexing ({len(results[0])}) time: {(time.time() - start_time):.5f} seconds')\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  18944,   18981,   18992,   19062,   19069,   37777,   89076,\n",
      "        125511,  137608,  138261,  138296,  138305,  138349,  144502,\n",
      "        189104,  249691,  249831,  250054,  250067,  250093,  250161,\n",
      "        250187,  250247,  250275,  250386,  251335,  251354,  251414,\n",
      "        251473,  251505,  251559,  251569,  251894,  253602,  254562,\n",
      "        256120,  256224,  256397,  331441,  360984,  439241,  439245,\n",
      "        439300,  439305,  464727,  464756,  464778,  522492,  649908,\n",
      "        695780,  695829,  695989,  696181,  696460,  696839,  696916,\n",
      "        697014,  863902,  863909,  865540,  865558,  877577,  877619,\n",
      "        877706,  889653,  997085, 1014338, 1030313, 1052840, 1052849,\n",
      "       1054274, 1077178, 1087042, 1088300, 1088332, 1088919, 1107306,\n",
      "       1130649, 1139762])]\n"
     ]
    }
   ],
   "source": [
    "token_str = 'dog'\n",
    "brown_token_sequence, brown_token_id = brown.tokenize(token_str, simple_indexing=True)\n",
    "brown_token_index = brown.get_token_index(brown_token_sequence, brown_index_id)\n",
    "print(brown_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
